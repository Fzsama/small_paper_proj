,topic_id,content,doc_index,full_doc
0,0,Leveraging artificial intelligence to enhance human team cognition in human-ai teams.,9,Leveraging Artificial Intelligence for Team Cognition in Human-AI Teams nan nan
1,0,"Human-AI cooperation can significantly improve performance on sequential decision making problems by leveraging AI's computational capabilities and human intuition, leading to more effective collaboration between humans and artificial intelligence systems.",199,Towards Human-AI Cooperation on Sequential Decision Making Problems nan nan
2,0,"Human factors science needs to adapt to the evolution of the human-machine relationship with AI technology, proposing new conceptual models and frameworks such as human-AI joint cognitive systems, human-AI joint cognitive ecosystems, and intelligent sociotechnical systems to provide comprehensive solutions for optimizing the human-AI collaboration.",29,"New research paradigms and agenda of human factors science in the intelligence era nan This paper first proposes the innovative concept of human factors science to characterize engineering psychology, human factors engineering, ergonomics, human-computer interaction, and other similar fields. Although the perspectives in these fields differ, they share a common goal: optimizing the human-machine relationship by applying a human-centered design approach. AI technology has brought in new characteristics, and our recent research reveals that the human-machine relationship presents a trans-era evolution from human-machine interaction to human-AI teaming. These changes have raised questions and challenges for human factors science, compelling us to re-examine current research paradigms and agendas. In this context, this paper reviews and discusses the implications of the following three conceptual models and frameworks that we recently proposed to enrich the research paradigms for human factors science. (1) human-AI joint cognitive systems: this model differs from the traditional human-computer interaction paradigm and regards an intelligent system as a cognitive agent with a certain level of cognitive capabilities. Thus, a human-AI system can be characterized as a joint cognitive system in which two cognitive agents (human and intelligent agents) work as teammates for collaboration. (2) human-AI joint cognitive ecosystems: an intelligent ecosystem with multiple human-AI systems can be represented as a human-AI joint cognitive ecosystem. The overall system performance of the intelligent ecosystem depends on optimal collaboration and design across the multiple human-AI systems. (3) intelligent sociotechnical systems (iSTS): human-AI systems are designed, developed, and deployed in an iSTS environment. From a macro perspective, iSTS focuses on the interdependency between the technical and social subsystems. The successful design, development, and deployment of a human-AI system within an iSTS environment depends on the synergistic optimization between the two subsystems. This paper further enhances these frameworks from the research paradigm perspective. We propose three new research paradigms for human factors science in the intelligence ear: human-AI joint cognitive systems, human-AI joint cognitive ecosystems, and intelligent sociotechnical systems, enabling comprehensive human factors solutions for AI-based intelligent systems. Further analyses show that the three new research paradigms will benefit future research in human factors science. Furthermore, this paper looks forward to the future research agenda of human factors science from three aspects: human-AI interaction, intelligent human-machine interface, and human-AI teaming. We believe the proposed research paradigms and the future research agenda will mutually promote each other, further advancing human factors science in the intelligence era."
3,1,The AI-powered code generation models like GitHub Copilot are prone to security weaknesses due to their training on large corpora of code containing bugs and vulnerabilities.,338,"Assessing the Security of GitHub Copilot's Generated Code - A Targeted Replication Study nan AI-powered code generation models have been developing rapidly, allowing developers to expedite code generation and thus improve their productivity. These models are trained on large corpora of code (primarily sourced from public repositories), which may contain bugs and vulnerabilities. Several concerns have been raised about the security of the code generated by these models. Recent studies have investigated security issues in AI-powered code generation tools such as GitHub Copilot and Amazon Code Whisperer, revealing several security weaknesses in the code generated by these tools. As these tools evolve, it is expected that they will improve their security protocols to prevent the suggestion of insecure code to developers. This paper replicates the study of Pearce et al., which investigated security weaknesses in Copilot and uncovered several weaknesses in the code suggested by Copilot across diverse scenarios and languages (Python, C, and Verilog). Our replication examines Copilot's security weaknesses using newer versions of Copilot and CodeQL (the security analysis framework). The replication focused on the presence of security vulnerabilities in Python code. Our results indicate that, with the improvements in newer versions of Copilot, the percentage of vulnerable code suggestions has reduced from 36.54% to 27.25%. Nonetheless, it remains evident that the model still suggests insecure code."
4,1,Large language models like GPT and Codex can significantly increase productivity in programming tasks when used as AI assistance for code generation.,553,"Significant Productivity Gains through Programming with Large Language Models nan Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems."
5,1,"The use of large language models and AI code assistants like GitHub Copilot can introduce vulnerabilities and compromise trust, but with targeted risk management and further research, the benefits of these tools can be realized while minimizing the risks.",404,"PwnPilot: Reflections on Trusting Trust in the Age of Large Language Models and AI Code Assistants nan At the dawn of a new era in software engineering, one defined by large language models (LLMs) and AI code assistants like GitHub Copilot, new meaning can be found from a historic Turing Award Lecture that concluded one cannot trust source code they did not totally create themselves. In this paper, a targeted, systematic survey of the latest research results from 2019 to early 2023 highlights the possible risks of using AI code assistants that produce substantial source code contributions, and the potential for an AI Copilot to unknowingly become PwnPilot, a malevolent digital actor that introduces vulnerabilities and compromises trust. During a period of explosive growth for generative AI, renewed reflections on trusting trust point to conclusions similar to the original assertions of Ken Thompson in 1984. But despite a recent theoretical roadblock from proof of ability to plant undetectable backdoors in machine learning models, the potential for enhanced productivity from AI code assistants may still be realizable with an acceptable level of risk, perhaps even for safety critical and sensitive security relevant contexts. In support of that goal, a number of near-term risk management options and longer term research paths are identified as enablers for practitioners and inputs to potential research roadmaps toward more secure and trusted AI code generation."
