,pub_years,3_in_1
0,2024,"Human-AI Teaming: Following the IMOI Framework nan Human-AI teams are becoming more and more important in decision making (DM). In this work we propose approaching Human-AI teams in DM in a similar way that we approach Human-only teams. In this respect, processes that proved to be important for the functioning of human-only teams, need to be understood in these diverse socio-technical Human-AI settings. Following research in organizational psychology, we propose examining the work that has been done in Human-AI teams following the Input - Mediator - Output - Input framework. In this paper, we reviewed related work on DM in Human-AI teams, to demonstrate that by taking this perspective we can identify aspects, important for the successful functioning of Human-AI teams. Potential future work in the area is then highlighted."
2,2023,"Decision Making Strategies and Team Efficacy in Human-AI Teams nan Human-AI teams are increasingly prevalent in various domains. We investigate how the decision-making of a team member in a human-AI team impacts the outcome of the collaboration and perceived team-efficacy. In a large scale study on Mechanical Turk (n=125), we find significant differences across different decision making styles and disclosed AI identity disclosure in an AI-driven collaborative game. We find that autocratic decision-making negatively impacts team-efficacy in Human-AI teams, similar to its effects on human-only teams. We find that decision making style and AI-identity disclosure impacts how individuals make decisions in a collaborative context. We discuss our findings of the differences of collaborative behavior in human-human-AI teams and human-AI-AI teams."
3,2021,"Modeling and Guiding the Creation of Ethical Human-Al Teams nan With artificial intelligence continuing to advance, so too do the ethical concerns that can potentially negatively impact humans and the greater society. When these systems begin to interact with humans, these concerns become much more complex and much more important. The field of human-AI teaming provides a relevant example of how AI ethics can have significant and continued effects on humans. This paper reviews research in ethical artificial intelligence, as well as ethical teamwork through the lens of the rapidly advancing field of human-AI teaming, resulting in a model demonstrating the requirements and outcomes of building ethical human-AI teams. The model is created to guide the prioritization of ethics in human-AI teaming by outlining the ethical teaming process, outcomes of ethical teams, and external requirements necessary to ensure ethical human-AI teams. A final discussion is presented on how the developed model will influence the implementation of AI teammates, as well as the development of policy and regulation surrounding the domain in the coming years."
4,2024,"AI-teaming: Redefining collaboration in the digital era. nan Integrating artificial intelligence (AI) into human teams, forming human-AI teams (HATs), is a rapidly evolving field. This overview examines the complexities of team constellations and dynamics, trust in AI teammates, and shared cognition within HATs. Adding an AI teammate often reduces coordination, communication, and trust. Further, trust in AI tends to decline over time due to initial overestimation of capabilities, impairing teamwork. Despite AI's potential to enhance performance in contexts like chess and medicine, HATs frequently underperform due to poor team cognition and inadequate mutual understanding. Future research must address these issues with interdisciplinary collaboration between computer science and psychology and advance robust theoretical frameworks to realize the full potential of human-AI teaming."
5,2021,"Who/What Is My Teammate? Team Composition Considerations in Human-AI Teaming nan There are many unknowns regarding the characteristics and dynamics of human-AI teams, including a lack of understanding of how certain human-human teaming concepts may or may not apply to human-AI teams and how this composition affects team performance. This article outlines an experimental research study that investigates essential aspects of human-AI teaming such as team performance, team situation awareness, and perceived team cognition in various mixed composition teams (human-only, human-human-AI, human-AI-AI, and AI-only) through a simulated emergency response management scenario. Results indicate dichotomous outcomes regarding perceived team cognition and performance metrics, as perceived team cognition was not predictive of performance. Performance metrics like team situational awareness and team score showed that teams composed of all human participants performed at a lower level than mixed human-AI teams, with the AI-only teams attaining the highest performance. Perceived team cognition was highest in human-only teams, with mixed composition teams reporting perceived team cognition 58% below the all-human teams. These results inform future mixed teams of the potential performance gains in utilizing mixed teams' over human-only teams in certain applications, while also highlighting mixed teams' adverse effects on perceived team cognition."
6,2024,"Stimulating Creative Hypothesis Discovery by Future Human-AI Teaming nan We propose Human-AI teaming to enhance hypothesis discovery creation (HDHAT), envisioning a future where humans and AI collaborate as teams in more creative activities. Our preliminary qualitative experiment on creativity among people extracted implications for supporting creative activities in future Human-AI teams, highlighting the importance of dynamic creative processes and strategy sharing."
7,2023,"The role of shared mental models in human-AI teams: a theoretical review nan Mental models are knowledge structures employed by humans to describe, explain, and predict the world around them. Shared Mental Models (SMMs) occur in teams whose members have similar mental models of their task and of the team itself. Research on human teaming has linked SMM quality to improved team performance. Applied understanding of SMMs should lead to improvements in human-AI teaming. Yet, it remains unclear how the SMM construct may differ in teams of human and AI agents, how and under what conditions such SMMs form, and how they should be quantified. This paper presents a review of SMMs and the associated literature, including their definition, measurement, and relation to other concepts. A synthesized conceptual model is proposed for the application of SMM literature to the human-AI setting. Several areas of AI research are identified and reviewed that are highly relevant to SMMs in human-AI teaming but which have not been discussed via a common vernacular. A summary of design considerations to support future experiments regarding Human-AI SMMs is presented. We find that while current research has made significant progress, a lack of consistency in terms and of effective means for measuring Human-AI SMMs currently impedes realization of the concept."
8,2022,"Modeling Goal Alignment in Human-AI Teaming: A Dynamic Game Theory Approach nan As human-AI teaming becomes increasingly prevalent, goal alignment has emerged as a critical yet unsolved issue. Misaligned goals can be amplified by the situation and strategic interactions, which can further impact the teaming process and performance. These interrelated factors lack a systematic and computational model. To address this gap, we developed a dynamic game theoretical framework simulating the human-AI interdependency by integrating the Drift Diffusion Model simulating the goal alignment process. A 3 (Situation Structure) x 3 (Strategic Behaviors) x 2 (Initial Goal Alignment) simulation study of human-AI teaming was designed. Results showed that teaming with an altruistic agent in a competitive situation leads to the highest team performance. Moreover, goal alignment process can dissolve the initial goal conflict. Our study provides a first step of modeling goal alignment and implies a tradeoff between a balanced and cooperative team to guide human-AI teaming design."
9,2023,"Investigating AI Teammate Communication Strategies and Their Impact in Human-AI Teams for Effective Teamwork nan Recently, AI is integrating into teams to collaborate with humans as a teammate with the goal of achieving unprecedented team outcomes. Much of the coordination between humans and AI teammates relies on human-AI communication, which is challenging due to AI's limitations on natural language communication. Thus, it is essential to identify and develop effective communication strategies for AI teammates in human-AI teams to facilitate the coordination process. Through interviews with 60 participants who collaborated with an AI teammate in a multiplayer online game, in this paper, we explore communication strategies that humans expect AI teammates to apply to support human-AI coordination and collaboration in dyadic teaming environments, and how the AI teammate's communication can impact teaming processes. Our findings highlight four communication strategies AI teammates should apply to support their coordination with humans in dyadic teaming environments. We also find that AI teammates' proactive communication with humans could facilitate the development of human trust and situation awareness, whereas AI lacking such proactive communication is often not perceived as a teammate. Our study extends the current CSCW/HCI research on human-AI communication in teaming environments by shedding light on how communication should be structured in dyadic human-AI teams for effective and smooth collaboration."
10,2023,Leveraging Artificial Intelligence for Team Cognition in Human-AI Teams nan nan
11,2020,"HACO: A Framework for Developing Human-AI Teaming nan We witnessed great advancement in artificial intelligence (AI) powered technologies over the past few decades. Wide use of AI technologies has led to the creation of an ecosystem where human and AI systems are partners, complementing each other with their strengths. To build a successful human-AI team, there are several considerations, including context awareness, effective communication, pro-activeness, etc. In this paper, we present a taxonomy of human-AI teaming concepts. We extend a multi-agent framework, Java Agent Development Framework ( JADE), to support the proposed taxonomy. Our solution framework, Human-AI Collaboration (HACO), enables a model-driven development of human-AI teaming systems through graphical user interface. In this paper, we present the solution architecture for extending JADE with human-AI teaming taxonomy. A user study performed to assess the usefulness of HACO, shows that HACO is a promising framework. We evaluated the proposed framework by developing a set of use cases for a contact center and observed a signification reduction in the overall development effort."
12,2024,"More than Task Performance: Developing New Criteria for Successful Human-AI Teaming Using the Cooperative Card Game Hanabi nan As we shift to designing AI agents as teammates rather than tools, the social aspects of human-AI interaction become more pronounced. Consequently, to develop agents that are able to navigate the social dynamics that accompany cooperative teamwork, evaluation criteria that refer only to objective task performance will not be sufficient. We propose perceived cooperativity and teaming perception as subjective metrics for investigating successful human-AI teaming. Corresponding questionnaire scales were developed and tested in a pilot study employing the collaborative card game Hanabi, which has been identified as a unique setting for investigating human-AI teaming. Preliminary descriptive results suggest that rule-based and reinforcement learning-based agents differ in terms of perceived cooperativity and teaming perception. Future work will extend the results in a large user study to psychometrically evaluate the scales and test a conceptual framework that includes further aspects related to social dynamics in human-AI teaming."
13,2020,"An Ideal Human: Expectations of AI Teammates in Human-AI Teaming nan Driven by state-of-the-art AI technologies, human-AI collaboration has become an important area in computer-supported teamwork research. While human-AI collaboration has been investigated in various domains, more research is needed to explore human perceptions and expectations of AI teammates in human-AI teaming. To achieve an in-depth understanding of how people perceive AI teammates and what they expect from AI teammates in human-AI teaming, we conducted a survey with 213 participants and a follow-up interview with 20 participants. Considering the context-dependency of teamwork, we chose to study human-AI teaming in the context of multiplayer online games as a case study. This study shows that people have mixed feelings toward AI teammates but hold a positive attitude toward future collaboration with AI teammates in general. Our findings highlight people's expectations for AI teammates in a rapidly changing collaborative environment (e.g., instrumental skills for in-game tasks, shared understanding between humans and AI, communication capabilities, human-like behaviors and performance), as well as factors that impact people's willingness to team up with AI teammates (e.g., pre-existing attitudes toward AI, previous collaboration experience with humans). We contribute to CSCW by shedding light on how AI should be structured in human-AI teaming to support highly complex collaborative activities in CSCW environments."
14,2023,MULTITTRUST: 2nd Workshop on Multidisciplinary Perspectives on Human-AI Team Trust nan nan
15,2022,"An Agile New Research Framework for Hybrid Human-Al Teaming: Trust, Transparency, and Transferability nan We propose a new research framework by which the nascent discipline of human-AI teaming can be explored within experimental environments in preparation for transferal to real-world contexts. We examine the existing literature and unanswered research questions through the lens of an Agile approach to construct our proposed framework. Our framework aims to provide a structure for understanding the macro features of this research landscape, supporting holistic research into the acceptability of human-AI teaming to human team members and the affordances of AI team members. The framework has the potential to enhance decision-making and performance of hybrid human-AI teams. Further, our framework proposes the application of Agile methodology for research management and knowledge discovery. We propose a transferability pathway for hybrid teaming to be initially tested in a safe environment, such as a real-time strategy video game, with elements of lessons learned that can be transferred to real-world situations."
16,2023,"Human-AI teams-Challenges for a team-centered AI at work. nan As part of the Special Issue topic Human-Centered AI at Work: Common Ground in Theories and Methods, we present a perspective article that looks at human-AI teamwork from a team-centered AI perspective, i. e., we highlight important design aspects that the technology needs to fulfill in order to be accepted by humans and to be fully utilized in the role of a team member in teamwork. Drawing from the model of an idealized teamwork process, we discuss the teamwork requirements for successful human-AI teaming in interdependent and complex work domains, including e.g., responsiveness, situation awareness, and flexible decision-making. We emphasize the need for team-centered AI that aligns goals, communication, and decision making with humans, and outline the requirements for such team-centered AI from a technical perspective, such as cognitive competence, reinforcement learning, and semantic communication. In doing so, we highlight the challenges and open questions associated with its implementation that need to be solved in order to enable effective human-AI teaming."
17,2020,"HACO: a Framework for Developing Human-AI Teaming nan We witnessed great advancement in artificial intelligence (AI) powered technologies over the past few decades. Wide use of AI technologies has led to the creation of an ecosystem where human and AI systems are partners, complementing each other with their strengths. To build a successful human-AI team, there are several considerations, including context awareness, effective communication, pro-activeness, etc. In this paper, we present a taxonomy of human-AI teaming concepts. We extend a multi-agent framework, Java Agent Development Framework (JADE), to support the proposed taxonomy. Our solution framework, Human-AI Collaboration (HACO), enables a model-driven development of human-AI teaming systems through graphical user interface. In this paper, we present the solution architecture for extending JADE with human-AI teaming taxonomy. A user study performed to assess the usefulness of HACO, shows that HACO is a promising framework. We evaluated the proposed framework by developing a set of use cases for a contact center and observed a signification reduction in the overall development effort. The framework video can be viewed at https://youtu.be/lNyrrk8dMqU."
18,2022,"JSwarm: A Jingulu-Inspired Human-AI-Teaming Language for Context-Aware Swarm Guidance nan Bi-directional communication between humans and swarm systems begs for efficient languages to communicate information between the humans and the Artificial Intelligence (AI)-enabled agents in a manner that is most appropriate for the context. We discuss the criteria for effective teaming and functional bi-directional communication between humans and AI, and the design choices required to create effective languages. We then present a human-AI-teaming communication language inspired by the Australian Aboriginal language of Jingulu, which we call JSwarm. We present the motivation and structure of the language. An example is used to demonstrate how the language operates for a shepherding swarm guidance task."
19,2023,"Create Effective and Responsible AI User Experiences with The Human-AI Experience (HAX) Toolkit nan The HAX Toolkit (https://aka.ms/haxtoolkit) is a set of collaborative tools that helps teams working on user-facing AI plan, create, and evaluate human-AI user experiences. This course will help AI practitioners, human-AI interaction researchers, teachers, and students learn how to use the HAX Toolkit themselves and how to introduce it to others. The Toolkit is grounded in a set of Guidelines for Human-AI Interaction [1] that prescribe how AI systems should behave when interacting with people. Course attendees will explore the nuances of each guideline and learn how to use the AI patterns and examples in the HAX Design Library to apply the Guidelines. Course attendees will also learn how to guide cross-disciplinary teams in planning user-facing AI systems by using the HAX Workbook. For NLP systems, course attendees will learn to use the HAX Playbook [2] to anticipate and design for failures.The HAX Toolkit is a set of collaborative tools that helps teams working on user-facing AI plan, create, and evaluate human-AI user experiences. This course will help AI practitioners, human-AI interaction researchers, teachers, and students learn how to use the HAX Toolkit themselves and how to introduce it to others. Course attendees will learn the nuances of the Guidelines for Human-AI Interaction, how to lead cross-disciplinary teams in planning human-AI interaction using the HAX Workbook, and how to use the HAX Workbook to plan for failures of NLP systems."
20,2024,Editorial: Human-centered AI at work: common ground in theories and methods. nan nan
21,2023,"Human-AI teaming: leveraging transactive memory and speaking up for enhanced team effectiveness. nan In this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team's ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team's transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond."
22,2024,"A Retrospective Engineering Analysis of Human -AI Teams Using the Sidekick Principles nan The speed of development of Artificial Intelligence (AI) capabilities outpaces that of human systems engineering, particularly for the integration of human-AI teams (HAT) in production-level software. The emergence of frameworks for the principled design and development of hybrid systems offers opportunities to close this gap: They can help identify which common HAT components should be built and matured to accelerate the engineering of such hybrid systems. Leveraging the sidekick principles of human-AI teaming in a review of recent efforts, we identified a prioritized list of system requirements for reusable HAT components. We share these insights with a specific emphasis on the analytical microservices we believe are key to instantiating the sidekick principles."
24,2022,"Artificial Trust as a Tool in Human-AI Teams nan Mutual trust is considered a required coordinating mechanism for achieving effective teamwork in human teams. However, it is still a challenge to implement such mechanisms in teams composed by both humans and AI (human-AI teams), even though those are becoming increasingly prevalent. Agents in such teams should not only be trustworthy and promote appropriate trust from the humans, but also know when to trust a human teammate to perform a certain task. In this project, we study trust as a tool for artificial agents to achieve better team work. In particular, we want to build mental models of humans so that agents can understand human trustworthiness in the context of human-AI teamwork, taking into account factors such as human teammates', task's and environment's characteristics."
25,2023,"A Minecraft Based Simulated Task Environment for Human AI Teaming nan In this extended abstract we present the design, development, and evaluation of a Minecraft-based simulated task environment to conduct human and AI teaming research. With the deluge of AI-driven applications and their infiltration into many activities of daily living, it is becoming necessary to look at ways that humans and AI can work together. There is a tremendous research burden associated with accurately evaluating the best practices and tradeoffs when humans and AI have to collaborate together in completing critical tasks. Minecraft offers a low-cost alternative as an early investigating tool for researchers to build answers to emerging research questions before significantly investing in human-AI teaming activities in the real world. We demonstrate successfully via a simple rule-based AI, insights that could highly influence human-AI teaming activities can be derived to improve practical and viable development of protocols and procedures. Our findings indicate that simulated task environments play a critical role in furthering human AI teaming activities."
26,2021,"Human-AI Symbiosis: A Survey of Current Approaches [arXiv] nan In this paper, we aim at providing a comprehensive outline of the different threads of work in human-AI collaboration. By highlighting various aspects of works on the human-AI team such as the flow of complementing, task horizon, model representation, knowledge level, and teaming goal, we make a taxonomy of recent works according to these dimensions. We hope that the survey will provide a more clear connection between the works in the human-AI team and guidance to new researchers in this area."
28,2024,"A Cybersecurity Game to Probe Human-AI Teaming nan Recent advances in AI indicate that the future of cybersecurity workforce development lies in professionals working in Human-AI teams to defend online resources from opposing Human-AI teams of malicious attackers. However, there is little research on how human biases and attitudes affects the performance of human-AI teams in cybersecurity. To help explore this new research area, we describe a simulation game that helps students (future professionals) understand the concept of firewalls while enabling us to probe attitudes towards cybersecurity and AI, as well as trust and cooperation in HumanAI teams. Early study prototyping results indicate that students prefer an AI-teammate over a human in this simulation game setting. In addition, students seem to engage well with the game play, pointing towards this research platform's suitability for exploring trust and cooperation in human-AI teams for game-based cybersecurity training, and to support our prior results on differing perspectives on cybersecrity risk."
29,2024,"Towards Integrating Knowledge Graphs into Process-Oriented Human-AI Collaboration in Industry nan Human-AI collaboration in industrial manufacturing promises to overcome current limitations by combining the flexibility of human intelligence and the scaling and processing capabilities of machine intelligence. To ensure effective collaboration between human and AI team members, we envision a software-driven coordination mechanism that orchestrates the interactions between the participants in Human-AI teaming scenarios and help to synchronize the information flow between them. A structured process-oriented approach to systems engineering aims at generalizability, deployment efficiency and enhancing the quality of the resulting software by formalizing the human-AI interaction as a BPMN process model. During runtime, this process model is executed by the teaming engine, one of the core components of the Teaming.AI software platform. By incorporating dynamic execution traces of these process models into a knowledge graph structure and linking them to contextual background knowledge, we facilitate the monitoring of variations in process executions and inference of new insights during runtime. Knowledge graphs are a powerful tool for semantic integration of diverse data, thereby significantly improving the data quality, which is still one of the biggest issues in AI-driven software solutions. We present the Teaming.AI software platform and its key components as a framework for enabling transparent teamwork between humans and AI in industry. We discuss its application in the context of an industrial use case in plastic injection molding production. Overall, this Teaming.AI platform provides a robust, flexible and accountable solution for human-AI collaboration in manufacturing."
30,2023,"Human-AI teams in complex military operations: Soldiers' perception of intelligent AI agents as teammates in human-AI teams nan Military decision-making frequently involves complex problems in non-routine situations with minimal rule-based or automated solutions. As human information processing capabilities are limited, processing the required scale of information increases the decision-makers' workload, impacting their situational awareness and affecting the quality of soldiers' decisions. This study uses a route clearance task as a use-case scenario to understand the issues in team-level decision-making in military tasks, the challenges in successfully completing the mission, and the demands placed on AI teammates when working in a human-AI team. We interviewed eight subject matter experts with prior experience in route clearance operations. The themes generated by analyzing the interview transcripts provide insights into soldiers' perceptions of AI teammates as well as recommendations for successfully integrating human-AI teams in military settings."
31,2024,"Humans as teammates: The signal of human-AI teaming enhances consumer acceptance of chatbots nan Human-artificial intelligence (AI) teaming is a service system in which AI agents work interdependently toward a common goal alongside human agents. Although many consumer services rely on chatbots working with humans, little is known about the influence of human-AI teaming on consumers' perceptions and use of chatbots in online service encounters. Using signaling theory, the present research examines whether and how human-AI teaming (vs. independent AI) increases consumer acceptance of chatbots. Through six scenario-based studies and an interview, we found that human-AI teaming can use human capabilities to endorse the effectiveness and authenticity of AI, leading to increased chatbot acceptance. However, this effect was not observed when AI capability was clear or the human service experience was negative. We contribute to information systems research by showing the mechanism and boundary conditions underlying the effect of human-AI teaming on chatbot acceptance. We also provide practical insights for managers emphasizing how human teammates in AI-consumer conversations can increase consumer acceptance of AI. All rights reserved Elsevier."
32,2024,"New research paradigms and agenda of human factors science in the intelligence era nan This paper first proposes the innovative concept of human factors science to characterize engineering psychology, human factors engineering, ergonomics, human-computer interaction, and other similar fields. Although the perspectives in these fields differ, they share a common goal: optimizing the human-machine relationship by applying a human-centered design approach. AI technology has brought in new characteristics, and our recent research reveals that the human-machine relationship presents a trans-era evolution from human-machine interaction to human-AI teaming. These changes have raised questions and challenges for human factors science, compelling us to re-examine current research paradigms and agendas. In this context, this paper reviews and discusses the implications of the following three conceptual models and frameworks that we recently proposed to enrich the research paradigms for human factors science. (1) human-AI joint cognitive systems: this model differs from the traditional human-computer interaction paradigm and regards an intelligent system as a cognitive agent with a certain level of cognitive capabilities. Thus, a human-AI system can be characterized as a joint cognitive system in which two cognitive agents (human and intelligent agents) work as teammates for collaboration. (2) human-AI joint cognitive ecosystems: an intelligent ecosystem with multiple human-AI systems can be represented as a human-AI joint cognitive ecosystem. The overall system performance of the intelligent ecosystem depends on optimal collaboration and design across the multiple human-AI systems. (3) intelligent sociotechnical systems (iSTS): human-AI systems are designed, developed, and deployed in an iSTS environment. From a macro perspective, iSTS focuses on the interdependency between the technical and social subsystems. The successful design, development, and deployment of a human-AI system within an iSTS environment depends on the synergistic optimization between the two subsystems. This paper further enhances these frameworks from the research paradigm perspective. We propose three new research paradigms for human factors science in the intelligence ear: human-AI joint cognitive systems, human-AI joint cognitive ecosystems, and intelligent sociotechnical systems, enabling comprehensive human factors solutions for AI-based intelligent systems. Further analyses show that the three new research paradigms will benefit future research in human factors science. Furthermore, this paper looks forward to the future research agenda of human factors science from three aspects: human-AI interaction, intelligent human-machine interface, and human-AI teaming. We believe the proposed research paradigms and the future research agenda will mutually promote each other, further advancing human factors science in the intelligence era."
33,2024,"An Evaluation of Situational Autonomy for Human-AI Collaboration in a Shared Workspace Setting nan Designing interactions for human-AI teams (HATs) can be challenging due to an AI agent's potential autonomy. Previous work suggests that higher autonomy does not always improve team performance, and situation-dependent autonomy adaptation might be beneficial. However, there is a lack of systematic empirical evaluations of such autonomy adaptation in human-AI interaction. Therefore, we propose a cooperative task in a simulated shared workspace to investigate effects of fixed levels of AI autonomy and situation-dependent autonomy adaptation on team performance and user satisfaction. We derive adaptation rules for AI autonomy from previous work and a pilot study. We implement these rule for our main experiment and find that team performance was best when humans collaborated with an agent adjusting its autonomy based on the situation. Additionally, users rated this agent highest in terms of perceived intelligence. From these results, we discuss the influence of varying autonomy degrees on HATs in shared workspaces."
34,2023,"Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives [arXiv] nan Sonalysts is working on an initiative to expand our current expertise in teaming to Human-Artificial Intelligence (AI) teams by developing original research in this area. To provide a foundation for that research, Sonalysts is investigating the development of a Synthetic Task Environment (STE). In a previous report, we documented the findings of a recent outreach effort in which we asked military Subject Matter Experts (SMEs) and other researchers in the Human-AI teaming domain to identify the qualities that they most valued in a testbed. A surprising finding from that outreach was that several respondents recommended that our team look into existing human-AI teaming testbeds, rather than creating something new. Based on that recommendation, we conducted a systematic investigation of the associated landscape. In this report, we describe the results of that investigation. Building on the survey results, we developed testbed evaluation criteria, identified potential testbeds, and conducted qualitative and quantitative evaluations of candidate testbeds. The evaluation process led to five candidate testbeds for the research team to consider. In the coming months, we will assess the viability of the various alternatives and begin to execute our program of research."
35,2022,"Bad, mad and cooked apples: Responsibility for unlawful targeting in human-AI military teams [arXiv] nan A Nation's responsibility is to predict in advance and protect human wellbeing in conflict including protection from moral injury and unjust attribution of responsibility for their actions. This position paper considers responsibility for unlawful killings by human AI teams drawing on a metaphor from Neta Crawford's chapter, When Soldiers Snap: Bad Apples and Mad Apples, in Accountability for Killing: Moral responsibility for collateral damage in America's post 911 wars. This paper contends that although militaries may have some bad apples responsible for war crimes and some mad apples unable to be responsible for their actions during a conflict, increasingly militaries may cook their good apples by putting them in untenable decision making environments with AI. A cooked apple may be pushed beyond reasonable limits leading to a loss of situational awareness, cognitive overload, loss of agency and autonomy leading to automation bias. In these cases, moral responsibility and perhaps even legal responsibility for unlawful deaths may be contested for cooked apples, risking operators becoming moral crumple zones and or suffering moral injury from being part of larger human AI systems authorised by the state. Nations are responsible for minimising risks to humans within reasonable bounds and compliance with legal obligations in human AI military teams, and the military systems used to make or implement decisions. The paper suggests that best practise WHS frameworks might be drawn on in development, acquisition and training ahead of deployment of systems in conflicts to predict and mitigate risks of human AI military teams."
36,2023,Structuring AI Teammate Communication: An Exploration of AI's Communication Strategies in Human-AI Teams nan nan
37,2023,"A Minecraft based simulated task environment for human AI teaming nan In this extended abstract we present the design, development, and evaluation of a Minecraft-based simulated task environment to conduct human and AI teaming research. With the deluge of AI-driven applications and their infiltration into many activities of daily living, it is becoming necessary to look at ways that humans and AI can work together. There is a tremendous research burden associated with accurately evaluating the best practices and trade-offs when humans and AI have to collaborate together in completing critical tasks. Minecraft offers a low-cost alternative as an early investigating tool for researchers to build answers to emerging research questions before significantly investing in human-AI teaming activities in the real world. We demonstrate successfully via a simple rule-based AI, insights that could highly influence human-AI teaming activities can be derived to improve practical and viable development of protocols and procedures. Our findings indicate that simulated task environments play a critical role in furthering human AI teaming activities."
38,2021,"Determinants and Predictors of Intentionality and Perceived Reliability in Human-AI Interaction as a Means for Innovative Scientific Discovery nan With the increasing development of human-AI teaming structures within and across geographies, the time is ripe for a continuous and objective look at the predictors, barriers, and facilitators of human-AI scientific collaboration from a multidisciplinary point of view. This paper aims at contributing to this end by exploiting a set of factors affecting attitudes towards the adoption of human-AI interaction into scientific work settings. In particular, we are interested in identifying the determinants of trust and acceptability when considering the combination of hybrid human-AI approaches for improving research practices. This includes the way as researchers assume human-centered artificial intelligence (AI) and crowdsourcing as valid mechanisms for aiding their tasks. Through the lens of a unified theory of acceptance and use of technology (UTAUT) combined with an extended technology acceptance model (TAM), we pursue insights on the perceived usefulness, potential blockers, and adoption drivers that may be representative of the intention to use hybrid intelligence systems as a way of unveiling unknown patterns from large amounts of data and thus enabling novel scientific discoveries."
39,2024,"When in Doubt! Understanding the Role of Task Characteristics on Peer Decision-Making with AI Assistance nan With the integration of AI systems into our daily lives, human-AI collaboration has become increasingly prevalent. Prior work in this realm has primarily explored the effectiveness and performance of individual human and AI systems in collaborative tasks. While much of decision-making occurs within human peers and groups in the real world, there is a limited understanding of how they collaborate with AI systems. One of the key predictors of human-AI collaboration is the characteristics of the task at hand. Understanding the influence of task characteristics on human-AI collaboration is crucial for enhancing team performance and developing effective strategies for collaboration. Addressing a research and empirical gap, we seek to explore how the features of a task impact decision-making within human-AI group settings. In a 2 * 2 between-subjects study (N = 256) we examine the effects of task complexity and uncertainty on group performance and behaviour. The participants were grouped into pairs and assigned to one of four experimental conditions characterized by varying degrees of complexity and uncertainty. We found that high task complexity and high task uncertainty can negatively impact the performance of human-AI groups, leading to decreased group accuracy and increased disagreement with the AI system. We found that higher task complexity led to a higher efficiency in decision-making, while a higher task uncertainty had a negative impact on efficiency. Our findings highlight the importance of considering task characteristics when designing human-AI collaborative systems, as well as the future design of empirical studies exploring human-AI collaboration."
40,2023,"Digital capability requirements and improvement strategies: Organizational socialization of AI teammates nan Artificial Intelligence (AI) can enter organizations and become AI teammates in organizational teams, posing new challenges to organizational form, team management, and team working patterns. Establishing an efficient human-AI team requires an organization and its members to have superior digital capabilities and develop effective capability improvement strategies to realize the organizational socialization process of AI. In this paper, we adopt the Design Research (DR) approach in building a Human-AI Collaboration Platform (HACP) in a work team, integrating AI teammates based on HACP, and optimizing the platform and human-AI collaboration. Considering organizational digital capabilities as one of the core components, we propose a threelevel framework of digitalization capabilities for individuals, organizations, and across organizations, as well as the enhancement strategies of such capabilities. Besides, we clarify the foundation of digitalization capabilities and design, implement, and verify the enhancement strategies. The construction and successful implementation of the HACP in our study verify the effectiveness of the proposed capability framework and provide principle guidance for organizations to implement effective strategies to improve digital capabilities to facilitate the organizational socialization of AI teammates."
41,2020,"Modelling Hybrid Human-Artificial Intelligence Cooperation: A Call Center Customer Service Case Study nan As autonomous systems become an essential part of augmented decision-making in the workforce, we have the opportunity to change the relationship between human and machine into a more collaborative one. The future of industry, commercial and public services point in a direction where humans and artificial intelligence (AI) increasingly work together. AI systems are increasingly extending and enriching decision support by complementing and augmenting human capabilities. To further elevate this partnership, we need to form organic human-AI teams that communicate with, adapt to, and learn from each other. We create a new human-in-the-loop hybrid spectrum, that expands existing definitions of human and machine teaming. For a given situation and a team of humans and AI systems, we are interested in testing variations on human-AI cooperation outcomes. We examine a call center use case to determine how variations in human and machine teaming affects average handle time and response quality outputs that affect customer service. We have evaluated three scenarios: 1) human-only, 2) AI-only, and 3) human + AI collaboration. Under the parameter space we studied, we found that human + AI collaboration is optimal."
42,2024,"Towards Human-AI Teaming to Mitigate Alert Fatigue in Security Operations Centres nan Security Operations Centres (SOCs) play a pivotal role in defending organisations against evolving cyber threats. They function as central hubs for detecting, analysing, and responding promptly to cyber incidents with the primary objective of ensuring the confidentiality, integrity, and availability of digital assets. However, they struggle against the growing problem of alert fatigue, where the sheer volume of alerts overwhelms SOC analysts and raises the risk of overlooking critical threats. In recent times, there has been a growing call for human-AI teaming, wherein humans and AI collaborate with each other, leveraging their complementary strengths and compensating for their weaknesses. The rapid advances in AI and the growing integration of AI-enabled tools and technologies within SOCs give rise to a compelling argument for the implementation of human-AI teaming within the SOC environment. Therefore, in this article, we present our vision for human-AI teaming to address the problem of alert fatigue in the SOC. We propose the A(2) C Framework, which enables flexible and dynamic decision making by allowing seamless transitions between automated, augmented, and collaborative modes of operation. Our framework allows AI-powered automation for routine alerts, AI-driven augmentation for expedited expert decision making, and collaborative exploration for tackling complex, novel threats. By implementing and operationalising A(2)C, SOCs can significantly reduce alert fatigue while empowering analysts to efficiently and effectively respond to security incidents."
43,2024,"A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI Teams [arXiv] nan This paper introduces A2C, a multi-stage collaborative decision framework designed to enable robust decision-making within human-AI teams. Drawing inspiration from concepts such as rejection learning and learning to defer, A2C incorporates AI systems trained to recognise uncertainty in their decisions and defer to human experts when needed. Moreover, A2C caters to scenarios where even human experts encounter limitations, such as in incident detection and response in cyber Security Operations Centres (SOC). In such scenarios, A2C facilitates collaborative explorations, enabling collective resolution of complex challenges. With support for three distinct decision-making modes in human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible platform for developing effective strategies for human-AI collaboration. By harnessing the strengths of both humans and AI, it significantly improves the efficiency and effectiveness of complex decision-making in dynamic and evolving environments. To validate A2C's capabilities, we conducted extensive simulative experiments using benchmark datasets. The results clearly demonstrate that all three modes of decision-making can be effectively supported by A2C. Most notably, collaborative exploration by (simulated) human experts and AI achieves superior performance compared to AI in isolation, underscoring the framework's potential to enhance decision-making within human-AI teams."
44,2024,"On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration [arXiv] nan The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems."
45,2024,"The AI Collaborator: Bridging Human-AI Interaction in Educational and Professional Settings nan AI Collaborator, powered by OpenAI's GPT-4, is a groundbreaking tool designed for human-AI collaboration research. Its standout feature is the ability for researchers to create customized AI personas for diverse experimental setups using a user-friendly interface. This functionality is essential for simulating various interpersonal dynamics in team settings. AI Collaborator excels in mimicking different team behaviors, enabled by its advanced memory system and a sophisticated personality framework. Researchers can tailor AI personas along a spectrum from dominant to cooperative, enhancing the study of their impact on team processes. The tool's modular design facilitates integration with digital platforms like Slack, making it versatile for various research scenarios. AI Collaborator is thus a crucial resource for exploring human-AI team dynamics more profoundly."
46,2024,"CREW: Facilitating Human-AI Teaming Research nan With the increasing deployment of artificial intelligence (AI) technologies, the potential of humans working with AI agents has been growing at a great speed. Human-AI teaming is an important paradigm for studying various aspects when humans and AI agents work together. The unique aspect of Human-AI teaming research is the need to jointly study humans and AI agents, demanding multidisciplinary research efforts from machine learning to human-computer interaction, robotics, cognitive science, neuroscience, psychology, social science, and complex systems. However, existing platforms for Human-AI teaming research are limited, often supporting oversimplified scenarios and a single task, or specifically focusing on either human-teaming research or multi-agent AI algorithms. We introduce CREW, a platform to facilitate Human-AI teaming research and engage collaborations from multiple scientific disciplines, with a strong emphasis on human involvement. It includes pre-built tasks for cognitive studies and Human-AI teaming with expandable potentials from our modular design. Following conventional cognitive neuroscience research, CREW also supports multimodal human physiological signal recording for behavior analysis. Moreover, CREW benchmarks real-time human-guided reinforcement learning agents using state-of-the-art algorithms and well-tuned baselines. With CREW, we were able to conduct 50 human subject studies within a week to verify the effectiveness of our benchmark."
47,2021,"Trustworthy human-AI partnerships nan In this paper, we foreground some of the key research challenges that arise in the design of trustworthy human-AI partnerships. In particular, we focus on the challenges in designing human-AI partnerships that need to be addressed to help humans and organizations trust their machine counterparts individually or as a collective (e.g., as robot teams or groups of software agents). We also aim to identify the risks associatedwith human-AI partnerships and therefore determine the associated measures to mitigate these risks. By so doing, we will trigger new avenues of research that will address the key barriers to the adoption of AI-based systems more widely in our daily lives and in industry."
48,2024,"How Being Outvoted by AI Teammates Impacts Human-AI Collaboration nan Recent advances in artificial intelligence (AI) enable AI agents to go beyond simply supporting human activities and, instead, take more control in team decision-making. While significant literature has studied human-AI collaboration through the lens of AI as a second opinion system, this type of interaction is not fully representative of many human-human team collaboration scenarios, such as scenarios where each decision maker is granted equal voting rights for the team decision. In this research, we explore how imparting AI agents with equal voting rights to the human impacts human-AI decision-making and team performance. Using a human subjects experiment in which participants collaborate with two AI teammates for truss structure (aka, bridge) design, we manipulate a series of voting scenarios (e.g., AI agents outvoting the human vs. AI agents agreeing with the human) and AI performance levels (high vs. low performing). The results indicate that changes in human self-confidence are not consistent with whether the quality of the final team-voted design action is advantageous or disadvantageous relative to their own actions. The results also show that when humans are outvoted by their AI teammates, they do not show strong negative emotional reactions if the team-voted decision has an advantageous outcome. Additionally, AI performance significantly influences the human-AI team decision-making process and even one low-performing AI (i.e., an AI that is frequently incorrect) on the team can significantly deteriorate team performance. Taken together, this research provides empirical evidence on the effects of AI voting with equal decision authority on human-AI collaboration, as well as valuable insights supporting real-world applications of human-AI collaboration via voting."
49,2023,Do we Need to Trust Our Ai-Teammate to Perform? Investigating the Mediating Role of Team Trust in the Relationship Between Team Composition and Team Performance nan nan
50,2024,"The Impact of Imperfect XAI on Human-AI Decision-Making nan Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility of the explanations being incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making behavior in a bird species identification task, taking into account their level of expertise and an explanation's level of assertiveness. Our findings reveal the influence of imperfect XAI and humans' level of expertise on their reliance on AI and human-AI team performance. We also discuss how explanations can deceive decision-makers during human-AI collaboration. Hence, we shed light on the impacts of imperfect XAI in the field of computer-supported cooperative work and provide guidelines for designers of human-AI collaboration systems."
51,2024,"Towards Ethical AI: Empirically Investigating Dimensions of AI Ethics, Trust Repair, and Performance in Human-AI Teaming nan Objective Determining the efficacy of two trust repair strategies (apology and denial) for trust violations of an ethical nature by an autonomous teammate. Background While ethics in human-AI interaction is extensively studied, little research has investigated how decisions with ethical implications impact trust and performance within human-AI teams and their subsequent repair. Method Forty teams of two participants and one autonomous teammate completed three team missions within a synthetic task environment. The autonomous teammate made an ethical or unethical action during each mission, followed by an apology or denial. Measures of individual team trust, autonomous teammate trust, human teammate trust, perceived autonomous teammate ethicality, and team performance were taken. Results Teams with unethical autonomous teammates had significantly lower trust in the team and trust in the autonomous teammate. Unethical autonomous teammates were also perceived as substantially more unethical. Neither trust repair strategy effectively restored trust after an ethical violation, and autonomous teammate ethicality was not related to the team score, but unethical autonomous teammates did have shorter times. Conclusion Ethical violations significantly harm trust in the overall team and autonomous teammate but do not negatively impact team score. However, current trust repair strategies like apologies and denials appear ineffective in restoring trust after this type of violation. Application This research highlights the need to develop trust repair strategies specific to human-AI teams and trust violations of an ethical nature."
52,2023,"Human-AI team halves cost of designing step nan Engineers and algorithms have competed in a virtual test to design a step in the process of manufacturing computer chips. Pairing human expertise with computational efficiency proves most cost-effective, but only when the timing is right."
53,2024,"The Role of Autonomy Levels and Contextual Risk in Designing Safer AI Teammates nan As AI becomes more intelligent and autonomous, the concept of human-AI teaming has become more realistic and attractive. Despite the promises of AI teammates, human-AI teams face new, unique challenges. One such challenge is the declining ability of human team members to detect and respond to AI failures as they become further removed from the AI's decision-making loop. In this study, we conducted virtual experiments with twelve experts in two different teaming contexts, cyber incident response and medical triage, to understand how contextual risk impacts human teammate situational awareness and failure performance over a human-AI team's action cycle. Our results indicate that situational awareness is more closely tied to context, while failure performance is more closely tied to the team's action cycle. These results provide the foundation for future research into using contextual risk in determining optimal autonomy levels for AI teammates."
54,2022,"How transparency modulates trust in artificial intelligence. nan The study of human-machine systems is central to a variety of behavioral and engineering disciplines, including management science, human factors, robotics, and human-computer interaction. Recent advances in artificial intelligence (AI) and machine learning have brought the study of human-AI teams into sharper focus. An important set of questions for those designing human-AI interfaces concerns trust, transparency, and error tolerance. Here, we review the emerging literature on this important topic, identify open questions, and discuss some of the pitfalls of human-AI team research. We present opposition (extreme algorithm aversion or distrust) and loafing (extreme automation complacency or bias) as lying at opposite ends of a spectrum, with algorithmic vigilance representing an ideal mid-point. We suggest that, while transparency may be crucial for facilitating appropriate levels of trust in AI and thus for counteracting aversive behaviors and promoting vigilance, transparency should not be conceived solely in terms of the explainability of an algorithm. Dynamic task allocation, as well as the communication of confidence and performance metrics-among other strategies-may ultimately prove more useful to users than explanations from algorithms and significantly more effective in promoting vigilance. We further suggest that, while both aversive and appreciative attitudes are detrimental to optimal human-AI team performance, strategies to curb aversion are likely to be more important in the longer term than those attempting to mitigate appreciation. Our wider aim is to channel disparate efforts in human-AI team research into a common framework and to draw attention to the ecological validity of results in this field."
55,2023,"Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams nan The concept of a Human-AI team has gained increasing attention in recent years. For effective collaboration between humans and AI teammates, proactivity is crucial for close coordination and effective communication. However, the design of adequate proactivity for AI-based systems to support humans is still an open question and a challenging topic. In this paper, we present the development of a corpus-based user simulator for training and testing proactive dialog policies. The simulator incorporates informed knowledge about proactive dialog and its effect on user trust and simulates user behavior and personal information, including socio-demographic features and personality traits. Two different simulation approaches were compared, and a task-step-based approach yielded better overall results due to enhanced modeling of sequential dependencies. This research presents a promising avenue for exploring and evaluating appropriate proactive strategies in a dialog game setting for improving Human-AI teams."
56,2021,A Framework for Assessing and Designing Human Annotation Practices in Human-Ai Teaming nan nan
57,2023,Human-AI team halves cost of chip-design step nan nan
58,2023,"Examining the impact of varying levels of AI teammate influence on human-AI teams nan The implementation of AI teammates is creating a wealth of research that examines how AI teammates impact human-AI teams. However, AI teammates themselves are not static, and their roles and responsibilities in human-AI teams are likely to change as technologies advance in the coming years. As a result of this advancement, AI teammates will gain influence in teams, which refers to their ability to change and manipulate a team's shared resources. This study uses a mixed-methods experiment to examine how the amount of influence AI teammates have on a team's shared resources can impact the team outcomes of human teammate performance, teammate perceptions, and whole-team perception. Results indicate that AI teammates that increase their influence on shared resources over time can stagnate the improvement of human performance, but AI teammates that decrease their influence on shared resources can actually encourage humans to improve their own performance. Additionally, AI teammates that are highly influential on shared resources can make humans perceive a greater cognitive workload. However, qualitative results indicate that these impacts on human performance and perception do not consistently impact the acceptance humans form for AI teammates. Rather, humans form acceptance for AI teammates if said AI use its influence to manipulate resources to benefit the personal goals of human teammates. These results have critical implications for human-AI teaming as it shows that the influence AI teammates have on shared resources can be designed in a way that improves human performance. However, future research is going to need to focus more critically on how the personal goals humans have, which may not align with a team's overall goals, are going to mediate the effectiveness of the AI teammate influence."
59,2022,"The Role of Adaptation in Collective Human-AI Teaming. nan This paper explores a framework for defining artificial intelligence (AI) that adapts to individuals within a group, and discusses the technical challenges for collaborative AI systems that must work with different human partners. Collaborative AI is not one-size-fits-all, and thus AI systems must tune their output based on each human partner's needs and abilities. For example, when communicating with a partner, an AI should consider how prepared their partner is to receive and correctly interpret the information they are receiving. Forgoing such individual considerations may adversely impact the partner's mental state and proficiency. On the other hand, successfully adapting to each person's (or team member's) behavior and abilities can yield performance benefits for the human-AI team. Under this framework, an AI teammate adapts to human partners by first learning components of the human's decision-making process and then updating its own behaviors to positively influence the ongoing collaboration. This paper explains the role of this AI adaptation formalism in dyadic human-AI interactions and examines its application through a case study in a simulated navigationdomain."
60,2023,"Learning Complementary Policies for Human-AI Teams [arXiv] nan Human-AI complementarity is important when neither the algorithm nor the human yields dominant performance across all instances in a given context. Recent work that explored human-AI collaboration has considered decisions that correspond to classification tasks. However, in many important contexts where humans can benefit from AI complementarity, humans undertake course of action. In this paper, we propose a framework for a novel human-AI collaboration for selecting advantageous course of action, which we refer to as Learning Complementary Policy for Human-AI teams ( LCP-HAI). Our solution aims to exploit the human-AI complementarity to maximize decision rewards by learning both an algorithmic policy that aims to complement humans by a routing model that defers decisions to either a human or the AI to leverage the resulting complementarity. We then extend our approach to leverage opportunities and mitigate risks that arise in important contexts in practice: 1) when a team is composed of multiple humans with differential and potentially complementary abilities, 2) when the observational data includes consistent deterministic actions, and 3) when the covariate distribution of future decisions differ from that in the historical data. We demonstrate the effectiveness of our proposed methods using data on real human responses and semi-synthetic, and find that our methods offer reliable and advantageous performance across setting, and that it is superior to when either the algorithm or the AI make decisions on their own. We also find that the extensions we propose effectively improve the robustness of the human-AI collaboration performance in the presence of different challenging settings."
61,2023,"Effective human-AI work design for collaborative decision-making nan Purpose With the increase in the adoption of artificial intelligence (AI)-based decision-making, organizations are facilitating human-AI collaboration. This collaboration can occur in a variety of configurations with the division of labor, with differences in the nature of interdependence being parallel or sequential, along with or without the presence of specialization. This study intends to explore the extent to which humans express comfort with different models human-AI collaboration. Design/methodology/approach Situational response surveys were adopted to identify configurations where humans experience the greatest trust, role clarity and preferred feedback style. Regression analysis was used to analyze the results. Findings Some configurations contribute to greater trust and role clarity with AI as a colleague. There is no configuration in which AI as a colleague produces lower trust than humans. At the same time, the human distrust in AI may be less about human vs AI and more about the division of labor in which human-AI work. Practical implications The study explores the extent to which humans express comfort with different models of an algorithm as partners. It focuses on work design and the division of labor between humans and AI. The finding of the study emphasizes the role of work design in human-AI collaboration. There is human-AI work design that should be avoided as they reduce trust. Organizations need to be cautious in considering the impact of design on building trust and gaining acceptance with technology. Originality/value The paper's originality lies in focusing on the design of collaboration rather than on performance of the team."
62,2024,"An Evaluation of Situational Autonomy for Human-AI Collaboration in a Shared Workspace Seting nan Designing interactions for human-AI teams (HATs) can be challenging due to an AI agent's potential autonomy. Previous work suggests that higher autonomy does not always improve team performance, and situation-dependent autonomy adaptation might be beneficial. However, there is a lack of systematic empirical evaluations of such autonomy adaptation in human-AI interaction. Therefore, we propose a cooperative task in a simulated shared workspace to investigate effects of fixed levels of AI autonomy and situation-dependent autonomy adaptation on team performance and user satisfaction. We derive adaptation rules for AI autonomy from previous work and a pilot study. We implement these rule for our main experiment and find that team performance was best when humans collaborated with an agent adjusting its autonomy based on the situation. Additionally, users rated this agent highest in terms of perceived intelligence. From these results, we discuss the influence of varying autonomy degrees on HATs in shared workspaces."
63,2022,"Remote research methods for Human-AI-Robot Teaming nan This study focuses on methodological adaptations and considerations for remote research on Human-AI-Robot Teaming (HART) amidst the COVID-19 pandemic. Themes and effective remote research methods were explored. Central issues in remote research were identified, such as challenges in attending to participants' experiences, coordinating experimenter teams remotely, and protecting privacy and confidentiality. Instances of experimental design overcoming these challenges were identified in methods for recruitment and onboarding, training, team task scenarios, and measurement. Three case studies are presented in which interactive in-person testbeds for HART were rapidly redesigned to function remotely. Although COVID-19 may have temporarily constrained experimental design, future HART studies may adopt remote research methods to expand the research toolkit."
64,2024,"When Should I Lead or Follow: Understanding Initiative Levels in Human-AI Collaborative Gameplay nan Dynamics in Human-AI interaction should lead to more satisfying and engaging collaboration. Key open questions are how to design such interactions and the role personal goals and expectations play. We developed three AI partners of varying initiative (leader, follower, shifting) in a collaborative game called Geometry Friends. We conducted a within-subjects experiment with 60 participants to assess personal AI partner preference and performance satisfaction as well as perceived warmth and competence of AI partners. Results show that AI partners following human initiative are perceived as warmer and more collaborative. However, some participants preferred AI leaders for their independence and speed, despite being seen as less friendly. This suggests that assigning a leadership role to the AI partner may be suitable for time-sensitive scenarios. We identify design factors for developing collaborative AI agents with varying levels of initiative to create more effective human-AI teams that consider context and individual preference."
65,2024,"A Study of Human-AI Symbiosis for Creative Work: Recent Developments and Future Directions in Deep Learning nan Recent advances in Artificial Intelligence (AI), particularly deep learning, are having an enormous impact on our society today. Record numbers of jobs previously held by people have been automated, from manufacturing to transportation to customer services. The concerns of AI replacing humans by taking over people's jobs need to be urgently addressed. This article investigates some promising different directions of AI development: Instead of using AI to replace people, we should use AI to team up with people so that both can work better and smarter. Human-AI symbiosis refers to people and AI working together to jointly solve problems and perform specific tasks. The recent developments in deep learning models and frameworks have significantly improved the efficiency and performance of human and AI collaborations. In this article, some research work on human-AI collaborative environments has been extensively studied and analyzed to reveal the progress in this field. Although the teaming of humans and machines includes many complex tasks, the development has been very promising. One of the main goals in this field is to develop additional capabilities in machines capable of being successful teammates with a human partner. The correctness of the outcomes is often determined by the underlying technology and how performance and human satisfaction are measured through the collaborative nature of the system. We conclude that the teaming of humans and AI, particularly deep learning, has the advantage of combining the power of AI with the human domain expertise to improve performance and create value. Human-AI symbiosis could be a promising future direction for AI's continuing integration into the world."
66,2022,"Predictive models for human-AI nexus in group decision making nan Machine learning (ML) and artificial intelligence (AI) have had a profound impact on our lives. Domains like health and learning are naturally helped by human-AI interactions and decision making. In these areas, as ML algorithms prove their value in making important decisions, humans add their distinctive expertise and judgment on social and interpersonal issues that need to be considered in tandem with algorithmic inputs of information. Some questions naturally arise. What rules and regulations should be invoked on the employment of AI, and what protocols should be in place to evaluate available AI resources? What are the forms of effective communication and coordination with AI that best promote effective human-AI teamwork? In this review, we highlight factors that we believe are especially important in assembling and managing human-AI decision making in a group setting."
67,2023,"Towards Responsible AI: Developing Explanations to Increase Human-AI Collaboration nan Most current XAI models are primarily designed to verify input-output relationships of AI models, without considering context. This objective may not always align with the goals of Human-AI collaboration, which aim to enhance team performance and establish appropriate levels of trust. Developing XAI models that can promote justified trust is therefore still a challenge in the AI field, but it is a crucial step towards responsible AI. The focus of this research is to develop an XAI model optimized for human-AI collaboration, with a specific goal of generating explanations that improve understanding of the AI system's limitations and increase warranted trust in it. To achieve this goal, a user experiment was conducted to analyze the effects of including explanations in the decision-making process on AI trust."
68,2023,"ADAPTATION AND CHALLENGES IN HUMAN-AI PARTNERSHIP FOR THE DESIGN OF COMPLEX ENGINEERING SYSTEMS nan Exploring the opportunities for incorporating Artificial Intelligence (AI) to support team problem solving has been the focus of intensive ongoing research. However, while the incorporation of such AI tools into human team problem solving can improve team performance, it is still unclear what modality of AI integration will lead to a genuine human-AI partnership capable of mimicking the dynamic adaptability of humans. This work unites human designers with AI Partners as fellow team members who can both reactively and proactively collaborate in real-time towards solving a complex and evolving engineering problem. Team performance and problem-solving behaviors are examined using the HyForm collaborative research platform. The problem constraints are unexpectedly changed midway through problem solving to simulate the nature of dynamically evolving engineering problems. This work shows that after the shock is introduced, human-AI hybrid teams perform similarly to human teams, demonstrating the capability of AI Partners to adapt to unexpected events. Nonetheless, hybrid teams do struggle more with coordination and communication after the shock is introduced. Overall, this work demonstrates that these AI design Partners can participate as active partners within human teams during a large, complex task, showing promise for future integration in practice."
69,2023,"Developing human/AI interactions for chat-based customer services: lessons learned from the Norwegian government nan Advancements in human/AI interactions led to smartification of public services via the use of chatbots. Here, we present findings from a clinical inquiry research project in a key public service organisation in Norway. In this project, researchers and practitioners worked together to generate insights on the action possibilities offered to human service agents by chatbots and the potential for creating hybrid human/AI service teams. The project sensitised service agents to discover affordances based on their actual practices, rather than on the predefined use of chatbots. The different affordances identified can be useful for practitioners who design and deploy chatbot-based services. The action possibilities afforded by chatbots provide new ways for service agents and chatbots to work as a team addressing citizens' needs. Drawing from the whole research process, we offer three lessons learned from the Norwegian Government on human/AI partnerships, theory-based interventions, and institutionalised collaborative research that can be useful for researchers that want to engage with practice and organisations that want to evolve their technology use, stimulate innovation, and engage with research."
70,2024,"Human-AI Teaming in Critical Care: A Comparative Analysis of Data Scientists' and Clinicians' Perspectives on AI Augmentation and Automation nan Background: Artificial intelligence (AI) holds immense potential for enhancing clinical and administrative health care tasks. However, slow adoption and implementation challenges highlight the need to consider how humans can effectively collaborate with AI within broader socio-technical systems in health care. Objective: In the example of intensive care units (ICUs), we compare data scientists' and clinicians' assessments of the optimal utilization of human and AI capabilities by determining suitable levels of human-AI teaming for safely and meaningfully augmenting or automating 6 core tasks. The goal is to provide actionable recommendations for policy makers and health care practitioners regarding AI design and implementation. Methods: In this multimethod study, we combine a systematic task analysis across 6 ICUs with an international Delphi survey involving 19 health data scientists from the industry and academia and 61 ICU clinicians (25 physicians and 36 nurses) to define and assess optimal levels of human-AI teaming (level 1=no performance benefits; level 2=AI augments human performance; level 3=humans augment AI performance; level 4=AI performs without human input). Stakeholder groups also considered ethical and social implications. Results: Both stakeholder groups chose level 2 and 3 human-AI teaming for 4 out of 6 core tasks in the ICU. For one task (monitoring), level 4 was the preferred design choice. For the task of patient interactions, both data scientists and clinicians agreed that AI should not be used regardless of technological feasibility due to the importance of the physician-patient and nurse-patient relationship and ethical concerns. Human-AI design choices rely on interpretability, predictability, and control over AI systems. If these conditions are not met and AI performs below human-level reliability, a reduction to level 1 or shifting accountability away from human end users is advised. If AI performs at or beyond human-level reliability and these conditions are not met, shifting to level 4 automation should be considered to ensure safe and efficient human-AI teaming. Conclusions: By considering the sociotechnical system and determining appropriate levels of human-AI teaming, our study showcases the potential for improving the safety and effectiveness of AI usage in ICUs and broader health care settings. Regulatory measures should prioritize interpretability, predictability, and control if clinicians hold full accountability. Ethical and social implications must be carefully evaluated to ensure effective collaboration between humans and AI, particularly considering the most recent advancements in generative AI."
71,2021,"Designing for Bi-directional Transparency in Human-AI-Robot-Teaming nan This paper takes a practitioner's perspective on advancing bi-directional transparency in human-AI-robot teams (HARTs). Bi-directional transparency is important for HARTs because the better that people and artificially intelligent agents can understand one another's capabilities, limits, inputs, outputs and contexts in a given task environment; the better they can work as a team to accomplish shared goals, interdependent tasks, and overall missions. This understanding can be built, augmented, broken and repaired at various stages across the technology life cycle, including the conceptual design; iterative design of software, hardware and interfaces; marketing and sales; system training; operational use; and system updating and adaptation stages. This paper provides an overview of some best practices and challenges in building this bi-directional transparency at different points in the technology life cycle of human-AI-robot systems. The goal is to help advance a wider discussion and sharing of lessons learned from recent work in this area."
72,2024,"On the Utility of External Agent Intention Predictor for Human-AI Coordination nan Reaching a consensus on the team plans is vital to human-AI coordination. Although previous studies provide approaches through communications in various ways, it could still be hard to coordinate when the AI has no explainable plan to communicate. To cover this gap, we suggest incorporating external models to assist humans in understanding the intentions of AI agents. In this paper, we propose a two-stage paradigm that first trains a Theory of Mind (ToM) model from collected offline trajectories of the target agent, and utilizes the model in the process of human-AI collaboration by real-timely displaying the future action predictions of the target agent. Such a paradigm leaves the AI agent as a black box and thus is available for improving any agents. To test our paradigm, we further implement a transformer-based predictor as the ToM model and develop an extended online human-AI collaboration platform for experiments. The comprehensive experimental results verify that human-AI teams can achieve better performance with the help of our model. A user assessment attached to the experiment further demonstrates that our paradigm can significantly enhance the situational awareness of humans. Our study presents the potential to augment the ability of humans via external assistance in human-AI collaboration, which may further inspire future research."
73,2023,"Confounding-Robust Policy Improvement with Human-AI Teams [arXiv] nan Human-AI collaboration has the potential to transform various domains by leveraging the complementary strengths of human experts and Artificial Intelligence (AI) systems. However, unobserved confounding can undermine the effectiveness of this collaboration, leading to biased and unreliable outcomes. In this paper, we propose a novel solution to address unobserved confounding in human-AI collaboration by employing the marginal sensitivity model (MSM). Our approach combines domain expertise with AI-driven statistical modeling to account for potential confounders that may otherwise remain hidden. We present a deferral collaboration framework for incorporating the MSM into policy learning from observational data, enabling the system to control for the influence of unobserved confounding factors. In addition, we propose a personalized deferral collaboration system to leverage the diverse expertise of different human decision-makers. By adjusting for potential biases, our proposed solution enhances the robustness and reliability of collaborative outcomes. The empirical and theoretical analyses demonstrate the efficacy of our approach in mitigating unobserved confounding and improving the overall performance of human-AI collaborations."
74,2024,"Modeling perceived information needs in human-AI teams: improving AI teammate utility and driving team cognition nan As AI technologies advance, teams are beginning to see AI transition from a tool to a full-fledged teammate. Introducing an AI teammate brings several challenges, ranging from how human teammates perceive their new AI teammates from an affective standpoint to how AI should engage in the various teaming behaviors that make up effective teamwork. The current study used a mixed factorial survey and structural equation modeling to assess how participants in hypothetical human-AI teams respond to various forms of AI information-sharing, including information related to explainability, back-up behavior, situational awareness, and augmenting team memory. The study's results found that AI design features related to situational awareness and augmenting the teams' memory had the strongest effect on participants' attitudes and perceived team cognition with their teammates. However, much of this effect was mediated by participants' affective attitudes towards the AI as a teammate, with higher ratings leading directly to higher levels of perceived team cognition constructs. These results highlight the importance of fostering positive attitudes towards AI teammates, such as trust and cohesion in human-AI teams, to support the development of effective team cognition and the ability of AI information-sharing to bring about such positive impacts."
75,2024,"Human Systems Integration of Human-AI Teaming nan Nowadays, human systems integration (HSI) requires expansion, considering the inclusion of artificial intelligence (AI) in most critical systems. Consequently, systems engineering and AI must be developed in concert with the new perspective of human-AI teaming (HAT). This article attacks this endeavor by considering human factors such as situation awareness, decision-making, and risk-taking. It raises issues of function allocation, design and operations flexibility, and incremental design of technology, organizations, and people's competencies. More specifically, it brings the major issue of certification impossibility and the need for qualification of AI systems, shifting these systems from tools to partners."
76,2023,"Transdisciplinary Team Science: Transcending Disciplines to Understand Artificial Social Intelligence in Human-Agent Teaming nan We provide a transdisciplinary viewpoint on creating artificial social intelligence for human-agent teaming. We discuss theoretical, methodological, and technological insights, drawn from different disciplines, to more fully illuminate how cross-disciplinary research can inform research design and development. We unite ideas spanning human factors, cognitive and computer science, and organizational behavior. Grounding our ideas in real world challenges for human-AI teaming, and via a series of questions designed to facilitate synthesis across disciplines, we illustrate how transdisciplinary team science more effectively asks and answers complex questions on human-agent teaming. Our objective is to contribute to research and development in the field of human-AI and human-robot teaming by emphasizing a more human-centered perspective on AI."
77,2023,"Human-AI Collaboration: The Effect of AI Delegation on Human Task Performance and Task Satisfaction nan Recent work has proposed artificial intelligence (AI) models that can learn to decide whether to make a prediction for an instance of a task or to delegate it to a human by considering both parties' capabilities. In simulations with synthetically generated or context-independent human predictions, delegation can help improve the performance of human-AI teams-compared to humans or the AI model completing the task alone. However, so far, it remains unclear how humans perform and how they perceive the task when they are aware that an AI model delegated task instances to them. In an experimental study with 196 participants, we show that task performance and task satisfaction improve through AI delegation, regardless of whether humans are aware of the delegation. Additionally, we identify humans' increased levels of self-efficacy as the underlying mechanism for these improvements in performance and satisfaction. Our findings provide initial evidence that allowing AI models to take over more management responsibilities can be an effective form of human-AI collaboration in workplaces."
78,2019,"Trust Engineering for Human-AI Teams nan Human-AI teaming refers to systems in which humans and artificial intelligence (AI) agents collaborate to provide significant mission performance improvements over that which humans or AI can achieve alone. The goal is faster and more accurate decision-making by integrating the rapid data ingest, learning, and analyses capabilities of AI with the creative problem solving and abstraction capabilities of humans. The purpose of this panel is to discuss research directions in Trust Engineering for building appropriate bi-directional trust between humans and AI. Discussions focus on the challenges in systems that are increasingly complex and work within imperfect information environments. Panelists provide their perspectives on addressing these challenges through concepts such as dynamic relationship management, adaptive systems, co-discovery learning, and algorithmic transparency. Mission scenarios in command and control (C2), piloting, cybersecurity, and criminal intelligence analysis demonstrate the importance of bi-directional trust in human-AI teams."
79,2023,"AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity [arXiv] nan This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., Human-AI teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-generated decisions to stakeholders, regulators, and end-users in critical situations, ensuring responsibility and accountability, which helps establish trust in AI-driven security solutions. Therefore, in this position paper, we argue that human-AI teaming is worthwhile in cybersecurity, in which human expertise such as intuition, critical thinking, or contextual understanding is combined with AI's computational power to improve overall cyber defenses."
80,2022,"A Study of Drone-based AI for Enhanced Human-AI Trust and Informed Decision Making in Human-AI Interactive Virtual Environments nan Search and Rescue Operations (SRO) are notoriously difficult as they typically involve human operations in high risk and low visibility environments. Often, stakeholders only have a general perception of the possible adversities in the situational environment. Ultimately, the success of these operations is a function of the manpower available, the terrain of the region, informed-decision-making based on terrain mapping and objective success in completion of search and rescue tasks with lower human casualties. A practical solution to this problem is to leverage the use of autonomous systems such as drones and rescue robots that can scout the terrain to gather information to augment the rescue team's capabilities and mission success rates. In situations, such as a combat search and rescue mission, the mission might call for a cooperative effort between a Human and AI Agent, whereby both are able to share intelligence and coordinate in decision-making tasks. In this work, we present several novel contributions through a combat search and rescue simulation scenario that leverages a drone-based AI autonomous system for detection of targets-of-interest in the environment as a basis for human-AI teaming study. In this research, we examine various human factor metrics for different modes of interactions between the human agent and AI-driven drone/autonomous system agent to include implications on human mission completion with and without drone-based AI target detection-derived human situational awareness and time to mission completion. In addition, we introduce innovative AI techniques to model human agent (player) - AI agent (drone) exchanges through a hostage rescue scenario-based simulation and explore incentive strategies directed towards the human agent to encourage adoption of AI-based autonomous system as a cooperative intelligence asset and improve human-AI teaming performance. The unification of both AI techniques to model Human-AI interaction and incentive mechanisms to encourage usage of autonomous systems sets the foundation for assessing the efficacy of AI in Human Agent Teams."
81,2023,"Empowering human-AI teams via Intentional Behavioral Synchrony. nan As Artificial Intelligence (AI) proliferates across various sectors such as healthcare, transportation, energy, and military applications, the collaboration between human-AI teams is becoming increasingly critical. Understanding the interrelationships between system elements - humans and AI - is vital to achieving the best outcomes within individual team members' capabilities. This is also crucial in designing better AI algorithms and finding favored scenarios for joint AI-human missions that capitalize on the unique capabilities of both elements. In this conceptual study, we introduce Intentional Behavioral Synchrony (IBS) as a synchronization mechanism between humans and AI to set up a trusting relationship without compromising mission goals. IBS aims to create a sense of similarity between AI decisions and human expectations, drawing on psychological concepts that can be integrated into AI algorithms. We also discuss the potential of using multimodal fusion to set up a feedback loop between the two partners. Our aim with this work is to start a research trend centered on exploring innovative ways of deploying synchrony between teams of non-human members. Our goal is to foster a better sense of collaboration and trust between humans and AI, resulting in more effective joint missions."
82,2024,"The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features nan AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments-explanations, model bias disclosure and proxy correlation disclosure-affect fairness perception and parity. We find that explanations help people detect direct but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making."
83,2023,"Evolving Lvc To Include Evaluation Of Human-Ai Teaming Dynamics nan There are significant differences between using systems as human-controlled tools to accomplish a specific task and using systems designed to cooperate and partner with humans to achieve capabilities beyond either side acting alone. The live, virtual, constructive (LVC) paradigm increasingly emphasized by the DoD has wide acceptance and is congruent with how the military thinks about training, evaluation, and mission rehearsal. Consequently, it may help address these challenges. This paper aims to overview the current LVC construct, challenges associated with human-AI teaming and intentional design of these dynamics to achieve new capabilities, and the resulting need to evolve the LVC construct to improve our pursuit of understanding and evaluation that leads to effective fielding."
84,2024,"Understanding the influence of AI autonomy on AI explainability levels in human-AI teams using a mixed methods approach nan An obstacle to effective teaming between humans and AI is the agent's black box design. AI explanations have proven benefits, but few studies have explored the effects that explanations can have in a teaming environment with AI agents operating at heightened levels of autonomy. We conducted two complementary studies, an experiment and participatory design sessions, investigating the effect that varying levels of AI explainability and AI autonomy have on the participants' perceived trust and competence of an AI teammate to address this research gap. The results of the experiment were counter-intuitive, where the participants actually perceived the lower explainability agent as both more trustworthy and more competent. The participatory design sessions further revealed how a team's need to know influences when and what teammates need explained from AI teammates. Based on these findings, several design recommendations were developed for the HCI community to guide how AI teammates should share decision information with their human counterparts considering the careful balance between trust and competence in human-AI teams."
85,2023,"Appropriate Reliance on Artificial Intelligence in Radiology Education nan Users of artificial intelligence (AI) can become overreliant on AI, negatively affecting the performance of human-AI teams. For a future in which radiologists use interpretive AI tools routinely in clinical practice, radiology education will need to evolve to provide radiologists with the skills to use AI appropriately and wisely. In this work, we examine how overreliance on AI may develop in radiology trainees and explore how this problem can be mitigated, including through the use of AI-augmented education. Radiology trainees will still need to develop the perceptual skills and mastery of knowledge fundamental to radiology to use AI safely. We propose a framework for radiology trainees to use AI tools with appropriate reliance, drawing on lessons from human-AI interactions research."
86,2022,"Workshop on Trust and Reliance in AI-Human Teams (TRAIT) nan As humans increasingly interact (and even collaborate) with AI systems during decision-making, creative exercises, and other tasks, appropriate trust and reliance are necessary to ensure proper usage and adoption of these systems. Specifically, people should understand when to trust or rely on an algorithm's outputs and when to override them. While significant research focus has aimed to measure and promote trust in human-AI interaction, the field lacks synthesized definitions and understanding of results across contexts. Indeed, conceptualizing trust and reliance, and identifying the best ways to measure these constructs and effectively shape them in human-AI interactions remains a challenge. This workshop aims to establish building appropriate trust and reliance on (imperfect) AI systems as a vital, yet under-explored research problem. The workshop will provide a venue for exploring three broad aspects related to human-AI trust: (1) How do we clarify definitions and frameworks relevant to human-AI trust and reliance (e.g., what does trust mean in different contexts)? (2) How do we measure trust and reliance? And, (3) How do we shape trust and reliance? As these problems and solutions involving humans and AI are interdisciplinary in nature, we invite participants with expertise in HCI, AI, ML, psychology, and social science, or other relevant fields to foster closer communications and collaboration between multiple communities."
87,2024,Human-AI teams to improve accuracy and timeliness of oncology trial prescreening: Preplanned interim analysis of a randomized trial nan nan
88,2022,"Design patterns for human-AI co-learning: A wizard-of-Oz evaluation in an urban-search-and-rescue task nan The rapid advancement of technology empowered by artificial intelligence is believed to intensify the collaboration between humans and AI as team partners. Successful collaboration requires partners to learn about each other and about the task. This human-AI co-learning can be achieved by presenting situations that enable partners to share knowledge and experiences. In this paper we describe the development and implementation of a task context and procedures for studying co-learning. More specifically, we designed specific sequences of interactions that aim to initiate and facilitate the co-learning process. The effects of these interventions on learning were evaluated in an experiment, using a simplified virtual urban-search-and-rescue task for a humanrobot team. The human participants performed a victim rescue- and evacuation mission in collaboration with a wizard-of-Oz (i.e., a confederate of the experimenter who executed the robot-behavior consistent with an ontology-based AI-model). The designed interaction sequences, formulated as Learning Design Patterns (LDPs), were intended to bring about co-learning. Results show that LDPs support the humans understanding and awareness of their robot partner and of the teamwork. No effects were found on collaboration fluency, nor on team performance. Results are used to discuss the importance of co-learning, the challenges of designing humanAI team tasks for research into this phenomenon, and the conditions under which co-learning is likely to be successful. The study contributes to our understanding of how humans learn with and from AI-partners, and our propositions for designing intentional learning (LDPs) provide directions for applications in future human-AI teams."
89,2024,"Designing for Appropriate Reliance:Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making [arXiv] nan Appropriate reliance is critical to achieving synergistic human-AI collaboration. For instance, when users over-rely on AI assistance, their human-AI team performance is bounded by the model's capability. This work studies how the presentation of model uncertainty may steer users' decision-making toward fostering appropriate reliance. Our results demonstrate that showing the calibrated model uncertainty alone is inadequate. Rather, calibrating model uncertainty and presenting it in a frequency format allow users to adjust their reliance accordingly and help reduce the effect of confirmation bias on their decisions. Furthermore, the critical nature of our skin cancer screening task skews participants' judgment, causing their reliance to vary depending on their initial decision. Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making. We discuss the potential for model uncertainty presentation, initial user decision, and user demographics to be incorporated in designing personalized AI aids for appropriate reliance."
91,2024,"Adaptation Through Communication: Assessing Human-Artificial Intelligence Partnership for the Design of Complex Engineering Systems nan Exploring the opportunities for incorporating Artificial Intelligence (AI) to support team problem-solving has been the focus of intensive ongoing research. However, while the incorporation of such AI tools into human team problem-solving can improve team performance, it is still unclear what modality of AI integration will lead to a genuine human-AI partnership capable of mimicking the dynamic adaptability of humans. This work unites human designers with AI Partners as fellow team members who can both reactively and proactively collaborate in real-time toward solving a complex and evolving engineering problem. Team performance and problem-solving behaviors are examined using the HyForm collaborative research platform, which uses an online collaborative design environment that simulates a complex interdisciplinary design problem. The problem constraints are unexpectedly changed midway through problem-solving to simulate the nature of dynamically evolving engineering problems. This work shows that after the unexpected design constraints change, or shock, is introduced, human-AI hybrid teams perform similarly to human teams, demonstrating the capability of AI Partners to adapt to unexpected events. Nonetheless, hybrid teams do struggle more with coordination and communication after the shock is introduced. Overall, this work demonstrates that these AI design partners can participate as active partners within human teams during a large, complex task, showing promise for future integration in practice."
92,2023,"Towards Effective Human-AI Decision-Making: The Role of Human Learning in Appropriate Reliance on AI Advice [arXiv] nan The true potential of human-AI collaboration lies in exploiting the complementary capabilities of humans and AI to achieve a joint performance superior to that of the individual AI or human, i.e., to achieve complementary team performance (CTP). To realize this complementarity potential, humans need to exercise discretion in following AI 's advice, i.e., appropriately relying on the AI's advice. While previous work has focused on building a mental model of the AI to assess AI recommendations, recent research has shown that the mental model alone cannot explain appropriate reliance. We hypothesize that, in addition to the mental model, human learning is a key mediator of appropriate reliance and, thus, CTP. In this study, we demonstrate the relationship between learning and appropriate reliance in an experiment with 100 participants. This work provides fundamental concepts for analyzing reliance and derives implications for the effective design of human-AI decision-making."
93,2023,A Study of Explainable Real-Time Object Detection and Human-AI Teaming Interactions in Virtual Environments nan nan
94,2022,"Towards a Reference Software Architecture for Human-Al Teaming in Smart Manufacturing nan With the proliferation of AI-enabled software systems in smart manufacturing, the role of such systems moves away from a reactive to a proactive role that provides context-specific support to manufacturing operators. In the frame of the EIJ funded Teaming.AI project, we identified the monitoring of teaming aspects in human-AI collaboration, the runtime monitoring and validation of ethical policies, and the support for experimentation with data and machine learning algorithms as the most relevant challenges for human-AI teaming in smart manufacturing. Based on these challenges, we developed a reference software architecture based on knowledge graphs, tracking and scene analysis, and components for relational machine learning with a particular focus on its scalability. Our approach uses knowledge graphs to capture productand process specific knowledge in the manufacturing process and to utilize it for relational machine learning. This allows for context-specific recommendations for actions in the manufacturing process for the optimization of product quality and the prevention of physical harm. The empirical validation of this software architecture will be conducted in cooperation with three large-gale companies in the automotive, energy systems, and precision machining domain. In this paper we discuss the identified challenges for such a reference software architecture, present its preliminary status, and sketch our further research vision in this project."
95,2023,"Vero: An accessible method for studying human-AI teamwork nan Despite the recognized need to prepare for a future of human-AI collaboration, the technical skills necessary to develop and deploy AI systems are considerable, making such research difficult to perform without specialized knowledge. To make human-AI collaboration research more accessible, we developed a novel experimental method that combines a standard video conferencing platform, a set of animations, and Wizard of Oz methods to simulate a group interaction with an AI teammate. Through a case study, we demonstrate the flexibility and ease of deployment of this approach. We also provide evidence that the method creates a highly believable experience of interacting with an AI agent. By detailing this method, we hope that researchers regardless of background can replicate it to more easily answer questions that will inform the design and development of future human-AI collaboration technologies. All rights reserved Elsevier."
96,2024,"Designing for Appropriate Reliance: The Roles of AI Uncertainty Presentation, Initial User Decision, and User Demographics in AI-Assisted Decision-Making nan Appropriate reliance is critical to achieving synergistic human-AI collaboration. For instance, when users over-rely on AI assistance, their human-AI team performance is bounded by the model's capability. This work studies how the presentation of model uncertainty may steer users' decision-making toward fostering appropriate reliance. Our results demonstrate that showing the calibrated model uncertainty alone is inadequate. Rather, calibrating model uncertainty and presenting it in a frequency format allow users to adjust their reliance accordingly and help reduce the effect of confirmation bias on their decisions. Furthermore, the critical nature of our skin cancer screening task skews participants' judgment, causing their reliance to vary depending on their initial decision. Additionally, step-wise multiple regression analyses revealed how user demographics such as age and familiarity with probability and statistics influence human-AI collaborative decision-making. We discuss the potential for model uncertainty presentation, initial user decision, and user demographics to be incorporated in designing personalized AI aids for appropriate reliance."
97,2024,"Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making nan While existing literature has explored and revealed several insights pertaining to the role of human factors (e.g., prior experience, domain knowledge) and attributes of AI systems (e.g., accuracy, trustworthiness), there is a limited understanding around how the important task characteristics of complexity and uncertainty shape human decision-making and human-AI team performance. In this work, we aim to address this research and empirical gap by systematically exploring how task complexity and uncertainty influence human-AI decision-making. Task complexity refers to the load of information associated with a task, while task uncertainty refers to the level of unpredictability associated with the outcome of a task. We conducted a between-subjects user study (N = 258) in the context of a trip-planning task to investigate the impact of task complexity and uncertainty on human trust and reliance on AI systems. Our results revealed that task complexity and uncertainty have a significant impact on user reliance on AI systems. When presented with complex and uncertain tasks, users tended to rely more on AI systems while demonstrating lower levels of appropriate reliance compared to tasks that were less complex and uncertain. In contrast, we found that user trust in the AI systems was not influenced by task complexity and uncertainty. Our findings can help inform the future design of empirical studies exploring human-AI decision-making. Insights from this work can inform the design of AI systems and interventions that are better aligned with the challenges posed by complex and uncertain tasks. Finally, the lens of diagnostic versus prognostic tasks can inspire the operationalization of uncertainty in human-AI decision-making studies."
98,2024,"Scalable Interactive Machine Learning for Future Command and Control [arXiv] nan Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in complex, dynamic situations; 2) fostering resilient human-AI teams through optimizing roles, configurations, and trust; and 3) scaling algorithms and human-AI teams for flexibility across a range of potential contexts and situations."
99,2024,"The pursuit of happiness: the power and influence of AI teammate emotion in human-AI teamwork nan As the world evolves, human-AI teams (HAT) have become increasingly more capable in their ability to complete task objectives. Due to this rising importance, it has become essential to understand the interpersonal dynamism between humans and AI to further optimise their performance potential. Given the demonstrated utility of emotional communication within human-human team structures, this research investigates the nature of AI-sourced positive emotions on human teammates. Through 47 interviews, our findings show that for these AI teammates to be accepted, human teammates have preferences on understanding the emotional utility prior to its presentation, as well as which emotions are situationally acceptable. Also, findings show that integrating emotions within AI teammates has a positive influence on human perceptions and behaviour in a task. In further detail, emotions act as status updates that allow human teammates to not only better understand their teammates' mental states but also understand how their AI teammates perceive the situation around them. Together, this gives insight into how AI emotional expressions influence the perception of social support on the wider Human-AI team. Mainly how emotions can be used to increase acceptance of AI teammates and improve the overall experience human teammates have within the task."
100,2024,"AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach nan This paper presents a modeling effort to explore the underlying physics of temperature evolution during additive friction stir deposition (AFSD) by a human-AI teaming approach. AFSD is an emerging solid-state additive manufacturing technology that deposits materials without melting. However, both process modeling and modeling of the AFSD tool are at an early stage. In this paper, a human-AI teaming approach is proposed to combine models based on first principles with AI. The resulting human-informed machine learning method, denoted as AFSD-Physics, can effectively learn the governing equations of temperature evolution at the tool and the build from in-process measurements. Experiments are designed and conducted to collect in-process measurements for the deposition of aluminum 7075 with a total of 30 layers. The acquired governing equations are physically interpretable models with low computational cost and high accuracy. Model predictions show good agreement with the measurements. Experimental validation with new process parameters demonstrates the model's generalizability and potential for use in tool temperature control and process optimization. All rights reserved Elsevier."
101,2023,"Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making nan Human-AI collaboration for decision-making strives to achieve team performance that exceeds the performance of humans or AI alone. However, many factors can impact success of Human-AI teams, including a user's domain expertise, mental models of an AI system, trust in recommendations, and more. This article reports on a study that examines users' interactions with three simulated algorithmic models, all with equivalent accuracy rates but each tuned differently in terms of true positive and true negative rates. Our study examined user performance in a non-trivial blood vessel labeling task where participants indicated whether a given blood vessel was flowing or stalled. Users completed 140 trials across multiple stages, first without an AI and then with recommendations from an AI-Assistant. Although all users had prior experience with the task, their levels of proficiency varied widely.Our results demonstrated that while recommendations from an AI-Assistant can aid in users' decision making, several underlying factors, including user base expertise and complementary human-AI tuning, significantly impact the overall team performance. First, users' base performance matters, particularly in comparison to the performance level of the AI. Novice users improved, but not to the accuracy level of the AI. Highly proficient users were generally able to discern when they should follow the AI recommendation and typically maintained or improved their performance. Mid-performers, who had a similar level of accuracy to the AI, were most variable in terms of whether the AI recommendations helped or hurt their performance. Second, tuning an AI algorithm to complement users' strengths and weaknesses also significantly impacted users' performance. For example, users in our study were better at detecting flowing blood vessels, so when the AI was tuned to reduce false negatives (at the expense of increasing false positives), users were able to reject those recommendations more easily and improve in accuracy. Finally, users' perception of the AI's performance relative to their own performance had an impact on whether users' accuracy improved when given recommendations from the AI. Overall, this work reveals important insights on the complex interplay of factors influencing Human-AI collaboration and provides recommendations on how to design and tune AI algorithms to complement users in decision-making tasks."
102,2024,"Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants nan This action research study focuses on the integration of AI assistants in two Agile software development meetings: the Daily Scrum and a feature refinement, a planning meeting that is part of an in-house Scaled Agile framework. We discuss the critical drivers of success, and establish a link between the use of AI and team collaboration dynamics. We conclude with a list of lessons learnt during the interventions in an industrial context, and provide a assessment checklist for companies and teams to reflect on their readiness level. This paper is thus a road-map to facilitate the integration of AI tools in Agile setups."
103,2024,"When in Doubt! Understanding the Role of Task Characteristics on Peer Decision-Making with AI Assistance nan With the integration of AI systems into our daily lives, human-AI collaboration has become increasingly prevalent. Prior work in this realm has primarily explored the effectiveness and performance of individual human and AI systems in collaborative tasks. While much of decision-making occurs within human peers and groups in the real world, there is a limited understanding of how they collaborate with AI systems. One of the key predictors of human-AI collaboration is the characteristics of the task at hand. Understanding the influence of task characteristics on human-AI collaboration is crucial for enhancing team performance and developing effective strategies for collaboration. Addressing a research and empirical gap, we seek to explore how the features of a task impact decision-making within human-AI group settings. In a 2x2 between-subjects study (N = 256) we examine the effects of task complexity and uncertainty on group performance and behaviour. The participants were grouped into pairs and assigned to one of four experimental conditions characterized by varying degrees of complexity and uncertainty. We found that high task complexity and high task uncertainty can negatively impact the performance of human-AI groups, leading to decreased group accuracy and increased disagreement with the AI system. We found that higher task complexity led to a higher efficiency in decision-making, while a higher task uncertainty had a negative impact on efficiency. Our findings highlight the importance of considering task characteristics when designing human-AI collaborative systems, as well as the future design of empirical studies exploring human-AI collaboration."
104,2023,"Towards a Methodology for Developing Human-AI Collaborative Decision Support Systems nan Decision-making is a complex activity, often demanding collaboration, sometimes even in the form of dynamic (ad hoc) teams of loosely coupled participants collected to deal with a particular problem. At the same time, recent developments in the AI have shown that AI plays an important role in decision-making, and AI-agents may become full-fledged participants of collaborative decision support systems. However, integration of AI-agents into collaborative processes requires solving a number of tasks concerning human-AI interaction, interpretability, mutual learning, etc. This paper is a step towards a methodology to create decision support systems based on human-AI collaboration. An analysis of typical requirements to the collaborative decision support systems and typical scenarios that such systems have to implement sustains the introduced methodology. Based on this analysis, foundational problems needed settlements to develop human-AI collaborative decision support systems have been identified, and their possible solutions are offered. In the proposed methodology, ontologies play an important role, providing interoperability among heterogeneous participants. The methodology implies a technological backing in the form of a collaborative computational environment, helping to develop decision support systems for particular domains."
105,2024,"Modeling Trust Dimensions and Dynamics in Human-Agent Conversation: A Trajectory Epistemic Network Analysis Approach nan Human-AI conversation provides a natural, unobtrusive, yet under-explored way to investigate trust dynamics in human-AI teams (HATs). In this paper, we modeled dynamic trust evolution in conversations using a novel method, trajectory epistemic network analysis (T-ENA). T-ENA captures the multidimensional aspect of trust (i.e., analytic and affective), and trajectory analysis segments conversations to capture temporal changes of trust over time. Twenty-four participants performed a habitat maintenance task assisted by a conversational agent and verbalized their experiences and feelings after each task. T-ENA showed that agent reliability significantly affected people's conversations in the analytic process of trust, t(38.88)=15.18,p<0.001,Cohen's d=4.72, such as discussing agents' errors. The trajectory analysis showed that trust dynamics manifested through conversation topic diversity and flow. These results showed trust dimensions and dynamics in conversation should be considered interdependently and suggested that an adaptive conversational strategy for managing trust in HATs."
106,2023,"Interaction of Thoughts: Towards Mediating Task Assignment in Human-AI Cooperation with a Capability-Aware Shared Mental Model nan The existing work on task assignment of human-AI cooperation did not consider the differences between individual team members regarding their capabilities, leading to sub-optimal task completion results. In this work, we propose a capability-aware shared mental model (CASMM) with the components of task grouping and negotiation, which utilize tuples to break down tasks into sets of scenarios relating to difficulties and then dynamically merge the task grouping ideas raised by human and AI through negotiation. We implement a prototype system and a 3-phase user study for the proof of concept via an image labeling task. The result shows building CASMM boosts the accuracy and time efficiency significantly through forming the task assignment close to real capabilities within few iterations. It helps users better understand the capability of AI and themselves. Our method has the potential to generalize to other scenarios such as medical diagnoses and automatic driving in facilitating better human-AI cooperation."
107,2024,"I Know This Looks Bad, But I Can Explain: Understanding When AI Should Explain Actions In Human-AI Teams nan Explanation of artificial intelligence (AI) decision-making has become an important research area in humancomputer interaction (HCI) and computer-supported teamwork research. While plenty of research has investigated AI explanations with an intent to improve AI transparency and human trust in AI, how AI explanations function in teaming environments remains unclear. Given that a major benefit of AI giving explanations is to increase human trust understanding howAI explanations impact human trust is crucial to effective human-AI teamwork. An online experiment was conducted with 156 participants to explore this question by examining how a teammate's explanations impact the perceived trust of the teammate and the effectiveness of the team and how these impacts vary based on whether the teammate is a human or an AI. This study shows that explanations facilitate trust in AI teammates when explaining why AI disobeyed humans' orders but hindered trust when explaining why an AI lied to humans. In addition, participants' personal characteristics (e.g., their gender and the individual's ethical framework) impacted their perceptions of AI teammates both directly and indirectly in different scenarios. Our study contributes to interactive intelligent systems and HCI by shedding light on how an AI teammate's actions and corresponding explanations are perceived by humans while identifying factors that impact trust and perceived effectiveness. This work provides an initial understanding of AI explanations in human-AI teams, which can be used for future research to build upon in exploring AI explanation implementation in collaborative environments."
108,2022,"HMIway-env: A Framework for Simulating Behaviors and Preferences to Support Human-AI Teaming in Driving nan We introduce a lightweight simulation and modeling framework, HMIway-env, for studying human-machine teaming in the context of driving. The goal of the framework is to accelerate the development of adaptive AI systems which can respond to individual driver states, traits, and preferences, by serving as a data-generation engine and training environment for learning personalized human-AI teaming policies. We extend highway-env, an OpenAI Gym-based simulator environment, to enable specification of human driver behavior, and design of vehicle-driver interactions and outcomes. We describe one instance of our framework incorporating models for distracted and cautious driving, which we validate through crowd-sourced feedback, and show early experimental results toward the training of better intervention policies."
109,2023,"Ontology-Based Reflective Communication for Shared Human-AI Recognition of Emergent Collaboration Patterns nan When humans and AI-agents collaborate, they need to continuously learn about each other and the task. We propose a Team Design Pattern that utilizes adaptivity in the behavior of human and agent team partners, causing new Collaboration Patterns to emerge. Human-AI Co-Learning takes place when partners can formalize recognized patterns of collaboration in a commonly shared language, and can communicate with each other about these patterns. For this, we developed an ontology of Collaboration Patterns. An accompanying Graphical User Interface (GUI) enables partners to formalize and refine Collaboration Patterns, which can then be communicated to the partner. The ontology was evaluated empirically with human participants who viewed video recordings of joint human-agent activities. Participants were requested to identify Collaboration Patterns in the footage, and to formalize patterns by using the ontology's GUI. Results show that the ontology supports humans to recognize and define Collaboration Patterns successfully. To improve the ontology, it is suggested to include pre- and post-conditions of tasks, as well as parallel actions of team members."
110,2024,"Empirically Understanding the Potential Impacts and Process of Social Influence in Human-AI Teams nan In the coming years, Artificial Intelligence (AI) will be applied as a teammate that works alongside and collaborates with humans. Prior research in teaming and CSCW has shown that teammates have the ability to change the thoughts and behaviors of each other through simple interactions in a process known as social influence. However, to date, research has yet to identify the social influence that AI teammates could have in these human-AI teams, which has led to a limited understanding of how AI teammates will change the behaviors of their human teammates. To remedy this gap, we conduct a mixed-methods study (N=33) with young individuals to explore how humans could behaviorally adapt and perceive their behavioral adaptation due to interaction with an AI teammate. Qualitative results report that perceived three unique stages they had to experience for the social influence of their AI teammate to lead to adaptation (i.e., perceiving a sense of control, identifying a technological or performative justification, and gaining first-hand experience). Quantitative results validate and illustrate the results of this perceived process, as results show that participants adapted their behaviors to complement the behaviors of different types of AI teammates. This study contributes to the CSCW/HCI field by developing an initial understanding of AI teammates' social influence in human-AI teams, which will be a pivotal design and research consideration in future efforts."
111,2023,"Effective Human-AI Teams via Learned Natural Language Rules and Onboarding [arXiv] nan People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately."
112,2020,"A Workflow-Based Methodological Framework for Hybrid Human-AI Enabled Scientometrics nan With cutting edge scientific breakthroughs, human-centred algorithmic approaches have proliferated in recent years and information technology (IT) has begun to redesign socio-technical systems in the context of human-AI collaboration. As a result, distinct forms of interaction have emerged in tandem with the proliferation of infrastructures aiding interdisciplinary work practices and research teams. Concomitantly, large volumes of heterogeneous datasets are produced and consumed at a rapid pace across many scientific domains. This results in difficulties in the reliable analysis of scientific production since current tools and algorithms are not necessarily able to provide acceptable levels of accuracy when analyzing the content and impact of publication records from large continuous scientific data streams. On the other hand, humans cannot consider all the information available and may be adversely influenced by extraneous factors. Using this rationale, we propose an initial design of a human-AI enabled pipeline for performing scientometric analyses that exploits the intersection between human behavior and machine intelligence. The contribution is a model for incorporating central principles of human-machine symbiosis (HMS) into scientometric workflows, demonstrating how hybrid intelligence systems can drive and encapsulate the future of research evaluation."
113,2020,"Optimizing AI for Teamwork [arXiv] nan In many high-stakes domains such as criminal justice, finance, and healthcare, AI systems may recommend actions to a human expert responsible for final decisions, a context known as AI-advised decision making. When AI practitioners deploy the most accurate system in these domains, they implicitly assume that the system will function alone in the world. We argue that the most accurate AI team-mate is not necessarily the em best teammate; for example, predictable performance is worth a slight sacrifice in AI accuracy. So, we propose training AI systems in a human-centered manner and directly optimizing for team performance. We study this proposal for a specific type of human-AI team, where the human overseer chooses to accept the AI recommendation or solve the task themselves. To optimize the team performance we maximize the team's expected utility, expressed in terms of quality of the final decision, cost of verifying, and individual accuracies. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the improvements in utility while being small and varying across datasets and parameters (such as cost of mistake), are real and consistent with our definition of team utility. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on human-centered optimization problems motivated by human-AI collaborations."
114,2021,"Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi nan Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI teammates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human's perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We find that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance. (4)"
115,2024,"Adopting AI teammates in knowledge-intensive crowdsourcing contests: the roles of transparency and explainability nan PurposeAs the role of AI on human teams shifts from a tool to a teammate, the implementation of AI teammates into knowledge-intensive crowdsourcing (KI-C) contest teams represents a forward-thinking and feasible solution to improve team performance. Since contest teams are characterized by virtuality, temporality, competitiveness, and skill diversity, the human-AI interaction mechanism underlying conventional teams is no longer applicable. This study empirically analyzes the effects of AI teammate attributes on human team members' willingness to adopt AI in crowdsourcing contests.Design/methodology/approachA questionnaire-based online experiment was designed to perform behavioral data collection. We obtained 206 valid anonymized samples from 28 provinces in China. The Ordinary Least Squares (OLS) model was used to test the proposed hypotheses.FindingsWe find that the transparency and explainability of AI teammates have mediating effects on human team members' willingness to adopt AI through trust. Due to the different tendencies exhibited by members with regard to three types of cognitive load, nonlinear U-shaped relationships are observed among explainability, cognitive load, and willingness to adopt AI.Originality/valueWe provide design ideas for human-AI team mechanisms in KI-C scenarios, and rationally explain how the U-shaped relationship between AI explainability and cognitive load emerges."
116,2024,"Complementarity in Human-AI Collaboration: Concept, Sources, and Evidence nan Artificial intelligence (AI) can improve human decision-making in various application areas. Ideally, collaboration between humans and AI should lead to complementary team performance (CTP) -- a level of performance that neither of them can attain individually. So far, however, CTP has rarely been observed, suggesting an insufficient understanding of the complementary constituents in human-AI collaboration that can contribute to CTP in decision-making. This work establishes a holistic theoretical foundation for understanding and developing human-AI complementarity. We conceptualize complementarity by introducing and formalizing the notion of complementarity potential and its realization. Moreover, we identify and outline sources that explain CTP. We illustrate our conceptualization by applying it in two empirical studies exploring two different sources of complementarity potential. In the first study, we focus on information asymmetry as a source and, in a real estate appraisal use case, demonstrate that humans can leverage unique contextual information to achieve CTP. In the second study, we focus on capability asymmetry as an alternative source, demonstrating how heterogeneous capabilities can help achieve CTP. Our work provides researchers with a theoretical foundation of complementarity in human-AI decision-making and demonstrates that leveraging sources of complementarity potential constitutes a viable pathway toward effective human-AI collaboration."
117,2024,"Challenges and opportunities when bringing machines onto the team: Human-AI teaming and flood evacuation decisions nan Supporting humans and artificial intelligence (AI) machines as teammates in flood evacuation decisions relies on a carefully designed system with the capability for monitoring, analyzing, responding, and executing. In this context, research is needed to improve the integration of human knowledge into the AI machines. The goal is to achieve trusting partnerships between humans and machines that allow them to communicate, coordinate, and work effectively as a team. Further, methods for supporting transparency and explainability in future AI system design need to consider incorporating different types of human data (e.g., social media data, collected citizen knowledge, and stakeholder input) in the loop particularly as these factors relate to dynamically changing flood evacuation situations. This commentary lays out the rationale for human-AI teaming (HAT) systems in the context of flood evacuation decision making."
118,2023,"Social perception in Human-AI teams: Warmth and competence predict receptivity to AI teammates nan Advances in artificial intelligence (AI) promise a future where teams consist of people and intelligent machines, such as robots or virtual agents. In order for human-AI teams (HATs) to succeed, human team members will need to be receptive to their new AI counterparts. In this study, we draw on a tripartite model of human newcomer receptivity, which includes three components: reflection, knowledge utilization, and psychological acceptance. We hypothesize that two aspects of social perception-warmth and competence-are critical predictors of human receptivity to a new AI teammate. Study 1 uses a video vignette design in which participants imagine adding one of eight AI teammates to a referent team. Study 2 leverages a Wizard of Oz methodology in laboratory teams. In addition to testing the effects of perceived warmth and competence on receptivity components, Study 2 also explores the influence of receptivity components on perceived HAT viability. Though both studies find that perceived warmth and competence affect receptivity, we find competence is particularly important for knowledge utilization and psychological acceptance. Further, results of Study 2 show that psychological acceptance is positively related to perceived HAT viability. Implications for future research on social perception of AI teammates are discussed. All rights reserved Elsevier."
119,2021,"Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making nan Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using virtual pilot studies and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may reinforce human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards enhancing human performance with AI assistance."
120,2023,Comparing Psychometric and Behavioral Predictors of Compliance During Human-AI Interactions nan Optimization of human-AI teams hinges on the AI's ability to tailor its interaction to individual human teammates. A common hypothesis in adaptive AI research is that minor differences in people's predisposition to trust can significantly impact their likelihood of complying with recommendations from the AI. Predisposition to trust is often measured with self-report inventories that are administered before interactions. We benchmark a popular measure of this kind against behavioral predictors of compliance. We find that the inventory is a less effective predictor of compliance than the behavioral measures in datasets taken from three previous research projects. This suggests a general property that individual differences in initial behavior are more predictive than differences in self-reported trust attitudes. This result also shows a potential for easily accessible behavioral measures to provide an AI with more accurate models without the use of (often costly) survey instruments.
121,2024,"On Evaluating Explanation Utility for Human-AI Decision Making in NLP nan Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations aid people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies. To help with this, we first review fitting existing metrics. We then establish requirements for datasets to be suitable for application-grounded evaluations. Among over 50 datasets available for explainability research in NLP, we find that 4 meet our criteria. By finetuning Flan-T5-3B, we demonstrate the importance of reassessing the state of the art to form and study human-AI teams. Finally, we present the exemplar studies of human-AI decision-making for one of the identified suitable tasks -- verifying the correctness of a legal claim given a contract."
122,2023,"AI-ACCELERATED DESIGN OF EVIDENCE SYNTHESIS FOR GLOBAL DEVELOPMENT nan When designing evidence-based policies and programs, decision-makers must distill key information from a vast and rapidly growing literature base. Identifying relevant literature from raw search results is time and resource intensive, and is often done by manual screening. In this study, we develop an AI agent based on a bidirectional encoder representations from transformers (BERT) model and incorporate it into a human team designing an evidence synthesis product for global development. We explore the effectiveness of the human-AI hybrid team in accelerating the evidence synthesis process. To further improve team efficiency, we enhance the human-AI hybrid team through active learning (AL). Specifically, we explore different sampling strategies, including random sampling, least confidence (LC) sampling, and highest priority (HP) sampling, to study their influence on the collaborative screening process. Results show that incorporating the BERT-based AI agent into the human team can reduce the human screening effort by 68.5% compared to the case of no AI assistance and by 16.8% compared to the industry standard of using an n-gram language model to encode texts and a support vector machine (SVM)-based classifier for identifying 80% of all relevant documents. When we apply the HP sampling strategy for AL, the human screening effort can be reduced even more to 78.3% for identifying 80% of all relevant documents compared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming workflow in the design process of three evidence gap maps (EGMs) for USAID and find it to be highly effective. These findings demonstrate how AI can accelerate the development of evidence synthesis products and promote timely evidence-based decision making in global development in a human-AI hybrid teaming context."
123,2022,"Quality Characteristics of a Software Platform for Human-AI Teaming in Smart Manufacturing nan As AI-enabled software systems become more prevalent in smart manufacturing, their role shifts from a reactive to a proactive one that provides context-specific support to machine operators. In the context of an international research project, we develop an AI-based software platform that shall facilitate the collaboration between human operators and manufacturing machines.We conducted 14 structured interviews with stakeholders of the prospective software platform in order to determine the individual relevance of selected quality characteristics for human-AI teaming in smart manufacturing. These characteristics include the ISO 25010:2011 standard for software quality and AI-specific quality characteristics such as trustworthiness, explicability, and auditability. The interviewees rated trustworthiness, functional suitability, reliability, and security as the most important quality characteristics for this context, and portability, compatibility, and maintainability as the least important. Also, we observed agreement regarding the relevance of the quality characteristics among interviewees having the same role. On the other hand, the relevance of each quality characteristics varied depending on the concrete use case of the prospective software platform.The interviewees also were asked about the key success factors related to human-AI teaming in smart manufacturing. They identified improving the production cycle, increasing operator efficiency, reducing scrap, and reducing ergonomic risks as key success criteria. In this paper, we also discuss metrics for measuring the fulfillment of these quality characteristics, which we intend to operationalize and monitor during operation of the prospective software platform."
124,2024,"The Role of User Control in Enhancing Human-AI Collaboration Effectiveness: Insights from a Pilot Study nan In this research program proposal, we aim to investigate why experts override AI suggestions and identify design principles for more effective human-AI teams. Specifically, we propose testing whether increasing the perceived locus of control of human decision-makers over AI functions will lead to fewer overrides and improved performance. We present a mixed-factorial, multi-trial experimental design in which participants receive AI recommendations regarding demand forecasting decisions in a business simulation. Prior to each trial, one group specifies how they want the AI to function (experimental), and the other group does not (control). We use electroencephalography and oculometry to capture attention to recommendations and user interface elements. Behavioral data from a preliminary pilot study with four participants align with our hypotheses. We observed that participants in the experimental condition applied smaller adjustments to AI suggestions and had higher decision performance than the control group. The experiment's results will contribute to our understanding of AI aversion and inform the design of human-AI interactions to improve performance."
125,2022,Collaborative Artificial Intelligence (AI) for Idea Generation in Design Teams nan nan
126,2024,Improvisation and Trust in Human/Autonomy Teams: A Task-Based Perspective nan nan
127,2019,"A Case for Backward Compatibility for Human-AI Teams [arXiv] nan AI systems are being deployed to support human decision making in high-stakes domains. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to changes that are at odds with the user's prior experiences and confidence in the AI's inferences, hurting therefore the overall team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes domains show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff, enabling more compatible yet accurate updates."
128,2021,"Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens nan Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees perceptions of AI making work performance evaluations. Through in-depth interviews with 21 workers, we found that 1) employees feel six types of burdens (i.e., emotional, mental, bias, manipulation, privacy, and social) toward AI's introduction to human resource management (HRM), and that 2) these burdens could be mitigated by incorporating transparency, interpretability, and human intervention to algorithmic decision-making. Based on our findings, we present design efforts to alleviate employees burdens. To leverage AI for HRM in fair and trustworthy ways, we call for the HCI community to design human-AI collaboration systems with various HR stakeholders."
129,2023,"Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control [arXiv] nan Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making."
130,2023,"The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features [arXiv] nan AI systems have been known to amplify biases in real world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments -- explanations, model bias disclosure and proxy correlation disclosure -- affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and the decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making."
131,2023,"Looping in the Human: Collaborative and Explainable Bayesian Optimization [arXiv] nan Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods."
133,2024,"Scalable interactive machine learning for future command and control nan Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in complex, dynamic situations; 2) fostering resilient human-AI teams through optimizing roles, configurations, and trust; and 3) scaling algorithms and human-AI teams for flexibility across a range of potential contexts and situations."
134,2020,"Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems nan Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone."
135,2023,"Adapt and overcome: Perceptions of adaptive autonomous agents for human-AI teaming nan Rapid advances in AI technologies have caused teams to explore the use of AI agents as full, active members of the team. The complex environments that teams occupy require human team members to constantly adapt their behaviors, and thus the ability of AI teammates to similarly adapt to changing situations significantly enhances the team's chances to succeed. In order to design such agents, it is important that we understand not only how to identify the amount of autonomous control AI agents have over their decisions, but also how changes to this control cognitively affects the rest of the team. Professional organizations often break their work cycles into phases that set limits on the team members' actions, and we propose that a similar process could be used to define the autonomy levels of AI teammates. Cyber incident response is an ideal context for this proposal, as we were able to use incident response phases to explore how a team's work cycle could guide an AI agent's changing level of autonomy. Using a mixed methods approach, we recruited 103 participants to complete a factorial survey containing ten contextual vignettes focused on an AI teammate's level of autonomy in incident response contexts, and from these participants we conducted twenty-two follow-on qualitative interviews that further explored how the participants felt an AI agent's adaptive capabilities would affect team performance and cohesiveness. Our results showed that work cycles can be used to assign autonomy levels to adaptive AI agents based upon the degree of formal processes and predictability of the team's tasks during the cycle, and that dynamic, human-like adaptation methods are vital to effective human-AI teams. This research provides significant contributions to the HCI community by proposing design recommendations for the development of adaptive autonomous teammates that both enhance Human-AI teams' productivity and promote positive team dynamics. All rights reserved Elsevier."
136,2023,"A Framework for Supporting Adaptive Human-AI Teaming in Air Traffic Control nan In recent years, the growth of cognitively complex systems has motivated researchers to study how to improve these systems' support of human work. At the same time, there is a momentum for introducing Artificial Intelligence (AI) in safety critical domains. The Air Traffic Control (ATC) system is a prime example of a cognitively complex safety critical system where AI applications are expected to support air traffic controllers in performing their tasks. Nevertheless, the design of AI systems that support effectively humans poses significant challenges. Central to these challenges is the choice of the model of how air traffic controllers perform their tasks. AI algorithms are notoriously sensitive to the choice of the models of how the human operators perform their tasks. The design of AI systems should be informed by knowledge of how people think and act in the context of their work environment. In this line of reasoning, the present study has set out to propose a framework of cognitive functions of air traffic controllers that can be used to support effectively adaptive Human - AI teaming. Our aim was to emphasize the staying in control element of the ATC. The proposed framework is expected to have meaningful implications in the design and effective operationalization of Human - AI teaming projects at the ATC Operations rooms."
137,2020,"Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems nan Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone."
138,2021,"Assessing Communication and Trust in an AI Teammate in a Dynamic Task Environment nan This research examines the relationship between anticipatory pushing of information and trust in human- autonomy teaming in a remotely piloted aircraft system - synthetic task environment. Two participants and one AI teammate emulated by a confederate executed a series of missions under routine and degraded conditions. We addressed the following questions: (1) How do anticipatory pushing of information and trust change from human to human and human to autonomous team members across the two sessions? and (2) How is anticipatory pushing of information associated with the trust placed in a teammate across the two sessions? This study demonstrated two main findings: (1) anticipatory pushing of information and trust differed between human-human and human-AI dyads, and (2) anticipatory pushing of information and trust scores increased among human-human dyads under degraded conditions but decreased in human-AI dyads."
139,2023,"Compensating for Sensing Failures via Delegation in Human-AI Hybrid Systems nan Given the increasing prevalence of intelligent systems capable of autonomous actions or augmenting human activities, it is important to consider scenarios in which the human, autonomous system, or both can exhibit failures as a result of one of several contributing factors (e.g., perception). Failures for either humans or autonomous agents can lead to simply a reduced performance level, or a failure can lead to something as severe as injury or death. For our topic, we consider the hybrid human-AI teaming case where a managing agent is tasked with identifying when to perform a delegated assignment and whether the human or autonomous system should gain control. In this context, the manager will estimate its best action based on the likelihood of either (human, autonomous) agent's failure as a result of their sensing capabilities and possible deficiencies. We model how the environmental context can contribute to, or exacerbate, these sensing deficiencies. These contexts provide cases where the manager must learn to identify agents with capabilities that are suitable for decision-making. As such, we demonstrate how a reinforcement learning manager can correct the context-delegation association and assist the hybrid team of agents in outperforming the behavior of any agent working in isolation."
140,2023,"Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task nan When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the AI's recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assistance, b) a high-precision restrained AI, and c) a high-recall zealous AI in over 3,466 person-hours of annotation work. In comparison, the zealous AI helps human teammates achieve significantly shorter task completion time and higher recall. In a follow-up study, we remove AI assistance for everyone and find negative training effects on annotators trained with the restrained AI. These findings and our analysis point to important implications for the design of AI assistance in recall-demanding scenarios."
141,2023,"DDoD: Dual Denial of Decision Attacks on Human-AI Teams nan Artificial intelligence (AI) systems have been increasingly used to make decision-making processes faster, more accurate, and more efficient. However, such systems are also at constant risk of being attacked. While the majority of attacks targeting AI-based applications aim to manipulate classifiers or training data and alter the output of an AI model, recently proposed sponge attacks against AI models aim to impede the classifier's execution by consuming substantial resources. In this work, we propose dual denial of decision (DDoD) attacks against collaborative human-AI teams. We discuss how such attacks aim to deplete both computational and human resources, and significantly impair decision-making capabilities. We describe DDoD on human and computational resources and present potential risk scenarios in a series of exemplary domains."
142,2022,"Verifiably Safe and Trusted Human-AI Systems: A Socio-technical Perspective nan Replacing human decision-making with machine decision-making results in challenges associated with stakeholders' trust in AI systems that interact with and keep the human user in the loop. We refer to such systems as Human-AI Systems (HAIS) and argue that technical safety and social trustworthiness of a HAIS are key to its wide-spread adoption by society. To develop a verifiably safe and trusted HAIS, it is important to understand how different stakeholders perceive an autonomous system (AS) as trusted, and how the context of application affects their perceptions. Technical approaches to meet trust and safety concerns are widely investigated and under-used in the context of measuring users' trust in autonomous AI systems. Interdisciplinary socio-technical approaches, grounded in social science (trust) and computer science (safety), are less considered in HAIS investigations. This paper aims to elaborate on the need for the application of formal methods, for ensuring safe behaviour of HAIS, based on the real-life understanding of users about trust, and analysing trust dynamics. This work puts forward core challenges in this area and presents a research agenda on verifiably safe and trusted human-AI systems."
143,2023,"How Time Pressure in Different Phases of Decision-Making Influences Human-AI Collaboration nan Human cognitive and decision-making abilities depreciate under pressure, motivating the emergence of artificial intelligence (AI) systems as decision support tools to assist people in performing tasks under stress. In this work, we study human decision-making behavior and task performance under time pressure---induced from limitedinitial observation time (time to perform the task before providing an initial response without AI input) andfinal decision time (time to weigh an AI's suggestion before reaching a collective human-AI team answer)---for spatial reasoning and count estimation tasks. Our results show that, while the impact of initial observation time on AI-assisted decision-making was dependent on task nature, participants were more likely to follow AI suggestions when they were provided with longer final decision time; moreover, although participants generally tended to adhere to their initial responses, they had more agency when they were more logically engaged in a task. Our results offer a nuanced understanding of human-AI collaboration under time pressure in different phases of the decision-making process."
144,2019,"Social Integration of Artificial Intelligence: Functions, Automation Allocation Logic and Human-Autonomy Trust nan Artificial intelligence (AI) is finding more uses in the human society resulting in a need to scrutinise the relationship between humans and AI. Technology itself has advanced from the mere encoding of human knowledge into a machine to designing machines that know how to autonomously acquire the knowledge they need, learn from it and act independently in the environment. Fortunately, this need is not new; it has scientific grounds that could be traced back to the inception of computers. This paper uses a multi-disciplinary lens to explore how the natural cognitive intelligence in a human could interface with the artificial cognitive intelligence of a machine. The scientific journey over the last 50 years will be examined to understand the Human-AI relationship, and to present the nature of, and the role of trust in, this relationship. Risks and opportunities sitting at the human-AI interface will be studied to reveal some of the fundamental technical challenges for a trustworthy human-AI relationship. The critical assessment of the literature leads to the conclusion that any social integration of AI into the human social system would necessitate a form of a relationship on one level or another in society, meaning that humans will always actively participate in certain decision-making loopseither in-the-loop or on-the-loopthat will influence the operations of AI, regardless of how sophisticated it is."
146,2023,"Improving the State of the Art for Training Human-AI Teams: Technical Report #1 -- Results of Subject-Matter Expert Knowledge Elicitation Survey [arXiv] nan A consensus report produced for the Air Force Research Laboratory by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on human-AI teams. We decided to use Joint All-Domain Command and Control (JADC2) as a focus point for developing the STE because the volume of sensor inputs and decision options within the JADC2 concept likely requires the use of AI systems to enable timely decisions. Given this focus, we engaged a number of Subject-Matter Experts (SMEs) with Command and Control experience to gain insight into developing a STE that embodied the teaming challenges associated with JADC2. This report documents our initial engagement with those stakeholders. The research team identified thirteen Sonalysts employees with military backgrounds and Command and Control experience, and invited them to participate. Twelve respondents completed the survey. The team then analyzed the responses to identify themes that emerged and topics that would benefit from further analysis. The results indicated that our SMEs were amenable to research using tasks that were analogous to those encountered in military environments, as long as they required teams to process a great deal of incoming data to arrive at complex decisions. The SMEs felt that the testbed should support 'teams of teams that represented a matrixed organization, and that it should support a robust array to spoken, text-based, and face-to-face communications."
147,2023,"Mental Models of AI Performance and Bias of Nontechnical Users nan Understanding human mental models of AI are critical for designing human-centered AI. Examining mental models provides depth in understanding of how users want to interact with AI, when users may need additional explanation of the system, and what knowledge is shared between the user and the AI. This work investigates users' mental models of an AI-decision aid. An experiment was designed to mimic a realistic emergency preparedness scenario in which a resource must be allocated into 1 of 100 possible locations based on a variety of dynamic visual heat maps. The users are assisted in resource placement by an AI-decision aid. The experiment was divide into two experimental blocks. The first of which was used to determine mental model accuracy. The second of which was used to examine preferences in meeting the individual and human-AI team goals. The users are asked to determine whether the AI is satisfying a set of constraints to best serve the affected population. Users are also asked to provide an overall score for the AI performance in resource placement. It was found that users tended to exhibit a binary bias in which they tended to categorize performance into discrete bins rather than on a continuous scale, however, the users were able to distinguish between individual and team goals in a human-AI team decision task and did not exhibit a bias towards the human goals. Mental Models, Decision Support, Artificial Intelligence"
148,2022,"Extended Abstract: Human-AI Teaming: Cases and Considerations for Professional Communicators nan Increasingly, professional communication is being produced by machines-that is, artificial intelligence (AI)-based systems that either assist human writers or that produce writing mostly on their own. As writing machines become more advanced, human professional communicators will be called on to partner with these machines as collaborators in various roles. This panel-consisting of three papers presented by five authors-considers some of the implications extending from this technology development: How is professional communication changing because of human-machine teaming (HMT)? What new skills and literacies will be required-from machines as well as humans-as we increasingly partner with machines to create professional communications?"
149,2024,"AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach [arXiv] nan This paper presents a modeling effort to explore the underlying physics of temperature evolution during additive friction stir deposition (AFSD) by a human-AI teaming approach. AFSD is an emerging solid-state additive manufacturing technology that deposits materials without melting. However, both process modeling and modeling of the AFSD tool are at an early stage. In this paper, a human-AI teaming approach is proposed to combine models based on first principles with AI. The resulting human-informed machine learning method, denoted as AFSD-Physics, can effectively learn the governing equations of temperature evolution at the tool and the build from in-process measurements. Experiments are designed and conducted to collect in-process measurements for the deposition of aluminum 7075 with a total of 30 layers. The acquired governing equations are physically interpretable models with low computational cost and high accuracy. Model predictions show good agreement with the measurements. Experimental validation with new process parameters demonstrates the model's generalizability and potential for use in tool temperature control and process optimization."
150,2024,"Collaborative human-AI trust (CHAI-T): A process framework for active management of trust in human-AI collaboration nan Collaborative human-AI (HAI) teaming combines the unique skills and capabilities of humans and machines in sustained teaming interactions leveraging the strengths of each. In tasks involving regular exposure to novelty and uncertainty, collaboration between adaptive, creative humans and powerful, precise artificial intelligence (AI) promises new solutions and efficiencies. User trust is essential to creating and maintaining these collaborative relationships. Established models of trust in traditional forms of AI typically recognize the contribution of three primary categories of trust antecedents: characteristics of the human user, characteristics of the technology, and environmental factors. The emergence of HAI teams, however, requires an understanding of human trust that accounts for the specificity of task contexts and goals, integrates processes of interaction, and captures how trust evolves in a teaming environment over time. Drawing on both the psychological and computer science literature, the process framework of trust in collaborative HAI teams (CHAI-T) presented in this paper adopts the tripartite structure of antecedents established by earlier models, while incorporating team processes and performance phases to capture the dynamism inherent to trust in teaming contexts. These features enable active management of trust in collaborative AI systems, with practical implications for the design and deployment of collaborative HAI teams."
151,2024,"Action Over Words: Predicting Human Trust in AI Partners Through Gameplay Behaviors nan In the burgeoning field of human-AI interaction, trust emerges as a cornerstone because many think that it is critical to the effectiveness of collaboration and the acceptance of AI systems. Traditional methods of assessing trust have predominantly relied on self-reported measures, requiring participants to articulate their perceptions and attitudes through questionnaires. However, these explicit methods may not fully capture the nuanced dynamics of trust, especially in real-time and complex interaction environments. This paper introduces an innovative approach to evaluating trust in human-AI teams, pivoting from the conventional reliance on verbal or written feedback to analyzing gameplay behaviors as implicit indicators of trust levels. Utilizing the Overcooked-AI environment, our study explores how participants' interactions with AI agents of varying performance levels can reveal underlying trust mechanisms without a single query posed to the human players. This approach not only bypasses the efficiency challenges posed by repetitive and lengthy trust assessment methods, but also provides insights comparable to them. We highlight the potential of non-verbal cues and action patterns as reliable trust indicators by comparing the predictive accuracies of questionnaire-based models with those derived from gameplay behavior analysis. Furthermore, our findings suggest that these implicit measures can be integrated into adaptive systems and algorithms for real-time trust calibration in human-agent teaming settings. This shift towards an action-oriented trust assessment challenges existing paradigms and opens new avenues for understanding and enhancing human-AI collaboration."
152,2024,"Safety, Trust, and Ethics Considerations for Human-AI Teaming in Aerospace Control nan Designing a safe, trusted, and ethical AI may be practically impossible; however, designing AI with safe, trusted, and ethical use in mind is possible and necessary in safety and mission-critical domains like aerospace. Safe, trusted, and ethical use of AI are often used interchangeably; however, a system can be safely used but not trusted or ethical, have a trusted use that is not safe or ethical, and have an ethical use that is not safe or trusted. This manuscript serves as a primer to illuminate the nuanced differences between these concepts, with a specific focus on applications of Human-AI teaming in aerospace system control, where humans may be in, on, or out-of-the-loop of decision-making."
153,2023,"The Impact of Gender and Personality in Human-AI Teaming: The Case of Collaborative Question Answering nan This paper discusses the results of an exploratory study aimed at investigating the impact of conversational agents (CAs) and specifically their agential characteristics on collaborative decision-making processes. The study involved 29 participants divided into 8 small teams engaged in a question-and-answer trivia-style game with the support of a text-based CA, characterized by two independent binary variables: personality (gentle and cooperative vs blunt and uncooperative) and gender (female vs male). A semi-structured group interview was conducted at the end of the experimental sessions to investigate the perceived utility and level of satisfaction with the CAs. Our results show that when users interact with a gentle and cooperative CA, their user satisfaction is higher. Furthermore, female CAs are perceived as more useful and satisfying to interact with than male CAs. We show that group performance improves through interaction with the CAs, confirming that a stereotype favoring the female with a gentle and cooperative personality combination exists in regard to perceived satisfaction, even though this does not lead to greater perceived utility. Our study extends the current debate about the possible correlation between CA characteristics and human acceptance and suggests future research to investigate the role of gender bias and related biases in human-AI teaming."
154,2023,"Supporting Human-AI Teams:Transparency, explainability and situation awareness nan System autonomy and AI are being developed for a wide variety of applications where they will likely work in tandem with people, forming human-AI teams (HAT). Situation awareness (SA) of autonomous systems and AI has been established as critical for effective interaction and oversight of these systems. As AI capabilities grow, and more effective teaming behaviors are expected of AI systems, there will also be an increased need for shared SA between the human and AI teammates. Methods for supporting team SA within HAT are discussed in terms of team SA requirements, team SA mechanisms, team SA displays and team SA processes. A framework for understanding the types of information that needs to be shared within HAT is provided, including a focus on taskwork SA, agent SA, and teamwork SA. AI based on learning systems creates new challenges for the development of good SA and mental models. AI transparency and explainability are discussed in terms of their separate roles for supporting SA and mental models in HAT. The SA Oriented Design (SAOD) process is described as a systematic methodology for developing transparent AI displays for HAT and an example of its application to automated driving in a Tesla is provided. All rights reserved Elsevier."
155,2023,"Improving the State of the Art for Training Human-AI Teams: Technical Report #2 -- Results of Researcher Knowledge Elicitation Survey [arXiv] nan A consensus report produced for the Air Force Research Laboratory (AFRL) by the National Academies of Sciences, Engineering, and Mathematics documented a prevalent and increasing desire to support human-Artificial Intelligence (AI) teaming across military service branches. Sonalysts has begun an internal initiative to explore the training of Human-AI teams. The first step in this effort is to develop a Synthetic Task Environment (STE) that is capable of facilitating research on Human-AI teams. Our goal is to create a STE that offers a task environment that could support the breadth of research that stakeholders plan to perform within this domain. As a result, we wanted to sample the priorities of the relevant research community broadly, and the effort documented in this report is our initial attempt to do so. We created a survey that featured two types of questions. The first asked respondents to report their agreement with STE features that we anticipated might be important. The second represented open-ended questions that asked respondents to specify their priorities within several dimensions of the anticipated STE. The research team invited nineteen researchers from academic and Government labs to participate, and 11 were able to complete the survey. The team analyzed their responses to identify themes that emerged and topics that would benefit from further analysis. The most significant finding of the survey was that a number of researchers felt that various open-source STEs that would meet our needs already exist. Researchers also emphasized the need for automated transcription and coding tools to ease the burden of assessing inter-team communications; the importance of robust data capture and export capabilities; and the desirability of extensive flexibility across many aspects of the tool."
156,2022,"Role of Human-AI Interaction in Selective Prediction nan Recent work has shown the potential benefit of selective prediction systems that can learn to defer to a human when the predictions of the AI are unreliable, particularly to improve the reliability of AI systems in high-stakes applications like healthcare or conservation. However, most prior work assumes that human behavior remains unchanged when they solve a prediction task as part of a human-AI team as opposed to by themselves. We show that this is not the case by performing experiments to quantify human-AI interaction in the context of selective prediction. In particular, we study the impact of communicating different types of information to humans about the AI system's decision to defer. Using realworld conservation data and a selective prediction system that improves expected accuracy over that of the human or AI system working individually, we show that this messaging has a significant impact on the accuracy of human judgements. Our results study two components of the messaging strategy: 1) Whether humans are informed about the prediction of the AI system and 2) Whether they are informed about the decision of the selective prediction system to defer. By manipulating these messaging components, we show that it is possible to significantly boost human performance by informing the human of the decision to defer, but not revealing the prediction of the AI. We therefore show that it is vital to consider how the decision to defer is communicated to a human when designing selective prediction systems, and that the composite accuracy of a human-AI team must be carefully evaluated using a human-in-the-loop framework."
157,2020,"A Situation Awareness-Based Framework for Design and Evaluation of Explainable AI nan Recent advances in artificial intelligence (AI) have drawn attention to the need for AI systems to be understandable to human users. The explainable AI (XAI) literature aims to enhance human understanding and human-AI team performance by providing users with necessary information about AI system behavior. Simultaneously, the human factors literature has long addressed important considerations that contribute to human performance, including how to determine human informational needs. Drawing from the human factors literature, we propose a three-level framework for the development and evaluation of explanations about AI system behavior. Our proposed levels of XAI are based on the informational needs of human users, which can be determined using the levels of situation awareness (SA) framework from the human factors literature. Based on our levels of XAI framework, we also propose a method for assessing the effectiveness of XAI systems."
158,2021,"Turning HART into HEART: Human Emotional AI/Robot Teaming nan The advancement of technology requires a new teaming dynamic between humans and AI/robots. As emotions and affect are critical in forming human-human relationships, they are also expected to have a significant impact on human-AI/robot team dynamics. However, little research has been conducted on the effects of emotions on this relationship and team performance. In this perspective paper, I propose turning HART (human-AI/robot teaming) into HEART (human-emotional AI/robot teaming) and demonstrate its potential effects. To this end, existing frameworks on emotion research are introduced, followed by a brief literature survey on emotional AI/robot research and its potential applications to HART. Finally, further considerations are discussed. I hope this paper can spark lively discussions on a new legitimate framework, HEART, to more accurately understand and predict human behavior and team performance in HART settings."
159,2022,"Workshop on Trust and Reliance in AI-Human Teams (TRAIT) nan As humans increasingly interact (and even collaborate) with AI systems during decision-making, creative exercises, and other tasks, appropriate trust and reliance are necessary to ensure proper usage and adoption of these systems. Specifically, people should understand when to trust or rely on an algorithm's outputs and when to override them. While significant research focus has aimed to measure and promote trust in human-AI interaction, the field lacks synthesized definitions and understanding of results across contexts. Indeed, conceptualizing trust and reliance, and identifying the best ways to measure these constructs and effectively shape them in human-AI interactions remains a challenge.This workshop aims to establish building appropriate trust and reliance on (imperfect) AI systems as a vital, yet under-explored research problem. The workshop will provide a venue for exploring three broad aspects related to human-AI trust: (1) How do we clarify definitions and frameworks relevant to human-AI trust and reliance (e.g., what does trust mean in different contexts)? (2) How do we measure trust and reliance? And, (3) How do we shape trust and reliance? As these problems and solutions involving humans and AI are interdisciplinary in nature, we invite participants with expertise in HCI, AI, ML, psychology, and social science, or other relevant fields to foster closer communications and collaboration between multiple communities."
160,2024,Understanding and Facilitating Human-AI Teaming for Real-World Computer Vision Tasks nan nan
161,2023,"A Missing Piece in the Puzzle: Considering the Role of Task Complexity in Human-AI Decision Making nan Recent advances in the performance of machine learning algorithms have led to the adoption of AI models in decision making contexts across various domains such as healthcare, finance, and education. Different research communities have attempted to optimize and evaluate human-AI team performance through empirical studies by increasing transparency of AI systems, or providing explanations to aid human understanding of such systems. However, the variety in decision making tasks considered and their operationalization in prior empirical work, has led to an opacity around how findings from one task or domain carry forward to another. The lack of a standardized means of considering task attributes prevents straight-forward comparisons across decision tasks, thereby limiting the generalizability of findings. We argue that the lens of 'task complexity' can be used to tackle this problem of under-specification and facilitate comparison across empirical research in this area. To retrospectively explore how different HCI communities have considered the influence of task complexity in designing experiments in the realm of human-AI decision making, we survey literature and provide an overview of empirical studies on this topic. We found a serious dearth in the consideration of task complexity across various studies in this realm of research. Inspired by Robert Wood's seminal work on the construct, we operationalized task complexity with respect to three dimensions (component, coordinative, and dynamic) and quantified the complexity of decision tasks in existing work accordingly. We then summarized current trends and proposed research directions for the future. Our study highlights the need to account for task complexity as an important design choice. This is a first step to help the scientific community in drawing meaningful comparisons across empirical studies in human-AI decision making and to provide opportunities to generalize findings across diverse domains and experimental settings."
162,2021,"Assessing Communication and Trust in an AI Teammate in a Dynamic Task Environment nan This research examines the relationship between anticipatory pushing of information and trust in human-autonomy teaming in a remotely piloted aircraft system - synthetic task environment. Two participants and one AI teammate emulated by a confederate executed a series of missions under routine and degraded conditions. We addressed the following questions: (1) How do anticipatory pushing of information and trust change from human to human and human to autonomous team members across the two sessions? and (2) How is anticipatory pushing of information associated with the trust placed in a teammate across the two sessions? This study demonstrated two main findings: (1) anticipatory pushing of information and trust differed between human-human and human-AI dyads, and (2) anticipatory pushing of information and trust scores increased among human-human dyads under degraded conditions but decreased in human-AI dyads."
163,2023,"Data on human decision, feedback, and confidence during an artificial intelligence-assisted decision-making task. nan The data are collected from a human subjects study in which 100 participants solve chess puzzle problems with artificial intelligence (AI) assistance. The participants are assigned to one of the two experimental conditions determined by the direction of the change in AI performance at problem 20: 1) high- to low-performing and 2) low- to high-performing. The dataset contains information about the participants' move before an AI suggestion, the goodness evaluation score of these moves, AI suggestion, feedback, and the participants' confidence in AI and self-confidence during three initial practice problems and 30 experimental problems. The dataset contains 100 CSV files, one per participant. There is opportunity for this dataset to be utilized in various domains that research human-AI collaboration scenarios such as human-computer interaction, psychology, computer science, and team management in engineering/business. Not only can the dataset enable further cognitive and behavioral analysis in human-AI collaboration contexts but also provide an experimental platform to develop and test future confidence calibration methods."
164,2024,"Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision-Making nan While existing literature has explored and revealed several insights pertaining to the role of human factors (e.g., prior experience, domain knowledge) and attributes of AI systems (e.g., accuracy, trustworthiness), there is a limited understanding around how the important task characteristics of complexity and uncertainty shape human decision-making and human-AI team performance. In this work, we aim to address this research and empirical gap by systematically exploring how task complexity and uncertainty influence human-AI decision-making. Task complexity refers to the load of information associated with a task, while task uncertainty refers to the level of unpredictability associated with the outcome of a task. We conducted a between-subjects user study (N = 258) in the context of a trip-planning task to investigate the impact of task complexity and uncertainty on human trust and reliance on AI systems. Our results revealed that task complexity and uncertainty have a significant impact on user reliance on AI systems. When presented with complex and uncertain tasks, users tended to rely more on AI systems while demonstrating lower levels of appropriate reliance compared to tasks that were less complex and uncertain. In contrast, we found that user trust in the AI systems was not influenced by task complexity and uncertainty. Our findings can help inform the future design of empirical studies exploring human-AI decision-making. Insights from this work can inform the design of AI systems and interventions that are better aligned with the challenges posed by complex and uncertain tasks. Finally, the lens of diagnostic versus prognostic tasks can inspire the operationalization of uncertainty in human-AI decision-making studies."
165,2022,"Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making nan While artificial intelligence (AI) is increasingly applied for decision-making processes, ethical decisions pose challenges for AI applications. Given that humans cannot always agree on the right thing to do, how would ethical decision-making by AI systems be perceived and how would responsibility be ascribed in human-AI collaboration? In this study, we investigate how the expert type (human vs. AI) and level of expert autonomy (adviser vs. decider) influence trust, perceived responsibility, and reliance. We find that participants consider humans to be more morally trustworthy but less capable than their AI equivalent. This shows in participants' reliance on AI: AI recommendations and decisions are accepted more often than the human expert's. However, AI team experts are perceived to be less responsible than humans, while programmers and sellers of AI systems are deemed partially responsible instead."
166,2019,"iLEAP: a human-AI teaming based mobile language learning solution for dual language learners in early and special educations nan In this research paper, we present an AR- and AI-based mobile learning tool that provides: 1.) automatic and accurate intelligibility analysis at various levels: letter, word, phrase and sentences, 2.) immediate feedback and multimodal coaching on how to correct pronunciation, and 3.) evidence-based dynamic training curriculum tailored to each individuals learning patterns and needs, e.g., retention of corrected pronunciation and typical pronunciation errors. The use of visible and interactive virtual expert technology capable of intuitive AR-based interactions will greatly increase students acceptance and retention of a virtual coach. In school or at home, it will readily resemble an expert reading specialist to effectively guide and assist a student in practicing reading and speaking by him-/herself independently, which is particularly important for dual language learners (DLL) whose first language (L1) is not English as many of their parents dont speak English fluently and cannot offer the necessary help. Our human-AI teaming based solution overcomes the shortfall of conventional computer-based language learning tools and serve as a supportive and team-based learning platform that is critical for optimizing the learning outcomes."
167,2023,"The Hidden Rules of Hanabi: How Humans Outperform AI Agents nan Games that feature multiple players, limited communication, and partial information are particularly challenging for AI agents. In the cooperative card game Hanabi, which possesses all of these attributes, AI agents fail to achieve scores comparable to even first-time human players. Through an observational study of three mixed-skill Hanabi play groups, we identify the techniques used by humans that help to explain their superior performance compared to AI. These concern physical artefact manipulation, coordination play, role establishment, and continual rule negotiation. Our findings extend previous accounts of human performance in Hanabi, which are purely in terms of theory-of-mind reasoning, by revealing more precisely how this form of collective decision-making is enacted in skilled human play. Our interpretation points to a gap in the current capabilities of AI agents to perform cooperative tasks."
168,2020,"Human, AI, Robot Teaming and the Future of Work: Barriers and Opportunities for Advancement nan Global investments in artificial intelligence (AI) and robotics are on the rise, with the results to impact global economies, security, safety, and human well-being. The most heralded advances in this space are more often about the technologies that are capable of disrupting business-as-usual than they are about innovation that advances or supports a global workforce. TheFuture of Work at the Human-Technology Frontieris one of NSF's 10 Big Ideas for research advancement. This panel discussion focuses on the barriers and opportunities for a future of human and AI/robot teaming, with people at the center of complex systems that provide social, ethical, and economic value."
169,2022,"The Artificial Colleague: Evaluation of Work Satisfaction in Collaboration with Non-human Coworkers nan The advance of artificial intelligence-(AI)-based technologies has the potential to transform work tremendously. Work is a major part of life and, thus, its meaningfulness or lack thereof will impact overall well-being. Previous research investigated human-AI collaboration at work mostly with a focus on performance. However, little attention is given to how collaboration with AI influences the meaningfulness of work and job satisfaction. In this paper, we present an online experiment to compare the perception of meaningfulness and relationship to the collaborator across different task distributions and collaborators (human/AI). Our results show that working with a human is more motivating and meaningful compared to working with an AI independent of the task. Moreover, the AI is more often viewed as a subordinate, while the human is perceived as a teammate. These results provide preliminary implications for the design of collaboration with AI that consider job satisfaction."
170,2021,"Human-AI Interaction in Human Resource Management: Understanding Why Employees Resist Algorithmic Evaluation at Workplaces and How to Mitigate Burdens nan Recently, Artificial Intelligence (AI) has been used to enable efficient decision-making in managerial and organizational contexts, ranging from employment to dismissal. However, to avoid employees' antipathy toward AI, it is important to understand what aspects of AI employees like and/or dislike. In this paper, we aim to identify how employees perceive current human resource (HR) teams and future algorithmic management. Specifically, we explored what factors negatively influence employees' perceptions of AI making work performance evaluations. Through in-depth interviews with 21 workers, we found that 1) employees feel six types of burdens (i.e., emotional, mental, bias, manipulation, privacy, and social) toward AI's introduction to human resource management (HRM), and that 2) these burdens could be mitigated by incorporating transparency, interpretability, and human intervention to algorithmic decision-making. Based on our findings, we present design efforts to alleviate employees' burdens. To leverage AI for HRM in fair and trustworthy ways, we call for the HCI community to design human-AI collaboration systems with various HR stakeholders."
171,2023,"Effective Human-AI Teams via Learned Natural Language Rules and Onboarding nan People are relying on AI agents to assist them with various tasks. The human must know when to rely on the agent, collaborate with the agent, or ignore its suggestions. In this work, we propose to learn rules grounded in data regions and described in natural language that illustrate how the human should collaborate with the AI. Our novel region discovery algorithm finds local regions in the data as neighborhoods in an embedding space that corrects the human prior. Each region is then described using an iterative and contrastive procedure where a large language model describes the region. We then teach these rules to the human via an onboarding stage. Through user studies on object detection and question-answering tasks, we show that our method can lead to more accurate human-AI teams. We also evaluate our region discovery and description algorithms separately."
172,2022,"Mutually beneficial decision making in Human-AI teams: Understanding soldier's perception and expectations from AI teammates in human-AI teams nan Military decision-making, a complex process entailing multiple domains (Bosch et al., 2018), may involve routine as well as unprecedented situations requiring quick responses. The current decision-making process in military operations involves the decision-maker and their team collecting information during various phases of a mission, analyzing, and synthesizing it to determine the most suitable courses of action, and then considering their consequences (Olinover & Geva, 2021). Thus, this complex decision-making process may demand higher than normal attentional resources. However, humans are limited in their information processing ability as their capability to make decisions involves a complex interplay of such factors as perceptions, past experiences, severity, emotions, thoughts, and intellectual capacity (Harari, 2016), all of which may negatively affect the situational awareness of military professionals making decisions, thus endangering their and their teammates' lives. Human decision-making is prone to overconfidence bias, the overestimation of one's knowledge of their surroundings, the illusion of uncertainty, the inability to estimate the dynamic nature of different situations and surroundings (Kahneman, 2011). Taking action based on incomplete or inaccurate knowledge in high-risk dynamic environments leads people to take risks that they would have not if they had a better understanding of the consequences of their actions (Kahneman, 2011; Bosch et al., 2018)."
173,2023,"Can We Build it? Yes, We Can! Development Procedure of High-Fidelity Simulation Environments for Human-Agent Teams nan This paper presents a bottom-up approach to designing and developing a high-fidelity simulation environment that fosters human acceptance of artificial intelligence (AI) as teammates by refining their mental models of appropriate teamwork expectations. Our process begins by first identifying an appropriate contextual situation that warrants humans teaming with AI as opposed to other team-based configurations. Those criteria are based on the delegation of roles appropriate to the established strengths of humans/AI, as well as the interdependence created between those roles to accentuate the expectations of teammate behavior. Next, qualitative interviews should be conducted with diverse subject matter experts to gain a comprehensive understanding of the situation and the environmental attributes through various perspectives. Once analyzed using thematic analysis, themes present themselves as design recommendations on how to create a high-fidelity simulation environment that nurtures human-agent team collaboration."
174,2024,"Human-AI Teamwork Interface Design Using Patterns of Interactions nan Increasingly capable Artificial Intelligence (AI) agents are reaching new performance levels and humans need to work effectively with them in a variety of collaborative team structures. Of particular interest is shared control collaboration, where actions taken by the team depend on the blended combination of inputs from all team entities. We conducted an experiment where participants played a game and maneuvered a spacecraft while working in concert with an AI agent and simultaneously performing a secondary task. The experiment results (i) show different patterns of interactions across user interface designs, agent capabilities, and participants' game experience, (ii) underscore the importance of providing information about the shared control collaborative task to humans who are working with a less-capable agent, and (iii) imply the potential of real-time communication about agent's state to achieve better teamwork. Our findings are expected to be transferable to other shared control collaboration settings, including AI-enabled autopilots."
175,2023,"Storyboarding a Serious Game Environment for Evaluating the Impact of Empathetic AI for Sonar Operators: Empathetic AI for Sonar Operators nan Sonar operators work under high pressure and stressful conditions, handling large volumes of data that they must quickly perceive and interpret in order to make high-stakes decisions. Artificial Intelligence (AI) has the potential to aid them by processing large amounts of information and highlighting the critical data amongst the noise for the sonar operators to focus on. In order for such an AI to be effective, we need to build a cooperative human-AI team."
176,2023,"Self-beliefs, Transactive Memory Systems, and Collective Identification in Teams: Articulating the Socio-Cognitive Underpinnings of COHUMAIN. nan Socio-cognitive theory conceptualizes individual contributors as both enactors of cognitive processes and targets of a social context's determinative influences. The present research investigates how contributors' metacognition or self-beliefs, combine with others' views of themselves to inform collective team states related to learning about other agents (i.e., transactive memory systems) and forming social attachments with other agents (i.e., collective team identification), both important teamwork states that have implications for team collective intelligence. We test the predictions in a longitudinal study with 78 teams. Additionally, we provide interview data from industry experts in human-artificial intelligence teams. Our findings contribute to an emerging socio-cognitive architecture for COllective HUman-MAchine INtelligence (i.e., COHUMAIN) by articulating its underpinnings in individual and collective cognition and metacognition. Our resulting model has implications for the critical inputs necessary to design and enable a higher level of integration of human and machine teammates."
177,2023,"Are Two Heads Better Than One in AI-Assisted Decision Making? Comparing the Behavior and Performance of Groups and Individuals in Human-AI Collaborative Recidivism Risk Assessment nan With the prevalence of AI assistance in decision making, a more relevant question to ask than the classical question of are two heads better than one? is how groups' behavior and performance in AI-assisted decision making compare with those of individuals'. In this paper, we conduct a case study to compare groups and individuals in human-AI collaborative recidivism risk assessment along six aspects, including decision accuracy and confidence, appropriateness of reliance on AI, understanding of AI, decision-making fairness, and willingness to take accountability. Our results highlight that compared to individuals, groups rely on AI models more regardless of their correctness, but they are more confident when they overturn incorrect AI recommendations. We also find that groups make fairer decisions than individuals according to the accuracy equality criterion, and groups are willing to give AI more credit when they make correct decisions. We conclude by discussing the implications of our work."
178,2024,"Team up with AI or Human? Investigating Candidates' Self-Categorization as Fluidity and Ingroup-Serving Attribution When Judged by a Human-AI Hybrid Jury nan As artificial intelligence (AI) judges are increasingly pervasive in decision-making, it is important to investigate candidates' reactions to decisions made by human-AI hybrid juries. This study investigates candidates' attribution of credit for success and blame for failure to the three agents in question: a human judge, an algorithmic judge, and the candidate oneself. An experiment with 3 (jury type: human-dominated, algorithm-dominated, vs. equally dominated) x 2 (decision outcome: positive vs. negative) between-subjects factorial design was conducted, with 346 valid responses. Our findings demonstrate a partial ingroup-serving attribution dependent on the outcome favorability and a significant effect of relative power status within the human-AI hybrid jury on grouping and attribution. This study reflects the fluidity of identity and self-categorization of human users when facing AI and other humans. We propose that people take a utility-oriented glance at AI in multi-agent decision-making situations."
179,2022,"Who Goes First? Influences of Human-AI Workflow on Decision Making in Clinical Imaging nan Details of the designs and mechanisms in support of human-AI collaboration must be considered in the real-world fielding of AI technologies. A critical aspect of interaction design for AI-assisted human decision making are policies about the display and sequencing of AI inferences within larger decision-making workflows. We have a poor understanding of the influences of making AI inferences available before versus after human review of a diagnostic task at hand. We explore the effects of providing AI assistance at the start of a diagnostic session in radiology versus after the radiologist has made a provisional decision. We conducted a user study where 19 veterinary radiologists identified radiographic findings present in patients' X-ray images, with the aid of an AI tool. We employed two workflow configurations to analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and agreement, (iii) time spent and confidence in decision making, and (iv) perceived usefulness of the AI. We found that participants who are asked to register provisional responses in advance of reviewing AI inferences are less likely to agree with the AI regardless of whether the advice is accurate and, in instances of disagreement with the AI, are less likely to seek the second opinion of a colleague. These participants also reported that the AI advice to be less useful. Surprisingly, requiring provisional decisions on cases in advance of the display of AI inferences did not lengthen the time participants spent on the task. The study provides generalizable and actionable insights for the deployment of clinical AI tools in human-in-the-loop systems and introduces a methodology for studying alternative designs for human-AI collaboration. We make our experimental platform available as open source to facilitate future research on the influence of alternate designs on human-AI workflows."
180,2023,"Designing human-AI systems for complex settings: ideas from distributed, joint, and self-organising perspectives of sociotechnical systems and cognitive work analysis nan Real-world events like the COVID-19 pandemic and wildfires in Australia, Europe, and America remind us that the demands of complex operational settings are met by multiple, distributed teams interwoven with a large array of artefacts and networked technologies, including automation. Yet, current models of human-automation interaction, including those intended for human-machine teaming or collaboration, tend to be dyadic in nature, assuming individual humans interacting with individual machines. Given the opportunities and challenges of emerging artificial intelligence (AI) technologies, and the growing interest of many organisations in utilising these technologies in complex operations, we suggest turning to contemporary perspectives of sociotechnical systems for a way forward. We show how ideas of distributed cognition, joint cognitive systems, and self-organisation lead to specific concepts for designing human-AI systems, and propose that design frameworks informed by contemporary views of complex work performance are needed. We discuss cognitive work analysis as an example.Emerging developments in AI will pose challenges for the design of human-machine systems. Contemporary perspectives of sociotechnical systems, namely distributed cognition, joint cognitive systems, and self-organisation, have design implications that are unaccommodated by traditional methods. Cognitive work analysis may provide a way forward.Abbreviation: AI: Artificial intelligence."
181,2021,"Empowering Adaptive Human Autonomy Collaboration with Artificial Intelligence nan We can now see examples emerge of intelligent distributed hybrid systems with autonomous functions pursuing a specific goal while adapting dynamically to changing environments. Such solutions are made possible by convergence of new technologies, but achieving comprehensive monitoring of the multiple interactions in their organization and functions along their life cycles, in missions and/or safety critical contexts, still challenges system (of systems) engineering practices. This paper considers the main gaps towards trusted systems of systems, including human-AI collaboration, human-machine teaming and solution effectiveness monitoring in a life cycle perspective. These gaps call for an inter-disciplinary sociotechnical approach in engineering towards Adjustable Human Autonomy Collaboration (DUAL), whose justification is outlined in this position paper."
182,2022,"The Components of Trust for Collaborating With AI Colleagues nan AI technologies are capable of improving the performance and productivity of teams in a variety of work contexts. These advantages may be optimized when the AI agent is considered a full team member. A vital component of the agent's acceptance as a team member or colleague is the degree to which its human coworkers feel they can trust it. To explore what factors affect the perceptions of an AI agent as a trustworthy team member and a legitimate colleague, we interviewed twenty-two professionals representing various work roles. Our results revealed that the following qualities contribute to professionals' trust in AI as a colleague: a visual presence reflective of coworkers, engagement in feedback loop and team processes through human communication, and the ability for self-development. These findings contribute to the CSCW community by advancing the current understanding of human-AI teaming and informing the design of trustworthy AI agents into the workplace."
183,2024,"A Survey on Human-AI Teaming with Large Pre-Trained Models [arXiv] nan In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the synergistic potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit."
184,2023,Human-Machine Teaming to Improve Computer Vision nan nan
185,2019,"Six Challenges for Human-AI Co-learning nan The increasing use of ever-smarter AI-technology is changing the way individuals and teams learn and perform their tasks. In hybrid teams, people collaborate with artificially intelligent partners. To utilize the different strengths and weaknesses of human and artificial intelligence, a hybrid team should be designed upon the principles that foster successful human-machine learning and cooperation. The implementation of the identified principles sets a number of challenges. Machine agents should, just like humans, have mental models that contain information about the task context, their own role (self-awareness), and the role of others (theory of mind). Furthermore, agents should be able to express and clarify their mental states to partners. In this paper we identify six challenges for humans and machines to collaborate in an adaptive, dynamic and personalized fashion. Implications for research are discussed."
186,2024,"Don't be fooled: the misinformation effect of explanations in human-ai collaboration nan Across various applications, humans increasingly use black-box artificial intelligence (AI) systems without insight into these systems' reasoning. To counter this opacity, explainable AI (XAI) methods promise enhanced transparency and interpretability. While recent studies have explored how XAI affects human-AI collaboration, few have examined the potential pitfalls caused by incorrect explanations. The implications for humans can be far-reaching but have not been explored extensively. To investigate this, we ran a study (n=160) on AI-assisted decision-making in which humans were supported by XAI. Our findings reveal a misinformation effect when incorrect explanations accompany correct AI advice with implications post-collaboration. This effect causes humans to infer flawed reasoning strategies, hindering task execution and demonstrating impaired procedural knowledge. Additionally, incorrect explanations compromise human-AI team-performance during collaboration. With our work, we contribute to HCI by providing empirical evidence for the negative consequences of incorrect explanations on humans post-collaboration and outlining guidelines for designers of AI."
187,2024,"How Should an AI Trust its Human Teammates? Exploring Possible Cues of Artificial Trust nan In teams composed of humans, we use trust in others to make decisions, such as what to do next, who to help and who to ask for help. When a team member is artificial, they should also be able to assess whether a human teammate is trustworthy for a certain task. We see trustworthiness as the combination of (1) whether someone will do a task and (2) whether they can do it. With building beliefs in trustworthiness as an ultimate goal, we explore which internal factors (krypta) of the human may play a role (e.g., ability, benevolence, and integrity) in determining trustworthiness, according to existing literature. Furthermore, we investigate which observable metrics (manifesta) an agent may take into account as cues for the human teammate's krypta in an online 2D grid-world experiment ( n = 54). Results suggest that cues of ability, benevolence and integrity influence trustworthiness. However, we observed that trustworthiness is mainly influenced by human's playing strategy and cost-benefit analysis, which deserves further investigation. This is a first step towards building informed beliefs of human trustworthiness in human-AI teamwork."
188,2024,"Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant nan Playing partial information, restricted communication, cooperative (PIRCC) games with humans have proven challenging for AI, due to our reliance on social dynamics and sophisticated cognitive techniques. Yet, recent advances in generative AI may change this situation through new forms of human-AI collaboration. This paper investigates how teams of players interact with an AI assistant in the PIRCC game Codenames and the impact this has on cognition, social dynamics, and player experience. We observed gameplay and conducted post-game focus groups with 54 participants across ten groups. Each group played three rounds of Codenames, with an AI assistant supporting Cluegivers. We found the AI assistant enhanced players' convergent and divergent thinking, but interfered with formation of team mental models, highlighting a tension in the use of AI in creative team scenarios. The presence of the AI challenged many players' understanding of the 'spirit of the game'. Furthermore, the presence of the AI assistants weakened social connections between human teammates, but strengthened connections across teams. This paper provides an empirical account of an AI assistant's effect on cognition, social dynamics, and player experience in Codenames. We highlight the opportunities and challenges that arise when designing hybrid digital boardgames that include AI assistants."
189,2021,"Resilience in human-ai-robot teams: widening the scope of measurement nan Project overview: As Human-AI-Robot Teams (HARTs) become prevalent in safety-critical domains, team resilience becomes increasingly relevant for assessing their effectiveness. This study explores a dynamical systems approach to connect interaction-based measures of nominal teamwork with processes and outcomes related to positive adaptation in perturbed team contexts. Method: An experiment was conducted using a remotely-accessible testbed based on Next Generation Combat Vehicle concepts. Groups of 3-participants were recruited to form teams and completed two ground reconnaissance missions set in Minecraft involving movement, communication, and the identification of battlefield objects such as infantry and obstacles. Throughout these missions, teams experienced novel perturbations. Two novel perturbations are the focus of this study: 1) the Lost Connection perturbation(LC) in Mission 1, which disabled one combat vehicle's mobility, requiring repair from the other vehicle, and 2) the Lost Visibility perturbation(LV) in Mission 2, which restricted one combat vehicle's vision to three blocks away (Minecraft units) during a task to transport a civilian to a helicopter extraction site. The measures reported here include measures of team performance in response to perturbations in overall relaxation time (Tvotzky et al., 2012) and broken down across problem solving components (Hoffman and Hancock 2017), and dynamical systems measures of communication turn-taking, determinism (Gorman et al., 2012) and the Hurst exponent (Eke et al., 2002). The previously described measures were applied to two teams, denoted as Team A and Team B. These teams were selected based on their good (Team A) and bad (Team B) responses to the LV perturbation."
190,2020,"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance [arXiv] nan Increasingly, organizations are pairing humans with AI systems to improve decision-making and reducing costs. Proponents of human-centered AI argue that team performance can even further improve when the AI model explains its recommendations. However, a careful analysis of existing literature reveals that prior studies observed improvements due to explanations only when the AI, alone, outperformed both the human and the best human-AI team. This raises an important question: can explanations lead to complementary performance, i.e., with accuracy higher than both the human and the AI working alone? We address this question by devising comprehensive studies on human-AI teaming, where participants solve a task with help from an AI system without explanations and from one with varying types of AI explanation support. We carefully controlled to ensure comparable human and AI accuracy across experiments on three NLP datasets (two for sentiment analysis and one for question answering). While we found complementary improvements from AI augmentation, they were not increased by state-of-the-art explanations compared to simpler strategies, such as displaying the AI's confidence. We show that explanations increase the chance that humans will accept the AI's recommendation regardless of whether the AI is correct. While this clarifies the gains in team performance from explanations in prior work, it poses new challenges for human-centered AI: how can we best design systems to produce complementary performance? Can we develop explanatory approaches that help humans decide whether and when to trust AI input?"
191,2023,"Innovating AI Leadership Education nan This research to practice full paper explores a new educational framework for AI-informed leadership and evaluates its curriculum and pedagogical approach through a novel, tailored, research instrument. Artificial Intelligence continues to rapidly transform many aspects of markets, solutions, and organizational culture across companies, agencies, and institutions in the public and private sectors. Within complex organizations, AI tools, technologies, and applications inform how leaders engage in strategy-making, management, operations, human resources, and professional education. Non-technical managers and executives are increasingly expected to lead teams to implement responsible AI solutions with the promise to improve efficiency, effectiveness, productivity, profitability, and more. AI is rapidly transforming organizational culture, requiring non-technical leaders to develop AI literacy and essential skills to lead teams in implementing responsible AI solutions. In the face of AI-driven change, business leaders need to be AI literate and develop their own essential skills, knowledge, procedures, and perspectives to successfully set vision and strategy to lead teams that can leverage AI to achieve inward-facing and outward-facing business goals. This presents challenges and opportunities to develop new pedagogical approaches and measures to prepare and assess business leaders' AI leadership skills - including understanding human-AI systems in the workplace and their responsible development and ethical use. There are also cultural and organizational behavior challenges in successfully adopting these new capabilities into a global and diverse human-AI workforce at scale. To advance these, we present an innovative hands-on AI leadership curriculum, where participants learn by making and team problem-solving, for United States Air Force (USAF) leaders to learn about AI and its responsible use in human-robot teaming with autonomous robots. We contribute new measures to assess their attitudinal shifts in AI leadership with respect to culture, mindsets, and ethics. We present a pilot study to evaluate our curriculum design and pedagogical approach to foster positive shifts in our AI leadership measures."
192,2023,"Capturing Humans' Mental Models of AI: An Item Response Theory Approach nan Improving our understanding of how humans perceive AI team-mates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction."
193,2023,"Collective Intelligence in Human-AI Teams: A Bayesian Theory of Mind Approach nan We develop a network of Bayesian agents that collectively model the mental states of teammates from the observed communication. Using a generative computational approach to cognition, we make two contributions. First, we show that our agent could generate interventions that improve the collective intelligence of a human-AI team beyond what humans alone would achieve. Second, we develop a real-time measure of human's theory of mind ability and test theories about human cognition. We use data collected from an online experiment in which 145 individuals in 29 human-only teams of five communicate through a chat-based system to solve a cognitive task. We find that humans (a) struggle to fully integrate information from teammates into their decisions, especially when communication load is high, and (b) have cognitive biases which lead them to underweight certain useful, but ambiguous, information. Our theory of mind ability measure predicts both individual- and team-level performance. Observing teams' first 25% of messages explains about 8% of the variation in final team performance, a 170% improvement compared to the current state of the art."
194,2024,"Conceptual Knowledge Modelling for Human-AI Teaming in Data-Frugal Industrial Environments nan When AI interacts with humans in complex environments, such as aerospace manufacturing, safety of operation is of paramount importance. Trustworthiness of AI needs to be ensured through, among other things, explainability of its behaviour and rationale, which is typically a challenge for current deep neural network-based systems.We tackle the knowledge comprehensibility aspect of intrinsic explainability by suggesting a concept-level environment awareness model combining various complementary knowledge sources - statistical learning using dedicated property detectors through publicly available software, and crowd-sourced common-sense knowledge graphs. Our approach also addresses the issue of data-frugal learning, typical for environments with highly specific purpose-built artefacts. We adopt Gardenfors's Conceptual Spaces as a cognitively-motivated knowledge representation framework and apply our typicality quantification model in a use case on interpretable classification of manufacturing artefacts."
195,2021,"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance nan Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AIs recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?"
196,2021,"Human-AI-Collaboration in the Context of Information Asymmetry - A Behavioral Analysis of Demand Forecasting nan Digitalization enables the full potential ofArtificial Intelligence for the first time. This study deals with demand forecasting as a representation of supply chain planning. Statistical and judgmental approaches constitute the state-of-theart in methodology, but also present drawbacks such as human mental capacity constraints or data biases. Teaming of humans and AI promises synergies and better solutions, but challenging questions on how to organize collaborative tasks remain. Information asymmetry states an unsolved issue, as digitalization is going to take more time to be holistically established. Deploying a behavior analysis of an industrial case study, this paper investigates the impact of two different forms of interactions on the forecasting performance and ability of human planners to compensate the lack of contextual information included in anAI-based prediction. The results indicate that information asymmetry limits the magnitude of the decisionmaking anchor provided by the algorithm and affects the accuracy depending on the specific interaction form. Overall, an asymmetric sequential interaction set-up outperforms the other forecasts. Finally, this study states implications and limitations for human-AI collaboration."
197,2022,"THE IMPACT OF A STRATEGY OF DECEPTION ABOUT THE IDENTITY OF AN ARTIFICIAL INTELLIGENCE TEAMMATE ON HUMAN DESIGNERS nan Advances in artificial intelligence (AI) offer new opportunities for human-AI collaboration in engineering design. Human trust in AI is a crucial factor in ensuring an effective human-AI collaboration, and several approaches to enhance human trust in AI have been suggested in prior studies. However, it remains an open question in engineering design whether a strategy of deception about the identity of an AI teammate can effectively calibrate human trust in AI and improve human-AI joint performance. This research assesses the impact of the strategy of deception on human designers through a human subjects study where half of participants are told that they work with an AI teammate (i.e., without deception), and the other half of participants are told that they work with another human participant but in fact they work with an AI teammate (i.e., with deception). The results demonstrate that, for this study, the strategy of deception improves high proficiency human designers' perceived competency of their teammate. However, the strategy of deception does not raise the average number of team collaborations and does not improve the average performance of high proficiency human designers. For low proficiency human designers, the strategy of deception does not change their perceived competency and helpfulness of their teammate, and further reduces the average number of team collaborations while hurting their average performance at the beginning of the study. The potential reasons behind these results are discussed with an argument against using the strategy of deception in engineering design."
198,2023,"Outlining the design space of eXplainable swarm (xSwarm): experts perspective [arXiv] nan In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual's ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improve the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens the discussion on xSwarm and paves the way for more research in the field."
199,2021,"Studying human-AI collaboration protocols: the case of the Kasparov's law in radiological double reading nan Purpose The integration of Artificial Intelligence into medical practices has recently been advocated for the promise to bring increased efficiency and effectiveness to these practices. Nonetheless, little research has so far been aimed at understanding the best human-AI interaction protocols in collaborative tasks, even in currently more viable settings, like independent double-reading screening tasks. Methods To this aim, we report about a retrospective case-control study, involving 12 board-certified radiologists, in the detection of knee lesions by means of Magnetic Resonance Imaging, in which we simulated the serial combination of two Deep Learning models with humans in eight double-reading protocols. Inspired by the so-called Kasparov's Laws, we investigate whether the combination of humans and AI models could achieve better performance than AI models alone, and whether weak reader, when supported by fit-for-use interaction protocols, could out-perform stronger readers. Results We discuss two main findings: groups of humans who perform significantly worse than a state-of-the-art AI can significantly outperform it if their judgements are aggregated by majority voting (in concordance with the first part of the Kasparov's law); small ensembles of significantly weaker readers can significantly outperform teams of stronger readers, supported by the same computational tool, when the judgments of the former ones are combined within fit-for-use protocols (in concordance with the second part of the Kasparov's law). Conclusion Our study shows that good interaction protocols can guarantee improved decision performance that easily surpasses the performance of individual agents, even of realistic super-human AI systems. This finding highlights the importance of focusing on how to guarantee better co-operation within human-AI teams, so to enable safer and more human sustainable care practices."
200,2023,"COHUMAIN: Building the Socio-Cognitive Architecture of Collective Human-Machine Intelligence. nan In recent years, we have experienced rapid development of advanced technology, machine learning, and artificial intelligence (AI), intended to interact with and augment the abilities of humans in practically every area of life. With the rapid growth of new capabilities, such as those enabled by generative AI (e.g., ChatGPT), AI is increasingly at the center of human communication and collaboration, resulting in a growing recognition of the need to understand how humans and AI can integrate their inputs in collaborative teams. However, there are many unanswered questions regarding how human-AI collective intelligence will emerge and what the barriers might be. Truly integrated collaboration between humans and intelligent agents may result in a different way of working that looks nothing like what we know now, and it is important to keep the essential goal of human societal well-being and prosperity a priority. In this special issue, we begin to scope out the underpinnings of a socio-cognitive architecture for Collective HUman-MAchine INtelligence (COHUMAIN), which is the study of the capability of an integrated human and machine (i.e., intelligent technology) system to achieve goals in a wide range of environments. This topic consists of nine papers including a description of the conceptual foundation for a socio-cognitive architecture for COHUMAIN, empirical tests of some aspects of this architecture, research on proposed representations of intelligent agents that can jointly interact with humans, empirical tests of human-human and human-machine interactions, and philosophical and ethical issues to consider as we develop these systems."
201,2020,Toward Responsible Al by Planning to Fail nan nan
202,2024,"What you say vs what you do: Utilizing positive emotional expressions to relay AI teammate intent within human-AI teams nan With the expansive growth of AI's capabilities in recent years, researchers have been tasked with developing and improving human-centered AI collaborations, necessitating the creation of human-AI teams (HATs). However, the differences in communication styles between humans and AI often prevent human teammates from fully understanding the intent and needs of AI teammates. One core difference is that humans naturally leverage a positive emotional tone during communication to convey their confidence or lack thereof to convey doubt in their ability to complete a task. Yet, this communication strategy must be explicitly designed in order for an AI teammate to be human-centered. In this mixed-methods study, 45 participants completed a study examining how human teammates interpret the behaviors of their AI teammates when they express different positive emotions via specific words/phrases. Quantitative results show that, based on corresponding behaviors, AI teammates were able to use displays of emotion to increase trust in AI teammates and the positive mood of the human teammate. Additionally, our qualitative findings indicate that participants preferred their AI teammates to increase the intensity of their displayed emotions to help reduce the perceived risk of their AI teammate's behavior. When taken in sum, these findings describe the relevance of AI teammates expressing intensities of emotion while performing various behavioral decisions as a continued means to provide social support to the wider team and better task performance."
203,2022,"Towards Visual Explainable Active Learning for Zero-Shot Classification nan Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification."
204,2024,"ADVISE: Accelerating the Creation of Evidence Syntheses for Global Development Using Natural Language Processing-Supported Human-Artificial Intelligence Collaboration nan When designing evidence-based policies and programs, decision-makers must distill key information from a vast and rapidly growing literature base. Identifying relevant literature from raw search results is time and resource intensive, and is often done by manual screening. In this study, we develop an artificial intelligence (AI) agent based on a bidirectional encoder representations from transformers (BERT) model and incorporate it into a human team designing an evidence synthesis product for global development. We explore the effectiveness of the human-AI hybrid team in accelerating the evidence synthesis process. To further improve team efficiency, we enhance the human-AI hybrid team through active learning (AL). Specifically, we explore different sampling strategies, including random sampling, least confidence (LC) sampling, and highest priority (HP) sampling, to study their influence on the collaborative screening process. Results show that incorporating the BERT-based AI agent into the human team can reduce the human screening effort, i.e., the number of documents that humans need to screen, by 68.5% compared to the case of no AI assistance and by 16.8% compared to the industry-standard case of using a frequency-based language model and support vector machine-based classifier for identifying 80% of all relevant documents. When we apply the HP sampling strategy, the human screening effort can be reduced even more: by 78.3% for identifying 80% of all relevant documents compared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming workflow in the design process of three evidence gap maps for U.S. Agency for International Development and find it to be highly effective. These findings demonstrate how AI can accelerate the development of evidence synthesis products and promote timely evidence-based decision-making in global development."
205,2021,Towards Human-AI Cooperation on Sequential Decision Making Problems nan nan
206,2021,"A Two-Dimensional Explanation Framework to Classify AI as Incomprehensible, Interpretable, or Understandable nan Because of recent and rapid developments in Artificial Intelligence (AI), humans and AI-systems increasingly work together in human-agent teams. However, in order to effectively leverage the capabilities of both, AI-systems need to be understandable to their human teammates. The branch of eXplainable AI (XAI) aspires to make AI-systems more understandable to humans, potentially improving human-agent teamwork. Unfortunately, XAI literature suffers from a lack of agreement regarding the definitions of and relations between the four key XAI-concepts: transparency, interpretability, explainability, and understand-ability. Inspired by both XAI and social sciences literature, we present a two-dimensional framework that defines and relates these concepts in a concise and coherent way, yielding a classification of three types of AIsystems: incomprehensible, interpretable, and understandable. We also discuss how the established relationships can be used to guide future research into XAI, and how the framework could be used during the development of AI-systems as part of human-AI teams."
207,2019,"Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff nan AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of updates to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to behavioral changes that are at odds with the user's prior experiences and confidence in the AI's inferences. We show that updates that increase AI performance may actually hurt team performance. We introduce the notion of the compatibility of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates."
209,2024,"Friend or Foe? Teaming Between Artificial Intelligence and Workers with Variation in Experience nan As artificial intelligence (AI) applications become more pervasive, it is critical to understand how knowledge workers with different levels and types of experience can team with AI for productivity gains. We focus on the influence of two major types of human work experience (narrow experience based on the specific task volume and broad experience based on seniority) on the human-AI team dynamics. We developed an AI solution for medical chart coding in a publicly traded company and conducted a field study among the knowledge workers. Based on a detailed analysis performed at the medical chart level, we find evidence that AI benefits workers with greater task-based experience, but senior workers gain less from AI than their junior colleagues. Further investigation reveals that the relatively lower productivity lift from AI is not a result of seniority per se but lower trust in AI, likely triggered by the senior workers' broader job responsibilities. This study provides new empirical insights into the differential roles of worker experience in the collaborative dynamics between AI and knowledge workers, which have important societal and business implications."
210,2019,"Human Role in Digital Logistics: Relevance of Intuition in Interacting with AI nan Digital developments for logistics include many general and specific concepts as for example automation and Industry 4.0, Internet of Things (IoT), Physical Internet (PI) or Cyber-physical Systems (CPS). Overall, the human role in such settings will see profound changes - and many fears from workers are arising especially as there is no positive definition of new human work roles and expectations yet. We analyze the role of human intuition within an IoT and artificial intelligence application environment in logistics and supply chain processes and how it can be developed. Such a positive concept of increased efficiency by human-AI teams is an important cornerstone for digitalization as otherwise obstruction and fear may prevail with logistics and production workers."
211,2024,"A Unifying Post-Processing Framework for Multi-Objective Learn-to-Defer Problems nan Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a $d$-dimensional generalization to the fundamental lemma of Neyman and Pearson (d-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of baselines."
212,2023,"Approaching (super)human intent recognition in stag hunt with the Naive Utility Calculus generative model nan The human ability to utilize social and behavioral cues to infer each other's intents, infer motivations, and predict future actions is a central process to human social life. This ability represents a facet of human cognition that artificial intelligence has yet to fully mimic and master. Artificial agents with greater social intelligence have wide-ranging applications from enabling the collaboration of human-AI teams to more accurately modelling human behavior in complex systems. Here, we show that the Naive Utility Calculus generative model is capable of competing with leading models in intent recognition and action prediction when observing stag-hunt, a simple multiplayer game where agents must infer each other's intentions to maximize rewards. Moreover, we show the model is the first with the capacity to out-compete human observers in intent recognition after the first round of observation. We conclude with a discussion on implications for the Naive Utility Calculus and of similar generative models in general."
214,2021,"Supporting Social Interactions In Human-Ai Teams: Profiling Human Teammates From Sparse Data nan Artificial intelligence has been developed to perform all manner of tasks but has not gained capabilities to support social cognition. We suggest that teams comprised of both humans and artificially intelligent agents cannot achieve optimal team performance unless all teammates have the capacity to employ social-cognitive mechanisms. These form the foundation for generating inferences about their counterparts and enable execution of informed, appropriate behaviors. Social intelligence and its utilization are known to be vital components of human-human teaming processes due to their importance in guiding the recognition, interpretation, and use of the signals that humans naturally use to shape their exchanges. Although modern sensors and algorithms could allow AI to observe most social cues, signals, and other indicators, the approximation of human-to-human social interaction -based upon aggregation and modeling of such cues is currently beyond the capacity of potential AI teammates. Partially, this is because humans are notoriously variable. We describe an approach for measuring social-cognitive features to produce the raw information needed to create human agent profiles that can be operated upon by artificial intelligences."
215,2023,"ReadingQizMaker: A Human-NLP Collaborative System that Supports Instructors to Design High-Qality Reading Qiz Qestions nan Despite that reading assignments are prevalent, methods to encourage students to actively read are limited. We propose a system ReadingQuizMaker that supports instructors to conveniently design high-quality questions to help students comprehend readings. ReadingQuizMaker adapts to instructors' natural workfows of creating questions, while providing NLP-based process-oriented support. ReadingQuizMaker enables instructors to decide when and which NLP models to use, select the input to the models, and edit the outcomes. In an evaluation study, instructors found the resulting questions to be comparable to their previously designed quizzes. Instructors praised ReadingQuizMaker for its ease of use, and considered the NLP suggestions to be satisfying and helpful. We compared ReadingQuizMaker with a control condition where instructors were given automatically generated questions to edit. Instructors showed a strong preference for the human-AI teaming approach provided by ReadingQuizMaker. Our fndings suggest the importance of giving users control and showing an immediate preview of AI outcomes when providing AI support."
216,2019,"Human-AI Learning Performance in Multi-Armed Bandits nan People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some set tings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making."
217,2024,"Conformal Prediction Sets Improve Human Decision Making [arXiv] nan In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams."
218,2024,"Human factors considerations for the context-aware design of adaptive autonomous teammates nan Despite the gains in performance that AI can bring to human-AI teams, they also present them with new challenges, such as the decline in human ability to respond to AI failures as the AI becomes more autonomous. This challenge is particularly dangerous in human-AI teams, where the AI holds a unique role in the team's success. Thus, it is imperative that researchers find solutions for designing AI team-mates that consider their human team-mates' needs in their adaptation logic. This study explores adaptive autonomy as a solution to overcoming these challenges. We conducted twelve contextual inquiries with professionals in two teaming contexts in order to understand how human teammate perceptions can be used to determine optimal autonomy levels for AI team-mates. The results of this study will enable the human factors community to develop AI team-mates that can enhance their team's performance while avoiding the potentially devastating impacts of their failures.As AI becomes more autonomous, the human ability to detect and respond to their failures decreases as they become less a part of the AI's decision-making loop. This contextual inquiry study shows how human factors are affected by and should influence the design of adaptive AI team-mates in different teaming contexts."
219,2024,"MS Slide Designer: A Study on Human-AI Collaboration for Content Creation nan Human-AI collaboration can be used to improve the competency of people in the creative content space. Nevertheless, users struggle to understand what strategies can lead to a better partnership that can help them in creating aesthetic, creative, and coherent content. As part of our study, participants are requested to create slide presentations and asked to use the Microsoft Slide Designer AI tool as part of their design process. Thereafter, our team conducts comprehensive user-consented interviews to understand the users' experience with the tool, learn the kind of AI suggestions users would accept, and acquire knowledge about AI teaching good slide design. The study highlights that MS Slide Designer performs quite well when it deals with different forms of text data such as recognizing the context of words or recommending aesthetic themes for title or subtitle slides. However, the user's control over the presentation process is limited, resulting in a highly iterative slide creation process. This limitation becomes evident through user experiences, such as the user's inability to revert to previous slide versions after selecting a design, and the lack of consistent theme maintenance for the presentation. Based on the findings collected through the user interviews conducted, our research suggests strategies through which humans can perform more effective human-AI collaboration in the creative content space. One of our strategies highlights the need for MS Designer to be more interpretable seen through a new feature suggestion asking for tags with each design recommendation."
220,2021,"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores nan Explaining the decisions of an Artificial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers have either proposed new feature attribution methods, discussed or harnessed these tools in their work. However, despite humans being the target end-users, most attribution methods were only evaluated on proxy automatic-evaluation metrics [60, 78, 80]. In this paper, we conduct the first user study to measure attribution map effectiveness in assisting humans in ImageNet classification and Stanford Dogs fine-grained classification, and when an image is natural or adversarial (i.e. contains adversarial perturbations). Overall, feature attribution is surprisingly not more effective than showing humans nearest training-set examples. On a harder task of fine-grained dog categorization, presenting attribution maps to humans does not help, but instead hurts the performance of human-AI teams compared to AI alone. Importantly, we found automatic attribution-map evaluation measures to correlate poorly with the actual human-AI team performance. Our findings encourage the community to rigorously test their methods on the downstream human-in-the-loop applications and to rethink the existing evaluation metrics."
221,2024,"Explainable AI Enhances Glaucoma Referrals, Yet the Human-AI Team Still Falls Short of the AI Alone nan Primary care providers are vital for initial triage and referrals to specialty care. In glaucoma, asymptomatic and fast progression can lead to vision loss, necessitating timely referrals to specialists. However, primary eye care providers may not identify urgent cases, potentially delaying care. Artificial Intelligence (AI) offering explanations could enhance their referral decisions. We investigate how various AI explanations help providers distinguish between patients needing immediate or non-urgent specialist referrals. We built explainable AI algorithms to predict glaucoma surgery needs from routine eyecare data as a proxy for identifying high-risk patients. We incorporated intrinsic and post-hoc explainability and conducted an online study with optometrists to assess human-AI team performance, measuring referral accuracy and analyzing interactions with AI, including agreement rates, task time, and user experience perceptions. AI support enhanced referral accuracy among 87 participants (59.9%/50.8% with/without AI), though Human-AI teams underperformed compared to AI alone. Participants believed they included AI advice more when using the intrinsic model, and perceived it more useful and promising. Without explanations, deviations from AI recommendations increased. AI support did not increase workload, confidence, and trust, but reduced challenges. On a separate test set, our black-box and intrinsic models achieved an accuracy of 77% and 71%, respectively, in predicting surgical outcomes. We identify opportunities of human-AI teaming for glaucoma management in primary eye care, noting that while AI enhances referral accuracy, it also shows a performance gap compared to AI alone, even with explanations. Human involvement remains essential in medical decision making, underscoring the need for future research to optimize collaboration, ensuring positive experiences and safe AI use."
222,2024,"Gender Differences and Social Design in Human-AI Collaboration: Insights from Virtual Cobot Interactions Under Varying Task Loads nan This work explores the effects of users' gender and social design features of AI under different task load conditions on human-like attributions, social impact, work performance and perceived workload, user experience, and various other measures in Human-AI Interaction (HAII). Users had to execute sorting and dispatch tasks in collaboration with a virtual cobot. The degree of social gestalt of the cobot was varied by the ability to make small talk (i.e., talkative vs. non-talkative cobot), and the task load was increased by adding a secondary task (i.e., high vs. low task load condition). Overall, the talkative cobot led to a more positive perception of the cobot and increased social qualities like sense of meaning and team membership compared to the non-talkative cobot. The following gender effect was particularly interesting. The talkative cobot had a buffering effect for women and a distraction conflict effect for men in high task load conditions. When interacting with the talkative robot, women find the high task condition less stressful. In contrast thereto, the talkative cobot was distracting for men in the high task load condition. Our results highlight that social design choices and interindividual differences influence a successful collaboration between humans and AI. The work also shows the added value of systematic XR-simulations for the investigation and design of human-centered HAIIs (eXtended AI approach)."
223,2023,"Responsible (use of) AI. nan Although there is a rich history of philosophical definitions of ethics when applied to human behavior, applying the same concepts and principles to AI may be fraught with problems. Anthropomorphizing AI to have characteristics such as ethics may promote a dangerous, unrealistic expectation that AI can be trained to have inherent, guaranteed ethical behavior. The authors instead advocate for increased research into the ethical use of AI from initial ideation and design through operational use and sustainment. The authors advocate for five key research areas: (1) education in ethics and core AI concepts for AI developers, leaders, and users, (2) development and use of model cards or datasheets for datasets to provide transparency into the strengths, limits, and potential biases of a trained model, (3) employing human-centered design that seeks to understand human value structures within a task context and enable effective human-machine interaction through intuitive and transparent interfaces, (4) targeted use of run time assurance that monitors and modifies the inputs or outputs of a trained model when necessary to enforce ethical principles such as safety or limiting bias, and (5) developing best practices for the use of a joint human-AI co-creation and training experience to enable a shared mental model and higher performance through potential emergent behavior."
224,2020,"Personalization in Human-AI Teams: Improving the Compatibility-Accuracy Tradeoff [arXiv] nan AI systems that model and interact with users can update their models over time to reflect new information and changes in the environment. Although these updates can improve the performance of the AI system, they may actually hurt the performance for individual users. Prior work has studied the trade-off between improving the system accuracy following an update and the compatibility of the update with prior user experience. The more the model is forced to be compatible with prior updates, the higher loss in accuracy it will incur. In this paper, we show that in some cases it is possible to improve this compatibility-accuracy trade-off relative to a specific user by employing new error functions for the AI updates that personalize the weight updates to be compatible with the user's history of interaction with the system and present experimental results indicating that this approach provides major improvements to certain users."
225,2024,"Collective Attention in Human-AI Teams nan How does the presence of an AI assistant affect the collective attention of a team? We study 20 human teams of 3-4 individuals paired with one voice-only AI assistant during a challenging puzzle task. Teams are randomly assigned to an AI assistant with a human- or robotic-sounding voice that provides either helpful or misleading information about the task. Treating each individual AI interjection as a treatment intervention, we identify the causal effects of the AI on dynamic group processes involving language use. Our findings demonstrate that the AI significantly affects what teams discuss, how they discuss it, and the alignment of their mental models. Teams adopt AI-introduced language for both terms directly related to the task and for peripheral terms, even when they (a) recognize the unhelpful nature of the AI, (b) do not consider the AI a genuine team member, and (c) do not trust the AI. The process of language adaptation appears to be automatic, despite doubts about the AI's competence. The presence of an AI assistant significantly impacts team collective attention by modulating various aspects of shared cognition. This study contributes to human-AI teaming research by highlighting collective attention as a central mechanism through which AI systems in team settings influence team performance. Understanding this mechanism will help CSCW researchers design AI systems that enhance team collective intelligence by optimizing collective attention."
226,2023,"Human AI Teaming for Coronary CT Angiography Assessment: Impact on Imaging Workflow and Diagnostic Accuracy nan As the number of coronary computed tomography angiography (CTA) examinations is expected to increase, technologies to optimize the imaging workflow are of great interest. The aim of this study was to investigate the potential of artificial intelligence (AI) to improve clinical workflow and diagnostic accuracy in high-volume cardiac imaging centers. A total of 120 patients (79 men; 62.4 (55.0-72.7) years; 26.7 (24.9-30.3) kg/m(2)) undergoing coronary CTA were randomly assigned to a standard or an AI-based (human AI) coronary analysis group. Severity of coronary artery disease was graded according to CAD-RADS. Initial reports were reviewed and changes were classified. Both groups were similar with regard to age, sex, body mass index, heart rate, Agatston score, and CAD-RADS. The time for coronary CTA assessment (142.5 (106.5-215.0) s vs. 195.0 (146.0-265.5) s; p < 0.002) and the total reporting time (274.0 (208.0-377.0) s vs. 350 (264.0-445.5) s; p < 0.02) were lower in the human AI than in the standard group. The number of cases with no, minor, or CAD-RADS relevant changes did not differ significantly between groups (52, 7, 1 vs. 50, 8, 2; p = 0.80). AI-based analysis significantly improves clinical workflow, even in a specialized high-volume setting, by reducing CTA analysis and overall reporting time without compromising diagnostic accuracy."
227,2023,How to Make Agents and Influence Teammates: Understanding the Social Influence AI Teammates Have in Human-AI Teams nan nan
228,2022,"Visual correspondence-based explanations improve AI robustness and human-AI team accuracy nan Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (+1 to +4 points) on out-of-distribution (OOD) datasets while performing marginally worse (-1 to -2 points) on in-distribution tests than ResNet-50 and a k-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI team accuracy (i.e., that is higher than either AI-alone or human-alone), in ImageNet and CUB image classification tasks."
229,2024,"Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity [arXiv] nan Collaboration is key to STEM, where multidisciplinary team research can solve complex problems. However, inequality in STEM fields hinders their full potential, due to persistent psychological barriers in underrepresented students' experience. This paper documents teamwork in STEM and explores the transformative potential of computational modeling and generative AI in promoting STEM-team diversity and inclusion. Leveraging generative AI, this paper outlines two primary areas for advancing diversity, equity, and inclusion. First, formalizing collaboration assessment with inclusive analytics can capture fine-grained learner behavior. Second, adaptive, personalized AI systems can support diversity and inclusion in STEM teams. Four policy recommendations highlight AI's capacity: formalized collaborative skill assessment, inclusive analytics, funding for socio-cognitive research, human-AI teaming for inclusion training. Researchers, educators, policymakers can build an equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration, offering a vision for the future of STEM where diverse voices are actively encouraged and heard within collaborative scientific endeavors."
230,2024,"Optimizing risk-averse human-ai hybrid teams nan We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations."
231,2021,"A Human-AI Teaming Approach for Incremental Taxonomy Learning from Text nan Taxonomies provide a structured representation of semantic relations between lexical terms, acting as the backbone of many applications. The research proposed herein addresses the topic of taxonomy enrichment using an human-in-the-loop semi-supervised approach. I will be investigating possible ways to extend and enrich a taxonomy using corpora of unstructured text data. The objective is to develop a methodological framework potentially applicable to any domain."
232,2024,"Are You Really Sure? Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making nan In AI-assisted decision-making, it is crucial but challenging for humans to achieve appropriate reliance on AI. This paper approaches this problem from a human-centered perspective, human self-confidence calibration. We begin by proposing an analytical framework to highlight the importance of calibrated human self-confidence. In our first study, we explore the relationship between human self-confidence appropriateness and reliance appropriateness. Then in our second study, We propose three calibration mechanisms and compare their effects on humans' self-confidence and user experience. Subsequently, our third study investigates the effects of self-confidence calibration on AI-assisted decision-making. Results show that calibrating human self-confidence enhances human-AI team performance and encourages more rational reliance on AI (in some aspects) compared to uncalibrated baselines. Finally, we discuss our main findings and provide implications for designing future AI-assisted decision-making interfaces."
233,2021,"Understanding the Effect of Out-of-distribution Examples and Interactive Explanations on Human-AI Decision Making [arXiv] nan Although AI holds promise for improving human decision making in societally critical domains, it remains an open question how human-AI teams can reliably outperform AI alone and human alone in challenging prediction tasks (also known as complementary performance). We explore two directions to understand the gaps in achieving complementary performance. First, we argue that the typical experimental setup limits the potential of human-AI teams. To account for lower AI performance out-of-distribution than in-distribution because of distribution shift, we design experiments with different distribution types and investigate human performance for both in-distribution and out-of-distribution examples. Second, we develop novel interfaces to support interactive explanations so that humans can actively engage with AI assistance. Using in-person user study and large-scale randomized experiments across three tasks, we demonstrate a clear difference between in-distribution and out-of-distribution, and observe mixed results for interactive explanations: while interactive explanations improve human perception of AI assistance's usefulness, they may magnify human biases and lead to limited performance improvement. Overall, our work points out critical challenges and future directions towards complementary performance."
235,2023,"How Displaying AI Confidence Affects Reliance and Hybrid Human-AI Performance nan Confidence signals are often used in human interactions to communicate the likelihood of a decision being correct. Similarly, confidence may also be used to indicate the reliability of advice given by an AI. While previous work on explainable AI (XAI) has explored the effect of AI confidence on AI-advice adoption and joint accuracy of the human-AI team, most studies use AI-assistants that exceed human performance. It is unclear how displaying the confidence interacts with the accuracy of the AI. We conduct a comprehensive investigation of the effect of displaying AI confidence on two factors: 1) the accuracy of AI-assisted decision making, and 2) reliance on the AI's assistance. We conduct two behavioral experiments, one where participants were shown AI confidence, and another where no confidence ratings were shown. Our work goes beyond the typical focus on high accuracy AI assistants. In both experiments, participants were assisted by one of three AI classifiers of varying accuracy. Our results demonstrate that displaying AI confidence increases joint accuracy when people are assisted by a classifier that is better than humans on average. Conversely, when assisted by a classifier with performance worse than an average human, joint accuracy was better when no AI confidence was displayed. However, for the adoption of AI advice we observed the opposite pattern: people rely more on a higher accuracy classifier that does not display confidence compared to one that does, and people rely more on a lower accuracy classifier that does display AI confidence compared to one that does not."
236,2023,"Human-AI Teaming in the Automotive and Mobility Industry: Guiding Design to Support Joint Activity nan AI technologies hold the potential to improve safety, efficiency, and user experience in automotive and mobility applications, yet to make them useful tools they need to effectively team with humans. Based on field research and experiences on how people coordinate, descriptive models of joint activity propose requirements for effective coordination that can inform the design of cooperative competencies of AI technologies in this domain. Yet, the usefulness of joint activity constructs for design is not only determined by their theoretical validity but also by their practical ability to influence designers. To enable the broader adoption of joint activity constructs, our study aims to characterize the context of design in the automotive industry and develop design guidance to support joint activity. We present our ongoing work to evaluate the ability of these constructs to inform design and to understand their fit within the context of design in the automotive industry."
237,2024,"Looping in the Human: Collaborative and Explainable Bayesian Optimization nan Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization. We validate CoExBO's efficacy through human-AI teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods. Code is available https://github.com/ma921/CoExBO."
239,2021,"Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork nan AI practitioners typically strive to develop the most accurate systems, making an implicit assumption that the AI system will function autonomously. However, in practice, AI systems often are used to provide advice to people in domains ranging from criminal justice and finance to healthcare. In such Al-advised decision making, humans and machines form a team, where the human is responsible for making final decisions. But is the most accurate AI the best teammate'? We argue not necessarily - predictable performance may be worth a slight sacrifice in AI accuracy. Instead, we argue that AI systems should be trained in a human-centered manner, directly optimized for team performance. We study this proposal for a specific type of human-AI teaming, where the human overseer chooses to either accept the AI recommendation or solve the task themselves. To optimize the team performance for this setting we maximize the team's expected utility, expressed in terms of the quality of the final decision, cost of verifying, and individual accuracies of people and machines. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the most accuracy AI may not lead to highest team performance and show the benefit of modeling teamwork during training through improvements in expected team utility across datasets, considering parameters such as human skill and the cost of mistakes. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on AI optimization problems motivated by human-AI collaboration."
240,2023,"Investigating the Effects of Perceived Teammate Artificiality on Human Performance and Cognition nan Teammates powered by artificial intelligence (AI) are becoming more prevalent and capable in their abilities as a teammate. While these teammates have great potential in improving team performance, empirical work that explores the impacts of these teammates on the humans they work with is still in its infancy. Thus, this study explores how the inclusion of AI teammates impacts both the performative abilities of human-AI teams in addition to the perceptions those humans form. The current study found that participants perceiving their third teammate as artificial performed worse than those perceiving them as human. Furthermore, these performance differences were significantly moderated by the task's difficulty, with participants in the AI teammate condition significantly outperforming participants perceiving a human teammate in the highest difficulty task, which diverges from previous human-AI teaming literature. Alternatively, no significant effect of perceived teammate artificiality was found on shared mental model similarity. However, it did significantly affect participants' levels of perceived team cognition. Individual performance on medium difficulty maps also mediated the effect of perceived teammate artificiality on perceived team cognition. These results further build on the current understanding of how AI teammates impact perceptions of individual human teammates and how those perceptions subsequently impact their objective performance, which is critical in building more effective AI teammates to incorporate alongside humans."
241,2023,"AI in human teams: effects on technology use, members' interactions, and creative performance under time scarcity nan Time and technology permeate the fabric of teamwork across a variety of settings to affect outcomes which have a wide range of consequences. However, there is a limited understanding about the interplay between these factors for teams, especially as applied to artificial intelligence (AI) technology. With the increasing integration of AI into human teams, we need to understand how environmental factors such as time scarcity interact with AI technology to affect team behaviors. To address this gap in the literature, we investigated the interaction between the availability of intelligent technology and time scarcity in teams. Drawing from the theoretical perspective of computers are social actors and extant research on the use of heuristics and human-AI interaction, this study uses behavioral data from 56 teams who participated in a between-subjects 2 (intelligent assistant available*control/no intelligent assistant)*2 (time scarcity*control/no time scarcity) lab experiment. Results show that teams working under time scarcity used the intelligent assistant more often and underperformed on a creative task compared to teams without the temporal constraints. Further, teams who had an intelligent assistant available to them had fewer interactions between members compared to teams who did not have the technology. Implications for research and applications are discussed."
242,2022,"Responsible Human-Centered Artificial Intelligence for the Cognitive Enhancement of Knowledge Workers nan Over the past decade, the demand for high-performing knowledge workers (KWs) has grown at an unprecedented rate and shows no signs of slowing. Researchers, designers, engineers, and executives are examples of KWs that perform non-routine, creative work. The work outcomes of KWs as individuals, teams, and organizations play a vital role in the global economy and quality of life. One of the most significant challenges KWs face is balancing stressors on their cognitive and emotional well-being while seeking high productivity. Human cognitive enhancement proposes improving human abilities to acquire and generate knowledge and understand the world. Our cognitive enhancement application for KWs, called the Flow Choice Architecture (FCA), senses their cognitive and affective states, adds context, and recommends appropriate nudges to maximize their healthy flow time. This study provides insights into how FCA implements Human-Centered Design and Responsible Artificial Intelligence (RAI) principles as an interactive AI-powered application that promotes healthy flow performance during knowledge work. FCA applied the RAI tools from Microsoft's Human-AI eXperience Toolkit to evaluate FCA-specific scenarios. By defining FCA as a hybrid recommendation system and conversational AI agent, we found the following categories of human-AI failure scenarios in FCA: input errors, trigger errors, delimiter errors, and response generation errors. We recommend simulating these errors and undesirable behaviors to improve the design of explainable nudges, meaningful metrics, and well-tuned triggers. The outcome of this RAI evaluation was a robust FCA system design that meets the needs of KWs and enhances their capability to thrive and flourish at work."
243,2018,"Human-AI Learning Performance in Multi-armed Bandits [arXiv] nan People frequently face challenging decision-making problems in which outcomes are uncertain or unknown. Artificial intelligence (AI) algorithms exist that can outperform humans at learning such tasks. Thus, there is an opportunity for AI agents to assist people in learning these tasks more effectively. In this work, we use a multi-armed bandit as a controlled setting in which to explore this direction. We pair humans with a selection of agents and observe how well each human-agent team performs. We find that team performance can beat both human and agent performance in isolation. Interestingly, we also find that an agent's performance in isolation does not necessarily correlate with the human-agent team's performance. A drop in agent performance can lead to a disproportionately large drop in team performance, or in some settings can even improve team performance. Pairing a human with an agent that performs slightly better than them can make them perform much better, while pairing them with an agent that performs the same can make them them perform much worse. Further, our results suggest that people have different exploration strategies and might perform better with agents that match their strategy. Overall, optimizing human-agent team performance requires going beyond optimizing agent performance, to understanding how the agent's suggestions will influence human decision-making."
244,2022,"You Complete Me: Human-AI Teams and Complementary Expertise nan People consider recommendations from AI systems in diverse domains ranging from recognizing tumors in medical images to deciding which shoes look cute with an outfit. Implicit in the decision process is the perceived expertise of the AI system. In this paper, we investigate how people trust and rely on an AI assistant that performs with different levels of expertise relative to the person, ranging from completely overlapping expertise to perfectly complementary expertise. Through a series of controlled online lab studies where participants identified objects with the help of an AI assistant, we demonstrate that participants were able to perceive when the assistant was an expert or non-expert within the same task and calibrate their reliance on the AI to improve team performance. We also demonstrate that communicating expertise through the linguistic properties of the explanation text was effective, where embracing language increased reliance and distancing language reduced reliance on AI."
245,2021,"Systemic Oversimplification Limits the Potential for Human-AI Partnership nan The modern world is evolving rapidly, especially with respect to the development and proliferation of increasingly intelligent, artificial intelligence (AI) and AI-related technologies. Nevertheless, in many ways, what this class of technologies has offered as return on investment remains less impressive than what has been promised. In the present paper, we argue that the continued failure to realize the potential in modern AI and AI-related technologies is largely attributable to the oversimplified, yet pervasive ways that our global society treats the relationship between these technologies and humans. Oversimplified concepts, once conveyed, tend to perpetuate myths that in turn limit the impact of such technologies in human society. To counter these oversimplifications, we offer a theoretical construct, which we call the landscape of human-AI partnership. This construct characterizes individual capability for real-world task performance as a dynamic function of information certainty, available time to respond, and task complexity. With this, our goal is to encourage more nuanced discourse about novel ways to solve challenges to modern and future sociotechnical societies, but without defaulting to notions that remain rooted in today's technologies-as-tools ways of thinking. The core of our argument is that society at large must recognize that intelligent technologies are evolving well beyond being mere tools for human use and are instead becoming capable of operating as interdependent teammates. This means that how we think about interactions between humans and AI needs to go beyond a Human-or-AI conversation about task assignments to more contextualized Human-and-AI way of thinking about how best to capitalize on the strengths hidden within emergent capabilities of unique human-AI partnerships that have yet to be fully realized."
246,2024,"Assurance of Human-AI Interaction Based Systems for Spaceflight: A Discussion of Critical Aspects to Increase Mutual Trust and Reliability nan The increasing interest in machine learning (ML) for various spaceflight applications is becoming a prevalent topic of discussion within the relevant parts of the aerospace community. Not only can successful applications ease burden on human operators, but increased on-board spacecraft autonomy holds the potential to decrease reliance on the already strained Deep Space Network (DSN) and other ground station networks. ML schemes driven by supervised learning, wherein an underlying neural network (NN) model learns a mapping between a perceived input and an appropriate action, have been assessed for spaceflight scenarios involving trajectory planning and control, multibody transfers, and real-time guidance. However, there is a notable gap in a majority of the literature regarding the safety and assurance of such solutions should they be applied in practical scenarios. Furthermore, with current interest shifting towards ML for spaceflight via implementations of cognitive AI and human-machine teaming (HMT) schemes, discussions regarding assurance need to be undertaken to develop mutual trust and allow the safe shift in control authority from human intelligence to automated, black-box intelligence. These discussions may be driven by various factors that are qualitative and quantitative, and they can involve insights from numerous aspects such as engineering performance metrics, human psychology, cognitive modeling, and human performance. In this work, we aim to provide a generalized overview and begin the discussion of these major factors. We also examine how they can be synergistically combined to develop a higher fidelity of assessment methodologies human-AI interactive (HAII) schemes for spaceflight applications"
247,2022,"Understanding User Reliance on AI in Assisted Decision-Making nan Proper calibration of human reliance on AI is fundamental to achieving complementary performance in AI-assisted human decision-making. Most previous works focused on assessing user reliance, and more broadly trust, retrospectively, through user perceptions and task-based measures. In this work, we explore the relationship between eye gaze and reliance under varying task difficulties and AI performance levels in a spatial reasoning task. Our results show a strong positive correlation between percent gaze duration on the AI suggestion and user AI task agreement, as well as user perceived reliance. Moreover, user agency is preserved particularly when the task is easy and when AI performance is low or inconsistent. Our results also reveal nuanced differences between reliance and trust. We discuss the potential of using eye gaze to gauge human reliance on AI in real-time, enabling adaptive AI assistance for optimal human-AI team performance."
248,2024,"AI-enhanced collective intelligence. nan Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field."
249,2021,"Multi-agent Naive Utility Calculus: Intent Recognition in the Stag-Hunt Game nan The human ability to utilize social and behavioral cues to infer each others intents, infer motivations, and predict future actions is a central process to human social life. This ability represents a facet of human cognition that artificial intelligence has yet to fully mimic and master. Artificial agents with greater social intelligence have wide-ranging applications from enabling the collaboration of human-AI teams to more accurately modelling human behavior in complex systems. Here, we show that the Naive Utility Calculus generative model is capable of competing with leading models in intent recognition and action prediction when observingstag-hunt, a simple multiplayer game where agents must infer each others intentions to maximize rewards. Moreover, we show the model is the first with the capacity to out-compete human observers in intent recognition after the first round of observation."
251,2023,Comparing Psychometric and Behavioral Predictors of Compliance During Human-AI Interactions [arXiv] nan Optimization of human-AI teams hinges on the AI's ability to tailor its interaction to individual human teammates. A common hypothesis in adaptive AI research is that minor differences in people's predisposition to trust can significantly impact their likelihood of complying with recommendations from the AI. Predisposition to trust is often measured with self-report inventories that are administered before interactions. We benchmark a popular measure of this kind against behavioral predictors of compliance. We find that the inventory is a less effective predictor of compliance than the behavioral measures in datasets taken from three previous research projects. This suggests a general property that individual differences in initial behavior are more predictive than differences in self-reported trust attitudes. This result also shows a potential for easily accessible behavioral measures to provide an AI with more accurate models without the use of (often costly) survey instruments.
252,2023,"On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations nan Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefits of interactive explanations is unclear. In this paper, we map existing findings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classification of interactive techniques specific to XAI and group the resulting categories according to their role in the cognitive process of explanation: selective, mutable or dialogic. We identify the effects of interactivity on several user-based metrics. We find that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conflicting results regarding cognitive load and overconfidence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes."
253,2021,"Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants nan Deep neural networks (DNNs) routinely achieve state-of-the-art performance in a wide range of tasks, but it can often be challenging for them to meet end-user needs in practice. This case study reports on the development of human-AI onboarding materials (i.e., training materials for users prior to using an AI) for a DNN-based medical AI Assistant to aid in the grading of prostate cancer. Specifically, we describe how the process of developing these materials changed the teams understanding of end-user requirements, contributing to modifications in the development and assessment of the underlying machine learning model. Importantly, we discovered that onboarding materials served as a useful boundary object for cross-functional teams, uncovering a new way to assess the ML model and specify its end-user requirements. We also present evidence of the utility of the onboarding materials by describing how it affected user strategies and decision-making with AI in a study deployment to pathologists."
254,2024,"Analysis of the role of conversation-generating AI and student experience in a human-AI collaborative learning environment nan The purpose of this study is to explore the role of generative artificial intelligence in collaborative learning environments between humans and AI, to examine the relationship between humans and artificial intelligence, and to analyze human experiences within this relationship. The goal is to provide implications for the construction of an appropriate support system for the use of generative AI in teaching and learning situations at higher education institutions in the future. To achieve this aim, the study involved 11 university students in team-based problem-solving activities that allowed the use of conversational generative AI. The study analyzed the results of pre-tests, team activity reports that included the content of the team activities, and post-tests submitted by these students. The findings are as follows: First, in collaborative learning, the role of conversational generative AI was identified as 'idea brainstorming,' 'structuring of thoughts,' 'effective integration of opinions among team members,' and 'drafting, revising, and reviewing the team's opinionated writings.' Second, the experiences of students using conversational generative AI in collaborative learning showed a complex and coexisting pattern of contrasting positions. Specifically, students' feelings towards generative AI were divided into 'positive feelings' and 'concerns,' and there were coexisting positive and negative opinions regarding its use for writing essays containing thoughts and arguments. However, all participants expressed positive opinions about its educational utility, and suggested focusing on enhancing user competency, defining the scope of its use as a supportive tool, and addressing technical flaws of generative AI itself as priorities for educational use. Based on these findings, discussions and implications were presented."
255,2023,"Defining human-AI teaming the human-centered way: a scoping review and network analysis. nan Introduction: With the advancement of technology and the increasing utilization of AI, the nature of human work is evolving, requiring individuals to collaborate not only with other humans but also with AI technologies to accomplish complex goals. This requires a shift in perspective from technology-driven questions to a human-centered research and design agenda putting people and evolving teams in the center of attention. A socio-technical approach is needed to view AI as more than just a technological tool, but as a team member, leading to the emergence of human-AI teaming (HAIT). In this new form of work, humans and AI synergistically combine their respective capabilities to accomplish shared goals.Methods: The aim of our work is to uncover current research streams on HAIT and derive a unified understanding of the construct through a bibliometric network analysis, a scoping review and synthetization of a definition from a socio-technical point of view. In addition, antecedents and outcomes examined in the literature are extracted to guide future research in this field.Results: Through network analysis, five clusters with different research focuses on HAIT were identified. These clusters revolve around (1) human and (2) task-dependent variables, (3) AI explainability, (4) AI-driven robotic systems, and (5) the effects of AI performance on human perception. Despite these diverse research focuses, the current body of literature is predominantly driven by a technology-centric and engineering perspective, with no consistent definition or terminology of HAIT emerging to date.Discussion: We propose a unifying definition combining a human-centered and team-oriented perspective as well as summarize what is still needed in future research regarding HAIT. Thus, this work contributes to support the idea of the Frontiers Research Topic of a theoretical and conceptual basis for human work with AI systems."
256,2024,"CPS-TaskForge: Generating Collaborative Problem Solving Environments for Diverse Communication Tasks nan Teams can outperform individuals; could adding AI teammates further bolster performance of teams solving problems collaboratively? Collaborative problem solving (CPS) research commonly studies teams with two agents (human-human or human-AI), but team research literature finds that, for complex tasks, larger teams are more effective. Progress in studying collaboration with more than two agents, through textual records of team interactions, is hindered by a major data challenge: available CPS corpora are predominantly dyadic, and adapting pre-existing CPS tasks to more agents is non-trivial. We address this data challenge by developing a CPS task generator, CPS-TaskForge, that can produce environments for studying CPS under a wide array of conditions, and releasing a CPS task design checklist grounded in the theoretical PISA 2015 CPS framework to help facilitate the development of CPS corpora with more agents. CPS-TaskForge takes the form of a resource management (tower defense) game, and different CPS tasks can be studied by manipulating game design parameters. We conduct a case study with groups of 3-4 humans to validate production of diverse natural language CPS communication in a game instance produced by CPS-TaskForge. We discuss opportunities for advancing research in CPS (both with human-only and human-AI teams) using different task configurations. We will release data and code."
257,2022,"Experimental evidence of effective human-Al collaboration in medical decision-making nan Artificial Intelligence (Al) systems are precious support for decision-making, with many applications also in the medical domain. The interaction between mos and Al enjoys a renewed interest following the increased possibilities of deep learning devices. However, we still have limited evidence-based knowledge of the context, design, and psychological mechanisms that craft an optimal human-Al collaboration. In this multicentric study, 21 endoscopists reviewed 504 videos of lesions prospectively acquired from real colonoscopies. They were asked to provide an optical diagnosis with and without the assistance of an Al support system. Endoscopists were influenced by Al (or = 3.05), but not erratically: they followed the Al advice more when it was correct (or = 3.48) than incorrect (or = 1.85). Endoscopists achieved this outcome through a weighted integration of their and the Al opinions, considering the case-by-case estimations of the two reliabilities. This Bayesian-like rational behavior allowed the human-Al hybrid team to outperform both agents taken alone. We discuss the features of the human-Al interaction that determined this favorable outcome."
258,2024,"Assessing Trust in Active Learning Systems: Insights from Query Policies and Uncertainty Visualization nan Active learning (AL) systems have become increasingly popular for various applications in machine learning (ML), including medical imaging, environmental monitoring, and geospatial analysis. These systems rely on inputs dynamically queried from people to enhance classification. Ensuring appropriate analyst trust in these systems presents a significant obstacle as analyst over- or underreliance may adversely affect a given application. Common AL strategies enhance classification models by asking analysts to provide labels for data points with the highest degree of uncertainty. However, such model-centric policies do not consider potential priming effects on the analyst and how they will affect people's trust in the system post-training. We present an empirical study assessing how AL query policies and visualizations that enhance transparency in a classifier's decisions influence trust in automated image classifiers. We found that query policy may significantly influence an analyst's perception of system capabilities, while the level of visual transparency into classifier certainty may influence an analyst's ability to perform a classification task. Our study informs the design of interactive labeling systems to help mitigate the effects of overreliance and calibrate appropriate trust in automated systems."
259,2022,"Storyboarding a Serious Game Environment for Evaluating the Impact of Empathetic AI for Sonar Operators nan Sonar operators work under high pressure and stressful conditions, handling large volumes of data that they must quickly perceive and interpret in order to make high-stakes decisions. Artificial Intelligence (AI) has the potential to aid them by processing large amounts of information and highlighting the critical data amongst the noise for the sonar operators to focus on. In order for such an AI to be effective, we need to build a cooperative human-AI team. Many studies show that such cooperation is built through trust [1] and that such trust can be built through a mutual, empathetic understanding of one another [2]. We argue, then, that for a human-AI team to be most effective, we need an empathetic AI to work alongside our sonar operators as this will foster a trusting team dynamic. Our aim is to build this trust and in turn, reduce the cognitive demand placed on sonar operators to a level that facilitates optimal performance in such high-stakes scenarios. In the context of this research, it is critical to acknowledge the principles of Responsible Research and Innovation (RRI). Factors that demand particular attention under this framework encompass potential unintended consequences of our empathetic AI, the phenomenon of over-trust in artificial intelligence (AI) systems, the possibility of AI making erroneous recommendations, the potential for misuse, and the development of strategies to mitigate these concerns. We propose to address these potential risks through a multifaceted approach. This includes the introduction of confidence reporting mechanisms, emphasizing the operator's role as the final arbiter of decision-making, referencing empirical findings on over-trust, and fostering transparency regarding the inherent limitations of AI."
260,2024,"Allowing humans to interactively guide machines where to look does not always improve a human-AI team's classification accuracy nan Via thousands of papers in Explainable AI (XAI), attention maps and feature attribution maps have been established as a common means for explaining the input features that are important to AI's decisions. It is an interesting but unexplored question whether allowing users to edit the importance scores of input features at test time would improve the human-AI team's accuracy on downstream tasks. In this paper, we address this question by taking CHM-Corr, a state-of-the-art, ante-hoc explanation method that first predicts patch-wise correspondences between the input and the training-set images, and then uses them to make classification decisions. We build an interactive interface on top of CHM-Corr, enabling users to directly edit the initial feature attribution map provided by CHM-Corr. Via our CHM-Corr++ interface, users gain insights into if, when, and how the model changes its outputs, enhancing understanding beyond static explanations. Our user study with 18 machine learning researchers who performed 1,400 decisions shows that our interactive approach does not improve user accuracy on CUB-200 bird image classification over static explanations. This challenges the belief that interactivity inherently boosts XAI effectiveness and raises needs for future research. Our work contributes to the field by open-sourcing an interactive tool for manipulating model attention, and it lays the groundwork for future research to enable effective human-AI interaction in computer vision. We release code and data on github. Our interface are available here."
261,2024,"Decision Theoretic Foundations for Experiments Evaluating Human Decisions [arXiv] nan Decision-making with information displays is a key focus of research in areas like explainable AI, human-AI teaming, and data visualization. However, what constitutes a decision problem, and what is required for an experiment to be capable of concluding that human decisions are flawed in some way, remain open to speculation. We present a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the normative decision. We evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making. We motivate the value of studying well-defined decision problems by describing a characterization of performance losses they allow us to conceive. In contrast, the ambiguities of a poorly communicated decision problem preclude normative interpretation. We conclude with recommendations for practice."
262,2019,"Implicit Communication of Actionable Information in Human-AI teams nan Humans expect their collaborators to look beyond the explicit interpretation of their words. Implicature is a common form of implicit communication that arises in natural language discourse when an utterance leverages context to imply information beyond what the words literally convey. Whereas computational methods have been proposed for interpreting and using different forms of implicature, its role in human and artificial agent collaboration has not yet been explored in a concrete domain. The results of this paper provide insights to how artificial agents should be structured to facilitate natural and efficient communication of actionable information with humans. We investigated implicature by implementing two strategies for playing Hanabi, a cooperative card game that relies heavily on communication of actionable implicit information to achieve a shared goal. In a user study with 904 completed games and 246 completed surveys, human players randomly paired with an implicature AI are 71% more likely to think their partner is human than players paired with a non-implicature AI. These teams demonstrated game performance similar to other state of the art approaches."
263,2023,"Human-Centered Deferred Inference: Measuring User Interactions and Setting Deferral Criteria for Human-AI Teams nan Although deep learning holds the promise of novel and impactful interfaces, realizing such promise in practice remains a challenge: since dataset-driven deep-learned models assume a one-time human input, there is no recourse when they do not understand the input provided by the user. Works that address this via deferred inference-soliciting additional human input when uncertain-show meaningful improvement, but ignore key aspects of how users and models interact. In this work, we focus on the role of users in deferred inference and argue that the deferral criteria should be a function of the user and model as a team, not simply the model itself. In support of this, we introduce a novel mathematical formulation, validate it via an experiment analyzing the interactions of 25 individuals with a deep learning-based visiolinguistic model, and identify user-specific dependencies that are under-explored in prior work. We conclude by demonstrating two human-centered procedures for setting deferral criteria that are simple to implement, applicable to a wide variety of tasks, and perform equal to or better than equivalent procedures that use much larger datasets."
264,2023,"Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming nan Qualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools."
265,2017,"It Takes Two to Tango: Towards Theory of AI's Mind [arXiv] nan Theory of Mind is the ability to attribute mental states (beliefs, intents, knowledge, perspectives, etc.) to others and recognize that these mental states may differfrom ones own. Theory of Mind is critical to effective communication and to teams demonstrating higher collective performance. To effectively leverage the progress in Artificial Intelligence (AI) to make our lives more productive, it is important for humans and AI to work well together in a team. Traditionally, there has been much emphasis on research to make AI more accurate, and (to a lesser extent) on having it better understand human intentions, tendencies, beliefs, and contexts. The latter involves making AI more human-like and having it develop a theory of our minds. In this work, we argue that for human-AI teams to be effective, humans must also develop a theory of AIs mind - get to know its strengths, weaknesses, beliefs, and quirks. We instantiate these ideas within the domain of Visual Question Answering (VQA). We find that using just a few examples (50), lay people can be trained to better predict responses and oncoming failures of a complex VQA model. Surprisingly, we find that having access to the models internal states - its confidence in its top-k predictions, explicit or implicit attention maps which highlight regions in the image (and words in the question) the model is looking at (and listening to) while answering a question about an image - do not help people better predict its behavior."
266,2024,"Human Factors Considerations in Artificial Intelligence Applications for Nuclear Power Plants nan In recent years, there has been a wave of artificial intelligence (AI) technologies that offer to solve problems from shopping habits to mortgage approvals to critical systems operations. The rapidity of the development of these systems has led to both excitement and apprehension about the roles these systems should play in our modern societies. This paper focuses on the critical infrastructure industry, in general, and nuclear power generation, in particular, and seeks to scrutinize how we can leverage these novel technologies in human-centered ways to maintain or enhance the established high levels of reliability and resilience in these industries. First, we discuss the broader aspects of cognitive systems and activities that are critical to understanding the human-AI space. Then we explore different approaches to explainability in AI and the notions of trust. We then move on to discuss several human factors concepts and methods and how they can support the design of human-AI teams. We then explore recent research related to nuclear power that has been undertaken and evaluate the current industry and regulatory landscapes. Finally, we discuss identified research gaps and recommendations for solving these for the critical infrastructure space."
267,2022,"Measuring and Predicting Human Trust in Recommendations from an AI Teammate nan Predicting compliance with AI recommendations and knowing when to intervene are critical facets of human-AI teaming. AIs are typically deployed in settings where their abilities to evaluate decision variables far exceed the abilities of their human counterparts. However, even though AIs excel at weighing multiple issues and computing near optimal solutions with speed and accuracy beyond that of any human, they still make mistakes. Thus, perfect compliance may be undesirable. This means, just as individuals must know when to follow the advice of other people, it is critical for them to know when to adopt the recommendations from their AI. Well-calibrated trust is thought to be a fundamental aspect of this type of knowledge. We compare the ability of a common trust inventory and the ability of a behavioral measure of trust to predict compliance and success in a reconnaissance mission. We interpret the experimental results to suggest that the behavioral measure is a better predictor of overall mission compliance and success. We discuss how this measure could possibly be used in compliance interventions and related open questions."
268,2022,"Designing Hybrid Intelligence Techniques for Facilitating Collaboration Informed by Social Science nan Designing (socially) intelligent systems for facilitating collaborations in human-human and human-AI teams will require them to have a basic understanding of principles underlying social decisionmaking. Partner selection - the ability to identify and select suitable partners for collaborative relationships - is one relevant component of social intelligence and an important ingredient for successful relationship management. In everyday life, decision to engage in joint undertakings are often based on impressions made during social interactions with potential partners. These impressions, and consequently, partner selection are informed by (non)-verbal behavioral cues. Despite its importance, research investigating how these impressions and partner selection decisions unfold in naturalistic settings seem to be lacking. Thus, in this paper, we present a project focused on understanding, predicting and modeling partner selection and understanding its relationship with human impressions in semi- naturalistic settings, such as social interactions, with the aim of informing future designing approaches of (hybrid) intelligence system that can understand, predict and aid in initiating and facilitating (current and future) collaborations."
269,2024,"Exploring Functionalities for an Intelligent Pilot Advisory System in Normal Operation nan This paper delves into the functionalities of an Intelligent Pilot Advisory System (IPAS) in normal aviation operations. Building upon the foundational work of Intelligent Pilot Advisory System: The Journey From Ideation to an Early System Design of an AI-Based Decision Support System for Airline Flight Decks by Jakob Wurfel et al., which primarily focused on emergency scenarios, this study extends the IPAS's application to non-emergency contexts. Utilizing a user-centered approach, a workshop involving pilots, data scientists, and Human-Artificial Intelligence Teaming (HAT) experts was conducted to brainstorm and evaluate functionalities for this system in regular flight operations. The methodology combined creative and analytical techniques, including the 6-3-5 ideation method, mind mapping and design studio method, leading to rapid prototyping and iterative feedback. During the workshop, several key functionalities for the IPAS were identified, such as the Mission Monitoring and Advisory Function (MMAF), which provides real-time updates on flight-related factors, as well as the integration of pre-flight briefing and operational guidance. Based on the workshops results an early prototype was developed, showcasing a timeline-based presentation of information and interactive user interface elements. This prototype serves as the basis for initial feedback evaluation and ongoing refinement. By integrating AI and leveraging the amount of aviation data, this intelligent advisor aims to improve situational awareness, decision-making, and operational efficiency in normal flight operations. In this context, this paper highlights the need for extended pilot testing and integration with existing cockpit systems, emphasizing the importance of human-AI teaming aspects, customization, data security, and the system's impact on pilot skills, training and the environment."
270,2024,"HADT: Human-AI Diagnostic Team via Hierarchical Reinforcement Learning nan Medical online consultation is important to healthcare worldwide, with hundreds of millions of participants each year. However, expert-level online consultations are expensive due to the shortage of medical professionals, while AI models are unreliable because they have unpredictable risks. Therefore, we introduce human-machine collaboration to medical online consultation and focus on symptom inquiry, as the basis for disease diagnosis. There are two key issues: 1) how to design an intelligent assignment strategy that can determine whether doctors or models participate in each turn? 2) how to design an effective execution strategy that can improve the machine's inquiry ability among considerable symptoms? To address the above issues, we propose the Human-AI Diagnostic Team (HADT) framework based on Hierarchical Reinforcement Learning (HRL), which aims to achieve high accuracy with low manpower. Specifically, HADT has two layers. The upper one is responsible for assignment, in which we propose a module called master that enables intelligent human-machine assignments through the masked RL with reward shaping. The lower one is responsible for execution, consisting of a doctor and a proposed module called machine. This module can effectively ask about symptoms through the masked HRL with bottom-up training. Experiments on the public datasets show that HADT can achieve up to 89.4% accuracy with only 10.9% human effort, as confirmed by real clinical doctors using our online interface."
271,2023,"Development of Mental Models in Decision-Making Tasks nan This study aims to understand the development of users' mental models (MMs) over time. We use behavioral data obtained from process tracing to identify key components of MMs and their relative importance. Further, we investigate the stability and predictability of these components as users learn through system interaction. Human-in-the-loop experimentation was deployed in a dynamic geospatial environment and six information attributes were provided to inform participants' decisions. Partial Least Squares Regression was used to relate behavioral data and decision-making outcomes. We found that top-most performers initially adapt and progressively stabilize toward a suitable model as performance improves. In contrast, low performers lack adaptability and perform poorly. Overall, most participants are consistent with their choices as task familiarity increases. Identifying MMs and the underlying stability and predictability trends within performance groups has implications for improving user experience and curating decision support tools for human-AI teams."
272,2024,"Moving beyond the AI sales pitch - Empowering clinicians to ask the right questions about clinical AI. nan We challenge the dominant technology-centric narrative around clinical AI. To realise the true potential of the technology, clinicians must be empowered to take a whole-system perspective and assess the suitability of AI-supported tasks for their specific complex clinical setting. Key factors include the AI's capacity to augment human capabilities, evidence of clinical safety beyond general performance metrics and equitable clinical decision-making by the human-AI team. Proactively addressing these issues could pave the way for an accountable clinical buy-in and a trustworthy deployment of the technology."
273,2024,"Design and evaluation of AI copilots -- case studies of retail copilot templates nan Building a successful AI copilot requires a systematic approach. This paper is divided into two sections, covering the design and evaluation of a copilot respectively. A case study of developing copilot templates for the retail domain by Microsoft is used to illustrate the role and importance of each aspect. The first section explores the key technical components of a copilot's architecture, including the LLM, plugins for knowledge retrieval and actions, orchestration, system prompts, and responsible AI guardrails. The second section discusses testing and evaluation as a principled way to promote desired outcomes and manage unintended consequences when using AI in a business context. We discuss how to measure and improve its quality and safety, through the lens of an end-to-end human-AI decision loop framework. By providing insights into the anatomy of a copilot and the critical aspects of testing and evaluation, this paper provides concrete evidence of how good design and evaluation practices are essential for building effective, human-centered AI assistants."
274,2024,"Optimizing Delegation in Collaborative Human-AI Hybrid Teams [arXiv] nan When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention. Therefore our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention. We demonstrate our manager performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system. We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control. Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to ~187% that of the best solo agent performance."
275,2024,Leveraging Manufacturing Human-AI Team Interaction for Cyber-Physical-Social System Construction nan nan
276,2022,"Improving Human Situation Awareness in AI-advised Decision Making nan Human-autonomy teams aim to leverage the different strengths of humans and autonomous systems respectively to exceed the individual capabilities of each through collaboration. Highly effective human teams develop and utilize a shared mental model (SMM): a synchronized understanding of the external world and of the tasks, responsibilities, capabilities, and limits of each team member. Recent works assert that the same should apply to human-autonomy teams; however, contemporary AI commonly consists of black box systems, whose internal processes can not easily be viewed or interpreted. Users can easily develop inaccurate mental models of such systems, impeding SMM development and thus team performance.We seek ways to support the human's side of Human-AI SMMs in the context of AI-advised Decision Making, a form of teaming in which an AI suggests a solution to a human operator, who is responsible for the final decision. This work focuses on improving shared situation awareness by providing more context to the AI's internal processing. We hypothesize that this will lead the human to a more accurate mental model of the task and the AI, which in turn will improve team performance. We manipulate the human's situation awareness of the task environment and measure effects on the shared mental model. A between-subjects, randomized experiment is conducted in which participants in 6 treatment groups of varying amounts of contextual information (as a proxy for situation awareness) complete a task with an AI teammate. We find that improving shared situation awareness of decision points improves the human's overall performance, as well as their understanding of their AI teammate, without directly explaining the AI's internal mechanisms. Additionally, we find that increasing the human's situation awareness of task environment and AI teammate reduces over-reliance on the automated teammate."
277,2018,"Algorithms for the Greater Good! On Mental Modeling and Acceptable Symbiosis in Human-AI Collaboration [arXiv] nan Effective collaboration between humans and AI-based systems requires effective modeling of the human in the loop, both in terms of the mental state as well as the physical capabilities of the latter. However, these models can also open up pathways for manipulating and exploiting the human in the hopes of achieving some greater good, especially when the intent or values of the AI and the human are not aligned or when they have an asymmetrical relationship with respect to knowledge or computation power. In fact, such behavior does not necessarily require any malicious intent but can rather be borne out of cooperative scenarios. It is also beyond simple misinterpretation of intents, as in the case of value alignment problems, and thus can be effectively engineered if desired. Such techniques already exist and pose several unresolved ethical and moral questions with regards to the design of autonomy. In this paper, we illustrate some of these issues in a teaming scenario and investigate how they are perceived by participants in a thought experiment."
278,2023,"Artificial Intelligence Impersonating a Human: The Impact of Design Facilitator Identity on Human Designers nan Advances in artificial intelligence (AI) offer new opportunities for human-AI cooperation in engineering design. Human trust in AI is a crucial factor in ensuring an effective human-AI cooperation, and several approaches to enhance human trust in AI have been explored in prior studies. However, it remains an open question in engineering design whether human designers have more trust in an AI and achieve better joint performance when they are deceived into thinking they are working with another human designer. This research assesses the impact of design facilitator identity (human versus AI) on human designers through a human subjects study, where participants work with the same AI design facilitator and they can adopt their AI facilitator's design anytime during the study. Half of the participants are told that they work with an AI, and the other half of the participants are told that they work with another human participant but in fact they work with the AI design facilitator. The results demonstrate that, for this study, human designers adopt their facilitator's design less often on average when they are deceived about the identity of the AI design facilitator as another human designer. However, design facilitator identity does not have a significant impact on human designers' average performance, perceived workload, and perceived competency and helpfulness of their design facilitator in the study. These results caution against deceiving human designers about the identity of an AI design facilitator in engineering design."
279,2024,"Differences in student-AI interaction process on a drawing task: Focusing on students' attitude towards AI and the level of drawing skills nan Recent advances and applications of artificial intelligence (AI) have increased the opportunities for students to interact with AI in their learning tasks. Although various fields of scholarly research have investigated human-AI collaboration, the underlying processes of how students collaborate with AI in a student-AI teaming scenario have been scarcely investigated. To develop effective AI applications in education, it is necessary to understand differences in the student-AI interaction (SAI) process depending on students' characteristics. The present study attempts to fill this gap by exploring the differences in the SAI process amongst students with varying drawing proficiencies and attitudes towards AI in performing a public advertisement drawing task. Based on the empirical evidence obtained from the think-aloud protocols of 20 Korean undergraduate students, the study first conducted a lag sequential analysis to identify statistically significant linear patterns of each group and then chronologically incorporated them into the SAI duration via coded activity alignment series to distinguish the overall SAI process of each group. The study revealed the distinctive differences in SAI processes of students with different attitudes towards AI and drawing skills. To better facilitate student-AI teams for learning, a range of implications of educational AI development and instructional design is discussed."
280,2023,"What Lies Beneath? Exploring the Impact of Underlying AI Model Updates in AI-Infused Systems [arXiv] nan As AI models evolve, understanding the influence of underlying models on user experience and performance in AI-infused systems becomes critical, particularly while transitioning between different model versions. We studied the influence of model change by conducting two complementary studies in the context of AI-based facial recognition for historical person identification tasks. First, we ran an online experiment where crowd workers interacted with two different facial recognition models: an older version and a recently updated, developer-certified more accurate model. Second, we studied a real-world deployment of these models on a popular historical photo platform through a diary study with 10 users. Our findings sheds light on models affecting human-AI team performance, users' abilities to differentiate between different models, the folk theories they develop, and how these theories influence their preferences. Drawing from these insights, we discuss design implications for updating models in AI-infused systems."
281,2023,"Fostering Collective Intelligence in Human-AI Collaboration: Laying the Groundwork for COHUMAIN. nan Artificial Intelligence (AI) powered machines are increasingly mediating our work and many of our managerial, economic, and cultural interactions. While technology enhances individual capability in many ways, how do we know that the sociotechnical system as a whole, consisting of a complex web of hundreds of human-machine interactions, is exhibiting collective intelligence? Research on human-machine interactions has been conducted within different disciplinary silos, resulting in social science models that underestimate technology and vice versa. Bringing together these different perspectives and methods at this juncture is critical. To truly advance our understanding of this important and quickly evolving area, we need vehicles to help research connect across disciplinary boundaries. This paper advocates for establishing an interdisciplinary research domain-Collective Human-Machine Intelligence (COHUMAIN). It outlines a research agenda for a holistic approach to designing and developing the dynamics of sociotechnical systems. In illustrating the kind of approach, we envision in this domain, we describe recent work on a sociocognitive architecture, the transactive systems model of collective intelligence, that articulates the critical processes underlying the emergence and maintenance of collective intelligence and extend it to human-AI systems. We connect this with synergistic work on a compatible cognitive architecture, instance-based learning theory and apply it to the design of AI agents that collaborate with humans. We present this work as a call to researchers working on related questions to not only engage with our proposal but also develop their own sociocognitive architectures and unlock the real potential of human-machine intelligence."
282,2024,"Designing explainable AI to improve human-AI team performance: A medical stakeholder-driven scoping review nan The rise of complex AI systems in healthcare and other sectors has led to a growing area of research called Explainable AI (XAI) designed to increase transparency. In this area, quantitative and qualitative studies focus on improving user trust and task performance by providing system- and prediction-level XAI features. We analyze stakeholder engagement events (interviews and workshops) on the use of AI for kidney transplantation. From this we identify themes which we use to frame a scoping literature review on current XAI features. The stakeholder engagement process lasted over nine months covering three stakeholder group's workflows, determining where AI could intervene and assessing a mock XAI decision support system. Based on the stakeholder engagement, we identify four major themes relevant to designing XAI systems - 1) use of AI predictions, 2) information included in AI predictions, 3) personalization of AI predictions for individual differences, and 4) customizing AI predictions for specific cases. Using these themes, our scoping literature review finds that providing AI predictions before, during, or after decision-making could be beneficial depending on the complexity of the stakeholder's task. Additionally, expert stakeholders like surgeons prefer minimal to no XAI features, AI prediction, and uncertainty estimates for easy use cases. However, almost all stakeholders prefer to have optional XAI features to review when needed, especially in hard-to-predict cases. The literature also suggests that providing both systemand prediction-level information is necessary to build the user's mental model of the system appropriately. Although XAI features improve users' trust in the system, human-AI team performance is not always enhanced. Overall, stakeholders prefer to have agency over the XAI interface to control the level of information based on their needs and task complexity. We conclude with suggestions for future research, especially on customizing XAI features based on preferences and tasks."
283,2023,"Human Performance in Competitive and Collaborative Human-Machine Teams. nan In the modern world, many important tasks have become too complex for a single unaided individual to manage. Teams conduct some safety-critical tasks to improve task performance and minimize the risk of error. These teams have traditionally consisted of human operators, yet, nowadays, artificial intelligence and machine systems are incorporated into team environments to improve performance and capacity. We used a computerized task modeled after a classic arcade game to investigate the performance of human-machine and human-human teams. We manipulated the group conditions between team members; sometimes, they were instructed to collaborate, compete, or work separately. We evaluated players' performance in the main task (gameplay) and, in post hoc analyses, participant behavioral patterns to inform group strategies. We compared game performance between team types (human-human vs. human-machine) and group conditions (competitive, collaborative, independent). Adapting workload capacity analysis to human-machine teams, we found performance under both team types and all group conditions suffered a performance efficiency cost. However, we observed a reduced cost in collaborative over competitive teams within human-human pairings, but this effect was diminished when playing with a machine partner. The implications of workload capacity analysis as a powerful tool for human-machine team performance measurement arediscussed."
284,2024,"Trust and trustworthy artificial intelligence: A research agenda for AI in the environmental sciences nan Demands to manage the risks of artificial intelligence (AI) are growing. These demands and the government standards arising from them both call for trustworthy AI. In response, we adopt a convergent approach to review, evaluate, and synthesize research on the trust and trustworthiness of AI in the environmental sciences and propose a research agenda. Evidential and conceptual histories of research on trust and trustworthiness reveal persisting ambiguities and measurement shortcomings related to inconsistent attention to the contextual and social dependencies and dynamics of trust. Potentially underappreciated in the development of trustworthy AI for environmental sciences is the importance of engaging AI users and other stakeholders, which human-AI teaming perspectives on AI development similarly underscore. Co-development strategies may also help reconcile efforts to develop performance-based trustworthiness standards with dynamic and contextual notions of trust. We illustrate the importance of these themes with applied examples and show how insights from research on trust and the communication of risk and uncertainty can help advance the understanding of trust and trustworthiness of AI in the environmental sciences."
285,2024,"Human Designers' Dynamic Confidence and Decision-Making When Working With More Than One Artificial Intelligence nan As artificial intelligence (AI) systems become increasingly capable of performing design tasks, they are expected to be deployed to assist human designers' decision-making in a greater variety of ways. For complex design problems such as those with multiple objectives, one AI may not always perform its expected accuracy due to the complexity of decision-making, and therefore, multiple AIs may be implemented to provide design suggestions. For such assistance to be productive, human designers must develop appropriate confidence in each AI and in themselves and accept or reject AI inputs accordingly. This work conducts a human subjects experiment to examine the development of a human designer's confidence in each AI and self-confidence throughout decision-making assisted by two AIs and how these confidences influence the decision to accept AI inputs. Major findings demonstrate severe decreases in a human designer's confidence especially when working with one or more low-performing AI teammates and/or receiving negative feedback. Additionally, a human designer's decision to accept AI suggestions depends on their self-confidence and confidence in one of the two AIs. Finally, an additional AI does not increase a human designer's likelihood of conforming to AI suggestions. Therefore, in comparison to a scenario with one AI, the results in this work caution against the implementation of an additional AI to AI-assisted decision-making scenarios. The insights also inform the design and management of human-AI teams to improve the outcome of AI-assisted decision-making."
286,2023,"Harmony in intelligent hybrid teams: the influence of the intellectual ability of artificial intelligence on human members' reactions nan Purpose The purpose of this paper was to investigate the relationships among the intellectual ability of artificial intelligence (AI), cognitive emotional processes and the positive and negative reactions of human members. The authors also examined the moderating role of AI status in teams. Design/methodology/approach The authors designed an experiment and recruited 120 subjects who were randomly distributed into one of three groups classified by the upper, middle and lower organization levels of AI in the team. The findings in this study were derived from subjects' self-reports and their performance in the experiment. Findings Regardless of the position held by AI, human members believed that its intelligence level is positively correlated with dependence behavior. However, when the AI and human members are at the same level, the higher the intelligence of AI, the more likely it is that its direct interaction with team members will lead to conflicts. Research limitations/implications This paper only focuses on human-AI harmony in transactional work in hybrid teams in enterprises. As AI applications permeate, it should be considered whether the findings can be extended to a broader range of AI usage scenarios. Practical implications These results are helpful for understanding how to improve team performance in light of the fact that team members have introduced AI into their enterprises in large quantities. Originality/value This study contributes to the literature on how the intelligence level of AI affects the positive and negative behaviors of human members in hybrid teams. The study also innovatively introduces status into hybrid organizations."
287,2023,Converging Human Intelligence with AI Systems to Advance Flood Evacuation Decision Making nan nan
288,2023,"Explanations Can Reduce Overreliance on AI Systems During Decision-Making nan Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction."
289,2023,"Developing Team Design Patterns for Hybrid Intelligence Systems nan With artificial intelligence (AI) systems entering our working and leisure environments with increasing adaptation and learning capabilities, new opportunities arise for developing hybrid (human-AI) intelligence (HI) systems, comprising new ways of collaboration. However, there is not yet a structured way of specifying design solutions of collaboration for hybrid intelligence (HI) systems and there is a lack of best practices shared across application domains. We address this gap by investigating the generalization of specific design solutions into design patterns that can be shared and applied in different contexts. We present a human-centered bottom-up approach for the specification of design solutions and their abstraction into team design patterns. We apply the proposed approach for 4 concrete HI use cases and show the successful extraction of team design patterns that are generalizable, providing re-usable design components across various domains. This work advances previous research on team design patterns and designing applications of HI systems."
290,2023,"Grounded Copilot: How Programmers Interact with Code-Generating Models nan Powered by recent advances in code-generating models, AI assistants like Github Copilot promise to change the face of programming forever. But whatisthis new face of programming? We present the first grounded theory analysis of how programmers interact with Copilot, based on observing 20 participants-with a range of prior experience using the assistant-as they solve diverse programming tasks across four languages. Our main finding is that interactions with programming assistants arebimodal: inacceleration mode, the programmer knows what to do next and uses Copilot to get there faster; inexploration mode, the programmer is unsure how to proceed and uses Copilot to explore their options. Based on our theory, we provide recommendations for improving the usability of future AI programming assistants."
291,2024,"Outlining the Design Space of eXplainable Swarm (xSwarm): Experts' Perspective nan In swarm robotics, agents interact through local roles to solve complex tasks beyond an individual's ability. Even though swarms are capable of carrying out some operations without the need for human intervention, many safety-critical applications still call for human operators to control and monitor the swarm. There are novel challenges to effective Human-Swarm Interaction (HSI) that are only beginning to be addressed. Explainability is one factor that can facilitate effective and trustworthy HSI and improves the overall performance of Human-Swarm team. Explainability was studied across various Human-AI domains, such as Human-Robot Interaction and Human-Centered ML. However, it is still ambiguous whether explanations studied in Human-AI literature would be beneficial in Human-Swarm research and development. Furthermore, the literature lacks foundational research on the prerequisites for explainability requirements in swarm robotics, i.e., what kind of questions an explainable swarm is expected to answer, and what types of explanations a swarm is expected to generate. By surveying 26 swarm experts, we seek to answer these questions and identify challenges experts faced to generate explanations in Human-Swarm environments. Our work contributes insights into defining a new area of research of eXplainable Swarm (xSwarm) which looks at how explainability can be implemented and developed in swarm systems. This paper opens discussion on xSwarm and paves the way for more research in the field."
292,2023,Towards Scalable Structured Data from Clinical Text nan nan
293,2024,"Enhancing Collaborative Design Through Process Feedback with Motivational Interviewing: Can AI Play a Role? nan Collaborative design is a key element in Product/System development. However, delivering true collaboration in multidisciplinary teams is challenging. Feedback systems are one of the solutions to improve collaboration; although teams normally receive feedback on outcomes, the collaboration process itself is neglected. During a PBL course, 40 engineers from 22 disciplines and 12 countries were distributed in six teams. In addition to receiving outcome feedback, we used Motivational Interviewing (MI) techniques to provide process feedback for half of the design teams whereas the other half only received outcome feedback. At the same time, we employed a pre-trained Machine Learning (ML) technique to compare the teams' progress through teams' communication and sentiment analysis. Our results show that; (i) adding process feedback in the early stages of the design process enhances the collaborative design. (ii) ML algorithms can predict the progress. We suggest further research using Natural Language Processing (NLP) and supervised ML techniques for designing a new AI team-mate and mentoring assistant, as well as fostering Human-AI interaction styles via MI methods."
294,2023,"Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems nan The dazzling promises of AI systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. Recent research has shown that appropriate reliance is the key to achieving complementary team performance in AI-assisted decision making. This paper addresses an under-explored problem of whether the Dunning-Kruger Efect (DKE) among people can hinder their appropriate reliance on AI systems. DKE is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. Through an empirical study (N = 249), we explored the impact of DKE on human reliance on an AI system, and whether such efects can be mitigated using a tutorial intervention that reveals the fallibility of AI advice, and exploiting logic units-based explanations to improve user understanding of AI advice. We found that participants who overestimate their performance tend to exhibit under-reliance on AI systems, which hinders optimal team performance. Logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. While the tutorial intervention was highly efective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. Our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on AI systems. Our fndings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-AI decision making. This lays out promising future directions for relevant HCI research in this community."
295,2022,"Enhancing Human-AI (H-AI) Collaboration On Design Tasks Using An Interactive Text/Voice Artificial Intelligence (AI) Agent nan In this presentation, we demonstrate a way to develop a class of AI systems, the Disruptive Interjector (DI), which observe what a human is doing, then interject with suggestions that aid in idea generation or problem solving in a human-AI (H-AI) team; something that goes beyond current creativity support systems by replacing a human-human (H-H) team with a H-AI one. The proposed DI is distinct from tutors, chatbots, recommenders and other similar systems since they seek to diverge from a solution (rather than converge towards one) by encouraging consideration of other possibilities. We develop a conceptual design of the system, then present examples from deep Convolution Neural Networks[1, 7] learning models. The first example shows results from a model that was trained on an open-source dataset (publicly available online) of a community technical support chat transcripts, while the second one was trained on a design-focused dataset obtained from transcripts of experts engaged in engineering design problem solving (unavailable publicly). Based on the results from these models, we propose the necessary improvements on models and training datasets that must be resolved in order to achieve usable and reliable collaborative text/voice systems that fall in this class of AI systems."
296,2021,"Balancing Performance and Human Autonomy With Implicit Guidance Agent. nan The human-agent team, which is a problem in which humans and autonomous agents collaborate to achieve one task, is typical in human-AI collaboration. For effective collaboration, humans want to have an effective plan, but in realistic situations, they might have difficulty calculating the best plan due to cognitive limitations. In this case, guidance from an agent that has many computational resources may be useful. However, if an agent guides the human behavior explicitly, the human may feel that they have lost autonomy and are being controlled by the agent. We therefore investigated implicit guidance offered by means of an agent's behavior. With this type of guidance, the agent acts in a way that makes it easy for the human to find an effective plan for a collaborative task, and the human can then improve the plan. Since the human improves their plan voluntarily, he or she maintains autonomy. We modeled a collaborative agent with implicit guidance by integrating the Bayesian Theory of Mind into existing collaborative-planning algorithms and demonstrated through a behavioral experiment that implicit guidance is effective for enabling humans to maintain a balance between improving their plans and retaining autonomy."
297,2024,"Hiring an AI: Incorporating Personnel Selection Methods in User-Centered Design to Design AI Agents for Safety-Critical Domains nan Enhancing human teams with AI is currently aspired across application domains. However, the prospective team role of high-performing AI-agents is rarely considered. In this paper, we propose a new method complementing existing user-centered design processes to define an AI-agent personality that suits the team. Inspired by personnel selection methods, the hiring an AI workshop prompts teams from safety-critical domains to 1) remember past incidents, 2) envision the personality and competence of their desired teammate, and 3) validate how the teammate would act in described incidents. Clustering and modeling the data made it accessible to interdisciplinary teams. Incorporating the models into the ideation process and revisiting the data for a personality drill-down exceeded insights from contextual inquiries and had a measurable impact (i.e., downstream utility) on the visions. Participants' humanizing descriptions of a knowing and competent, yet modest teammate raise challenges for technical implementation and questions on future team-collaboration in safety-critical domains."
298,2024,"Exploring a Behavioral Model of Positive Friction in Human-AI Interaction nan Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences. However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries. More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings. This paper first proposes a 'positive friction' model that can help characterize how friction is currently beneficial in user and developer experiences with AI, diagnose the potential need for friction where it may not yet exist in these contexts, and inform how positive friction can be used to generate solutions, especially as advances in AI continue to be progress and new opportunities emerge. It then explores this model in the context of AI users and developers by proposing the value of taking a hybrid 'AI+human' lens, and concludes by suggesting questions for further exploration."
299,2025,"Exploring the problems, their causes and solutions of AI pair programming: A study on GitHub and Stack Overflow nan With the recent advancement of Artificial Intelligence (AI) and Large Language Models (LLMs), AI-based code generation tools become a practical solution for software development. GitHub Copilot, the AI pair programmer, utilizes machine learning models trained on a large corpus of code snippets to generate code suggestions using natural language processing. Despite its popularity in software development, there is limited empirical evidence on the actual experiences of practitioners who work with Copilot. To this end, we conducted an empirical study to understand the problems that practitioners face when using Copilot, as well as their underlying causes and potential solutions. We collected data from 473 GitHub issues, 706 GitHub discussions, and 142 Stack Overflow posts. Our results reveal that (1) Operation Issue and Compatibility Issue are the most common problems faced by Copilot users, (2) Copilot Internal Error, Network Connection Error, and Editor/IDE Compatibility Issue are identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify Configuration/Setting, and Use Suitable Version are the predominant solutions. Based on the results, we discuss the potential areas of Copilot for enhancement, and provide the implications for the Copilot users, the Copilot team, and researchers."
300,2024,"AI Agents Learning Human Decision Policies for Collaborative Situation Assessment in NORAD C2 Operations nan The modernization of Command and Control (C2) for North American Aerospace Defense (NORAD) entails supporting operators with trustworthy AI-based solutions that complement, rather than replace, human abilities. New forms of threats requires more than ever the ability to quickly derive actionable situational awareness from a set of heterogeneous sensors. In this paper, we investigate the use of Human-Automation Teaming (HAT) for achieving accurate, timely continental surveillance in the context of NORAD critical infrastructure protection. We developed a collaborative AI agent solution with awareness, anticipation and decision capabilities, augmented here with the ability to learn human decision policies for improved collaborative situation assessment. The study employs a simulated threat evaluation task to validate the effectiveness of the augmented AI-agent using a multi-model approach combining seven supervised machine learning algorithms. Results show that the policy capturing method classified threat levels with a predictive accuracy of 95% while considering three different types of targets (UAV, drone swarm, small aircraft). We conclude that integrating the policy capturing capability into a collaborative AI-agent constitutes a key step toward enabling a novel human-AI co-learning process for adjustable human-autonomy teaming."
301,2024,"Visualizing and Comparing Machine Learning Predictions to Improve Human-AI Teaming on the Example of Cell Lineage nan We visualize the predictions of multiple machine learning models to help biologists as they interactively make decisions about cell lineage-the development of a (plant) embryo from a single ovum cell. Based on a confocal microscopy dataset, traditionally biologists manually constructed the cell lineage, starting from this observation and reasoning backward in time to establish their inheritance. To speed up this tedious process, we make use of machine learning (ML) models trained on a database of manually established cell lineages to assist the biologist in cell assignment. Most biologists, however, are not familiar with ML, nor is it clear to them which model best predicts the embryo's development. We thus have developed a visualization system that is designed to support biologists in exploring and comparing ML models, checking the model predictions, detecting possible ML model mistakes, and deciding on the most likely embryo development. To evaluate our proposed system, we deployed our interface with six biologists in an observational study. Our results show that the visual representations of machine learning are easily understandable, and our tool, LineageD+, could potentially increase biologists' working efficiency and enhance the understanding of embryos."
302,2023,"From Copilot to Pilot: Towards AI Supported Software Development [arXiv] nan AI-supported programming has arrived, as shown by the introduction and successes of large language models for code, such as Copilot/Codex (Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on programming challenges is now possible. However, software engineering is much more than solving programming contests. Moving beyond code completion to AI-supported software engineering will require an AI system that can, among other things, understand how to avoid code smells, to follow language idioms, and eventually (maybe!) propose rational software designs. In this study, we explore the current limitations of AI-supported code completion tools like Copilot and offer a simple taxonomy for understanding the classification of AI-supported code completion tools in this space. We first perform an exploratory study on Copilot's code suggestions for language idioms and code smells. Copilot does not follow language idioms and avoid code smells in most of our test scenarios. We then conduct additional investigation to determine the current boundaries of AI-supported code completion tools like Copilot by introducing a taxonomy of software abstraction hierarchies where 'basic programming functionality' such as code compilation and syntax checking is at the least abstract level, software architecture analysis and design are at the most abstract level. We conclude by providing a discussion on challenges for future development of AI-supported code completion tools to reach the design level of abstraction in our taxonomy."
303,2023,"Artificial Agents Inspired by Human Motivation Psychology for Teamwork in Hazardous Environments nan Multi-agent literature explores personifying artificial agents with personality, emotions or cognitive biases to produce typical, believable agents. In this study, we demonstrate the potential of endowing artificial agents with a motivation, using human implicit motivation psychology theory that introduces 3 motive profiles - power, achievement and affiliation, to create diverse, risk-aware agents. We first devise a framework to model these motivated agents (or agents with any inherent behavior), that can activate different strategies depending on the circumstances. We conduct experiments on a fire-fighting task domain, evaluate how motivated teams perform, and draw conclusions on appropriate team compositions to be deployed in environments with different risk levels. We find that motivational diversity within teams is beneficial in dynamic collaborative environments, especially as the task risk level increases. Furthermore, we observed that the best team composition in terms of the performance metrics used to evaluate teams, does not remain the same as the collaboration level required to achieve goals changes. These results have implications for future designs of risk-aware autonomous teams and Human-AI teams, as they highlight the prospects of creating better artificial teammates and performance gains that could be achieved through anthro-pomorphized motivated agents."
304,2024,"AI in mixed reality - Copilot on HoloLens: Spatial computing with large language models nan Mixed reality together with AI presents a human-first interface that promises to transform operations. Copilot can assist industrial workers in real-time with speech and holograms; generative AI is used to search technical documentation, service records, training content, and other sources. Copilot then summarizes to provide interactive guidance."
305,2024,"War Elephants: Rethinking Combat AI and Human Oversight nan This paper explores the changes that pervasive AI is having on the nature of combat. We look beyond the substitution of AI for experts to an approach where complementary human and machine abilities are blended. Using historical and modern examples, we show how autonomous weapons systems can be effectively managed by teams of human AI Operators combined with AI/ML Proxy Operators. By basing our approach on the principles of complementation, we provide for a flexible and dynamic approach to managing lethal autonomous systems. We conclude by presenting a path to achieving an integrated vision of machine-speed combat where the battlefield AI is operated by AI Operators that watch for patterns of behavior within battlefield to assess the performance of lethal autonomous systems. This approach enables the development of combat systems that are likely to be more ethical, operate at machine speed, and are capable of responding to a broader range of dynamic battlefield conditions than any purely autonomous AI system could support."
307,2023,"On the Concerns of Developers When Using GitHub Copilot [arXiv] nan With the recent advancement of Artificial Intelligence (AI) and the emergence of Large Language Models (LLMs), AI-based code generation tools have achieved significant progress and become a practical solution for software development. GitHub Copilot, referred to as AI pair programmer, utilizes machine learning models that are trained on a large corpus of code snippets to generate code suggestions or auto-complete code using natural language processing. Despite its popularity, there is little empirical evidence on the actual experiences of software developers who work with Copilot. To this end, we conducted an empirical study to understand the issues and challenges that developers face when using Copilot in practice, as well as their underlying causes and potential solutions. We collected data from 476 GitHub issues, 706 GitHub discussions, and 184 Stack Overflow posts, and identified the issues, causes that trigger the issues, and solutions that resolve the issues when using Copilot. Our results reveal that (1) Usage Issue and Compatibility Issue are the most common problems faced by Copilot users, (2) Copilot Internal Issue, Network Connection Issue, and Editor/IDE Compatibility Issue are identified as the most frequent causes, and (3) Bug Fixed by Copilot, Modify Configuration/Setting, and Use Suitable Version are the predominant solutions. Based on the results, we delve into the main challenges users encounter when implementing Copilot in practical development, the possible impact of Copilot on the coding process, aspects in which Copilot can be further enhanced, and potential new features desired by Copilot users."
308,2023,"Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems nan The dazzling promises of AI systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. Recent research has shown that appropriate reliance is the key to achieving complementary team performance in AI-assisted decision making. This paper addresses an under-explored problem of whether the Dunning-Kruger Effect (DKE) among people can hinder their appropriate reliance on AI systems. DKE is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. Through an empirical study (N = 249), we explored the impact of DKE on human reliance on an AI system, and whether such effects can be mitigated using a tutorial intervention that reveals the fallibility of AI advice, and exploiting logic units-based explanations to improve user understanding of AI advice. We found that participants who overestimate their performance tend to exhibit under-reliance on AI systems, which hinders optimal team performance. Logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. While the tutorial intervention was highly effective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. Our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on AI systems. Our findings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-AI decision making. This lays out promising future directions for relevant HCI research in this community."
309,2023,"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot [arXiv] nan Generative AI tools hold promise to increase human productivity. This paper presents results from a controlled experiment with GitHub Copilot, an AI pair programmer. Recruited software developers were asked to implement an HTTP server in JavaScript as quickly as possible. The treatment group, with access to the AI pair programmer, completed the task 55.8% faster than the control group. Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers."
310,2024,"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming nan Code-recommendation systems, such as Copilot and CodeWhisperer, have the potential to improve programmer productivity by suggesting and auto-completing code. However, to fully realize their potential, we must understand how programmers interact with these systems and identify ways to improve that interaction. To seek insights about human-AI collaboration with code recommendations systems, we studied GitHub Copilot, a code-recommendation system used by millions of programmers daily. We developed CUPS, a taxonomy of common programmer activities when interacting with Copilot. Our study of 21 programmers, who completed coding tasks and retrospectively labeled their sessions with CUPS, showed that CUPS can help us understand how programmers interact with code-recommendation systems, revealing inefficiencies and time costs. Our insights reveal how programmers interact with Copilot and motivate new interface designs and metrics."
311,2023,Shadows of the Past: The Effects of User's Differences and Past Experiences on Human-AI Partnership nan nan
312,2024,"Development of Machine Learning Copilot to Assist Novices in Learning Flexible Laryngoscopy nan ObjectivesHere we describe the development and pilot testing of the first artificial intelligence (AI) software copilot to help train novices to competently perform flexible fiberoptic laryngoscopy (FFL) on a mannikin and improve their uptake of FFL skills.MethodsSupervised machine learning was used to develop an image classifier model, dubbed the anatomical region classifier, responsible for predicting the location of camera in the upper aerodigestive tract and an object detection model, dubbed the anatomical structure detector, responsible for locating and identifying key anatomical structures in images. Training data were collected by performing FFL on an AirSim Combo Bronchi X mannikin (United Kingdom, TruCorp Ltd) using an Ambu aScope 4 RhinoLaryngo Slim connected to an Ambu (R) aView (TM) 2 Advance Displaying Unit (Ballerup, Ambu A/S). Medical students were prospectively recruited to try the FFL copilot and rate its ease of use and self-rate their skills with and without the copilot.ResultsThis model classified anatomical regions with an overall accuracy of 91.9% on the validation set and 80.1% on the test set. The model detected anatomical structures with overall mean average precision of 0.642. Through various optimizations, we were able to run the AI copilot at approximately 28 frames per second (FPS), which is imperceptible from real time and nearly matches the video frame rate of 30 FPS. Sixty-four novice medical students were recruited for feedback on the copilot. Although 90.9% strongly agreed/agreed that the AI copilot was easy to use, their self-rating of FFL skills following use of the copilot were overall equivocal to their self-rating without the copilot.ConclusionsThe AI copilot tracked successful capture of diagnosable views of key anatomical structures effectively guiding users through FFL to ensure all anatomical structures are sufficiently captured. This tool has the potential to assist novices in efficiently gaining competence in FFL.Level of EvidenceNA Laryngoscope, 2024This AI copilot tracks successful capture of diagnosable views of key anatomical structures effectively guiding users through flexible laryngoscopy to ensure all anatomical structures are sufficiently captured. This tool has the potential to assist novices in efficiently gaining competence in this procedure.image"
313,2024,"Catalyzing Equity in STEM Teams: Harnessing Generative AI for Inclusion and Diversity. nan Collaboration is key to STEM, where multidisciplinary team research can solve complex problems. However, inequality in STEM fields hinders their full potential, due to persistent psychological barriers in underrepresented students' experience. This paper documents teamwork in STEM and explores the transformative potential of computational modeling and generative AI in promoting STEM-team diversity and inclusion. Leveraging generative AI, this paper outlines two primary areas for advancing diversity, equity, and inclusion. First, formalizing collaboration assessment with inclusive analytics can capture fine-grained learner behavior. Second, adaptive, personalized AI systems can support diversity and inclusion in STEM teams. Four policy recommendations highlight AI's capacity: formalized collaborative skill assessment, inclusive analytics, funding for socio-cognitive research, human-AI teaming for inclusion training. Researchers, educators, and policymakers can build an equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration, offering a vision for the future of STEM where diverse voices are actively encouraged and heard within collaborative scientific endeavors."
314,2019,"Intelligent Squad Weapon: Challenges to Displaying and Interacting with Artificial Intelligence in Small Arms Weapon Systems nan The Army plans to integrate artificial intelligence (AI)/machine learning (ML) and other intelligent decision-making aids into future dismounted Warfighter systems to augment situational awareness and target acquisition capabilities. However, due to the unique constraints of dismounted operations, successful implementation of intelligent decision-making aids in dismounted systems necessitates a human-in-the-loop approach, which includes the ability for the Warfighter to provide feedback to the autonomous system. Human-in-the- loop feedback can augment current machine learning techniques by reducing the size of datasets needed to train algorithms and allow algorithms to be flexible and adaptive to changing battlespace conditions. As such, research is required to define the bidirectional interactions between man and machine in this context, to optimize human-intelligent agent teaming for the dismounted Warfighter. In this paper, we focus on a specific application of dismounted Human-AI interaction to weapon mounted target acquisition (small arms fire control systems) and discuss issues pertaining to an important component of this optimization: how intelligent information is communicated to the end user. We consider how the intelligent information is presented to the Warfighter, and what underlying cognitive and perceptual processes can be leveraged to optimize teamed decision making. Such factors are critical to the successful implementation of human-in-the-loop AI in dismounted applications and ultimately the effectiveness of intelligent decision-making aids."
315,2023,"Foundations for Human-AI teaming for self-regulated learning with explainable AI (XAI) nan This discussion takes a human-centred perspective of the contributions of the collection. Its papers explore diverse, new uses of AI with rich, multimedia sensor data towards new ways to measure and understand self-regulated learning. This work can contribute to the learning sciences. It can also provide a foundation for future personalised teaching and learning systems with explainable AI (XAI) and learner control. I will discuss the papers from that perspective with a focus on an important form of XAI in education - the Open Learner Models (OLM). When suitably designed, OLMs can empower a learner to: (1)contribute dataabout themself and their self-regulated learning processes, complementing conventional and multimedia data; (2)scrutinise and controllearner data collection and use in AI-based systems and (3) be the controlling partner in AI-teaming that scaffolds their self-regulated learning processes. All rights reserved Elsevier."
316,2022,"The Situation Awareness Framework for Explainable AI (SAFE-AI) and Human Factors Considerations for XAI Systems nan Recent advances in artificial intelligence (AI) have drawn attention to the need for AI systems to be understandable to human users. The explainable AI (XAI) literature aims to enhance human understanding and human-AI team performance by providing users with necessary information about AI system behavior. Simultaneously, the human factors literature has long addressed important considerations that contribute to human performance, including how to determine human informational needs, human workload, and human trust in autonomous systems. Drawing from the human factors literature, we propose the Situation Awareness Framework for Explainable AI (SAFE-AI), a three-level framework for the development and evaluation of explanations about AI system behavior. Our proposed levels of XAI are based on the informational needs of human users, which can be determined using the levels of situation awareness (SA) framework from the human factors literature. Based on our levels of XAI framework, we also suggest a method for assessing the effectiveness of XAI systems. We further detail human workload considerations for determining the content and frequency of explanations as well as metrics that can be used to assess human workload. Finally, we discuss the importance of appropriately calibrating user trust in AI systems through explanations along with other trust-related considerations for XAI, and we detail metrics that can be used to evaluate user trust in these systems."
317,2023,"Practices and Challenges of Using GitHub Copilot: An Empirical Study [arXiv] nan With the advances in Machine Learning, there is a growing interest in AI-enabled tools for autocompleting source code. GitHub Copilot, also referred to as the AI Pair Programmer, has trained on billions of lines of open source GitHub code, and is one of such tools that has been increasingly used since its launch on June 2021. However, little effort has been devoted to understand the practices and challenges of using Copilot in programming with auto-completed source code. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub Discussions. More specifically, we searched and manually collected 169 SO posts and 655 GitHub discussions related to the usage of Copilot. We identified the programming languages, IDEs, technologies used with Copilot, functions implemented, benefits, limitations, and challenges when using Copilot. The results show that when practitioners use Copilot: (1) The major programming languages used with Copilot are JavaScript and Python, (2) the main IDE used with Copilot is Visual Studio Code, (3) the most common used technology with Copilot is Node.js, (4) the leading function implemented by Copilot is data processing, (5) the significant benefit of using Copilot is useful code generation, and (6) the main limitation encountered by practitioners when using Copilot is difficulty of integration. Our results suggest that using Copilot is like a double-edged sword, which requires developers to carefully consider various aspects when deciding whether or not to use it. Our study provides empirically grounded foundations and basis for future research on the role of Copilot as an AI pair programmer in software development."
318,2024,"Who Pilots the Copilots? Mapping a Generative AI's Actor-Network to Assess Its Educational Impacts nan Generative AI (GenAI) is praised as a transformative force for education, with the potential to significantly alter teaching and learning. Despite its promise, debates persist regarding GenAI impacts, with critical voices highlighting the necessity for thorough ethical scrutiny. While traditional ethical evaluations of GenAI tend to focus on the opacity of AI decision-making, we argue that the true challenge for ethical evaluation extends beyond the models themselves, and to the socio-technical networks shaping GenAI development and training. To address this limitation, we present an evaluation method, called Ethical Network Evaluation for AI (ENEA), which combines Latour's Actor-Network Theory-used to map network dynamics by tracing actors' interests and values-with Brusseau's AI Human Impact framework, which identifies ethical indicators for evaluating AI systems. By applying ENEA to GenAI copilots in education, we show how making Actor-Networks visible lets us unveil a great variety of dilemmas, guiding ethical auditing and stakeholder discussions."
319,2023,"Bringing Together Ergonomic Concepts and Cognitive Mechanisms for Human-AI Agents Cooperation nan The deployment of artificial intelligence from experimental settings to concrete applications implies to consider the social aspects of the environment and consequently to conceive the interaction between humans and computers endowed with the aim of being partners in action. This article proposes a review of the research initiatives regarding human-artificial agents interaction, including eXplainable Artificial Intelligence (XAI) and HRI/HCI. We argue that even if vocabulary and approaches are different, the concepts converge on the necessity for the artificial agents to provide an accurate mental model of their behavior to the humans they are interacting with. This has different implications depending on whether we consider a tool/user interaction or a cooperation interaction-which is far less documented despite being at the heart of the future concepts of autonomous vehicles. From this observation, the article uses the cognitive science corpus on joint-action to raise finer cognitive mechanisms proved to be essential for human joint-action which could be considered as cognitive requirements for future artificial agents, including shared task representation and mentalization. Finally, interactions content hypotheses are arisen to satisfy the identified mechanisms, including the ability for the artificial agent to elicit its intentions and to trigger mentalization toward them from the human cooperators."
320,2024,"The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment [arXiv] nan The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a large organization. We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. Initially, participants used GitHub Copilot for proposed use-cases, with their feedback gathered through regular surveys. In the second phase, they were divided into Control and Copilot groups, each tackling the same Python challenges, and their experiences were again surveyed. Results showed a notable boost in productivity and code quality with GitHub Copilot, though its impact on code security remained inconclusive. Participant responses were overall positive, confirming GitHub Copilot's effectiveness in large-scale software engineering environments. Early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction."
321,2023,"Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue [arXiv] nan Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the partner's missing knowledge is a more viable approach than predicting one's own. We show that explicit modeling of the partner's dialogue moves and mental states produces improved and more stable results than without. These results provide insight for future AI agents that can predict what knowledge their partner is missing and, therefore, can proactively communicate such information to help their partner acquire such missing knowledge toward a common understanding of joint tasks."
322,2023,"Super-Human and Super-AI Cognitive Augmentation of Human and Human-AI Teams Assisted by Brain Computer Interfaces nan In the last 10 years a major strand of my research within the Essex Brain-Computer Interfaces and Neural Engineering (BCI-NE) laboratory has focused on the idea of combining brain signals (and other physiological and behavioral information) across multiple people to achieve a form of emergent, perceptual and, more generally cognitive, group augmentation, which is more than the sum of the parts.Over this period (in projects funded mostly by the UK Ministry of Defense and also US DOD) we have developed a technology which has delivered significant (and in fact in some cases remarkable) improvements over the group performance achieved by more traditional methods of integrating information applying it successfully to progressively more and more real-world applications.Our efforts have particularly focused on the area of decision making. Decisions (for example made by government, military or hospital management) are often made with limited amounts of information, or indeed too much information for any single person to take in, hence involving a high degree of uncertainty. Yet, such decisions can be highly critical in nature, with mistakes possibly resulting in extremely adverse outcomes, including loss of lives. So, any improvements in accuracy or speed of decisions in such conditions is vitally important.In the last 5 years we have also started to study hybrid human-AI decision-making groups by the inclusion of one or more AI-based teammates which act as peers to the humans. We found that when the conditions are right (more on this in the talk), human-AI groups produce super-human and super-AI performance.In this presentation, I will review BCIs and our approach, and will discuss some of the applications we explored including the identification of visual targets in cluttered environments, the comprehension of military radio communication, face recognition, military recognisance missions, military outposts and strategic resource allocation in a pandemic.I will finally look at potential future developments."
323,2022,"Artificial intelligence technologies and compassion in healthcare: A systematic scoping review. nan Background: Advances in artificial intelligence (AI) technologies, together with the availability of big data in society, creates uncertainties about how these developments will affect healthcare systems worldwide. Compassion is essential for high-quality healthcare and research shows how prosocial caring behaviors benefit human health and societies. However, the possible association between AI technologies and compassion is under conceptualized and underexplored.Objectives: The aim of this scoping review is to provide a comprehensive depth and a balanced perspective of the emerging topic of AI technologies and compassion, to inform future research and practice. The review questions were: How is compassion discussed in relation to AI technologies in healthcare? How are AI technologies being used to enhance compassion in healthcare? What are the gaps in current knowledge and unexplored potential? What are the key areas where AI technologies could support compassion in healthcare?Materials and methods: A systematic scoping review following five steps of Joanna Briggs Institute methodology. Presentation of the scoping review conforms with PRISMA-ScR (Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews). Eligibility criteria were defined according to 3 concept constructs (AI technologies, compassion, healthcare) developed from the literature and informed by medical subject headings (MeSH) and key words for the electronic searches. Sources of evidence were Web of Science and PubMed databases, articles published in English language 2011-2022. Articles were screened by title/abstract using inclusion/exclusion criteria. Data extracted (author, date of publication, type of article, aim/context of healthcare, key relevant findings, country) was charted using data tables. Thematic analysis used an inductive-deductive approach to generate code categories from the review questions and the data. A multidisciplinary team assessed themes for resonance and relevance to research and practice.Results: Searches identified 3,124 articles. A total of 197 were included after screening. The number of articles has increased over 10 years (2011, n = 1 to 2021, n = 47 and from Jan-Aug 2022 n = 35 articles). Overarching themes related to the review questions were: (1) Developments and debates (7 themes) Concerns about AI ethics, healthcare jobs, and loss of empathy; Human-centered design of AI technologies for healthcare; Optimistic speculation AI technologies will address care gaps; Interrogation of what it means to be human and to care; Recognition of future potential for patient monitoring, virtual proximity, and access to healthcare; Calls for curricula development and healthcare professional education; Implementation of AI applications to enhance health and wellbeing of the healthcare workforce. (2) How AI technologies enhance compassion (10 themes) Empathetic awareness; Empathetic response and relational behavior; Communication skills; Health coaching; Therapeutic interventions; Moral development learning; Clinical knowledge and clinical assessment; Healthcare quality assessment; Therapeutic bond and therapeutic alliance; Providing health information and advice. (3) Gaps in knowledge (4 themes) Educational effectiveness of AI-assisted learning; Patient diversity and AI technologies; Implementation of AI technologies in education and practice settings; Safety and clinical effectiveness of AI technologies. (4) Key areas for development (3 themes) Enriching education, learning and clinical practice; Extending healing spaces; Enhancing healing relationships.Conclusion: There is an association between AI technologies and compassion in healthcare and interest in this association has grown internationally over the last decade. In a range of healthcare contexts, AI technologies are being used to enhance empathetic awareness; empathetic response and relational behavior; communication skills; health coaching; therapeutic interventions; moral development learning; clinical knowledge and clinical assessment; healthcare quality assessment; therapeutic bond and therapeutic alliance; and to provide health information and advice. The findings inform a reconceptualization of compassion as a human-AI system of intelligent caring comprising six elements: (1) Awareness of suffering (e.g., pain, distress, risk, disadvantage); (2) Understanding the suffering (significance, context, rights, responsibilities etc.); (3) Connecting with the suffering (e.g., verbal, physical, signs and symbols); (4) Making a judgment about the suffering (the need to act); (5) Responding with an intention to alleviate the suffering; (6) Attention to the effect and outcomes of the response. These elements can operate at an individual (human or machine) and collective systems level (healthcare organizations or systems) as a cyclical system to alleviate different types of suffering. New and novel approaches to human-AI intelligent caring could enrich education, learning, and clinical practice; extend healing spaces; and enhance healing relationships.Implications: In a complex adaptive system such as healthcare, human-AI intelligent caring will need to be implemented, not as an ideology, but through strategic choices, incentives, regulation, professional education, and training, as well as through joined up thinking about human-AI intelligent caring. Research funders can encourage research and development into the topic of AI technologies and compassion as a system of human-AI intelligent caring. Educators, technologists, and health professionals can inform themselves about the system of human-AI intelligent caring."
324,2022,"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions nan There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described 'AI pair programmer', GitHub Copilot, which is a language model trained over open-source GitHub code. However, code often contains bugs-and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot's code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk cybersecurity weaknesses, e.g. those from MITRE's Top 25 Common Weakness Enumeration (CWE) list. We explore Copilot's performance on three distinct code generation axes-examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40% to be vulnerable."
325,2024,"When Teams Embrace AI: Human Collaboration Strategies in Generative Prompting in a Creative Design Task nan Studies of Generative AI (GenAI)-assisted creative workflows have focused on individuals overcoming challenges of prompting to produce what they envisioned. When designers work in teams, how do collaboration and prompting influence each other, and how do users perceive generative AI and their collaborators during the co-prompting process? We engaged students with design or performance backgrounds, and little exposure to GenAI, to work in pairs with GenAI to create stage designs based on a creative theme. We found two patterns of collaborative prompting focused on generating story descriptions first, or visual imagery first. GenAI tools helped participants build consensus in the task, and allowed for discussion of the prompting strategies. Participants perceived GenAI as efficient tools rather than true collaborators, suggesting that human partners reduced the reliance on their use. This work highlights the importance of human-human collaboration when working with GenAI tools, suggesting systems that take advantage of shared human expertise in the prompting process."
326,2024,"The Elephant in the Room -- Why AI Safety Demands Diverse Teams nan We consider that existing approaches to AI safety and alignment may not be using the most effective tools, teams, or approaches. We suggest that an alternative and better approach to the problem may be to treat alignment as a social science problem, since the social sciences enjoy a rich toolkit of models for understanding and aligning motivation and behavior, much of which could be repurposed to problems involving AI models, and enumerate reasons why this is so. We introduce an alternate alignment approach informed by social science tools and characterized by three steps: 1. defining a positive desired social outcome for human/AI collaboration as the goal or North Star, 2. properly framing knowns and unknowns, and 3. forming diverse teams to investigate, observe, and navigate emerging challenges in alignment."
327,2022,"The medical profession transformed by artificial intelligence: Qualitative study nan BackgroundHealthcaare delivery will change through the increasing use of artificial intelligence (AI). Physicians are likely to be among the professions most affected, though to what extent is not yet clear. ObjectiveWe analyzed physicians' and AI experts' stances towards AI-induced changes. This concerned (1) physicians' tasks, (2) job replacement risk, and (3) implications for the ways of working, including human-AI interaction, changes in job profiles, and hierarchical and cross-professional collaboration patterns. MethodsWe adopted an exploratory, qualitative research approach, using semi-structured interviews with 24 experts in the fields of AI and medicine, medical informatics, digital medicine, and medical education and training. Thematic analysis of the interview transcripts was performed. ResultsSpecialized tasks currently performed by physicians in all areas of medicine would likely be taken over by AI, including bureaucratic tasks, clinical decision support, and research. However, the concern that physicians will be replaced by an AI system is unfounded, according to experts; AI systems today would be designed only for a specific use case and could not replace the human factor in the patient-physician relationship. Nevertheless, the job profile and professional role of physicians would be transformed as a result of new forms of human-AI collaboration and shifts to higher-value activities. AI could spur novel, more interprofessional teams in medical practice and research and, eventually, democratization and de-hierarchization. ConclusionsThe study highlights changes in job profiles of physicians and outlines demands for new categories of medical professionals considering AI-induced changes of work. Physicians should redefine their self-image and assume more responsibility in the age of AI-supported medicine. There is a need for the development of scenarios and concepts for future job profiles in the health professions as well as their education and training."
328,2023,"AI-Assisted Security: A Step towards Reimagining Software Development for a Safer Future nan The security threats to software are increasing dramatically, and software security is non-negotiable in this age of information technology. AI-based tools can make code suggestions, improve developer productivity, and block insecure coding patterns for secure software development. In this paper, we analyze how GitHub Copilot provides AI assistants to erase code vulnerabilities and study how we should use AI assistants for secure software development."
329,2023,"Assessing the Security of GitHub Copilot Generated Code -- A Targeted Replication Study [arXiv] nan AI-powered code generation models have been developing rapidly, allowing developers to expedite code generation and thus improve their productivity. These models are trained on large corpora of code (primarily sourced from public repositories), which may contain bugs and vulnerabilities. Several concerns have been raised about the security of the code generated by these models. Recent studies have investigated security issues in AI-powered code generation tools such as GitHub Copilot and Amazon CodeWhisperer, revealing several security weaknesses in the code generated by these tools. As these tools evolve, it is expected that they will improve their security protocols to prevent the suggestion of insecure code to developers. This paper replicates the study of Pearce et al., which investigated security weaknesses in Copilot and uncovered several weaknesses in the code suggested by Copilot across diverse scenarios and languages (Python, C and Verilog). Our replication examines Copilot security weaknesses using newer versions of Copilot and CodeQL (the security analysis framework). The replication focused on the presence of security vulnerabilities in Python code. Our results indicate that, even with the improvements in newer versions of Copilot, the percentage of vulnerable code suggestions has reduced from 36.54% to 27.25%. Nonetheless, it remains evident that the model still suggests insecure code."
331,2021,"Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants nan Deep neural networks (DNNs) routinely achieve state-of-the-art performance in a wide range of tasks, but it can often be challenging for them to meet end-user needs in practice. This case study reports on the development of human-AI onboarding materials (i.e., training materials for users prior to using an AI) for a DNN-based medical AI Assistant to aid in the grading of prostate cancer. Specifically, we describe how the process of developing these materials changed the team's understanding of end-user requirements, contributing to modifications in the development and assessment of the underlying machine learning model. Importantly, we discovered that onboarding materials served as a useful boundary object for cross-functional teams, uncovering a new way to assess the ML model and specify its end-user requirements. We also present evidence of the utility of the onboarding materials by describing how it affected user strategies and decision-making with AI in a study deployment to pathologists."
332,2024,"AdversaFlow: Visual Red Teaming for Large Language Models with Multi-Level Adversarial Flow. nan Large Language Models (LLMs) are powerful but also raise significant security concerns, particularly regarding the harm they can cause, such as generating fake news that manipulates public opinion on social media and providing responses to unethical activities. Traditional red teaming approaches for identifying AI vulnerabilities rely on manual prompt construction and expertise. This paper introduces AdversaFlow, a novel visual analytics system designed to enhance LLM security against adversarial attacks through human-AI collaboration. AdversaFlow involves adversarial training between a target model and a red model, featuring unique multi-level adversarial flow and fluctuation path visualizations. These features provide insights into adversarial dynamics and LLM robustness, enabling experts to identify and mitigate vulnerabilities effectively. We present quantitative evaluations and case studies validating our system's utility and offering insights for future AI security solutions. Our method can enhance LLM security, supporting downstream scenarios like social media regulation by enabling more effective detection, monitoring, and mitigation of harmful content and behaviors."
333,2022,"Assessing the Quality of GitHub Copilot's Code Generation nan The introduction of GitHub's new code generation tool, GitHub Copilot, seems to be the first well-established instance of an AI pair-programmer. GitHub Copilot has access to a large number of open-source projects, enabling it to utilize more extensive code in various programming languages than other code generation tools. Although the initial and informal assessments are promising, a systematic evaluation is needed to explore the limits and benefits of GitHub Copilot. The main objective of this study is to assess the quality of generated code provided by GitHub Copilot. We also aim to evaluate the impact of the quality and variety of input parameters fed to GitHub Copilot. To achieve this aim, we created an experimental setup for evaluating the generated code in terms of validity, correctness, and efficiency. Our results suggest that GitHub Copilot was able to generate valid code with a 91.5% success rate. In terms of code correctness, out of 164 problems, 47 (28.7%) were correctly, while 84 (51.2%) were partially correctly, and 33 (20.1%) were incorrectly generated. Our empirical analysis shows that GitHub Copilot is a promising tool based on the results we obtained, however further and more comprehensive assessment is needed in the future."
335,2023,"Demystifying Practices, Challenges and Expected Features of Using GitHub Copilot nan With the advances in machine learning, there is a growing interest in AI-enabled tools for autocompleting source code. GitHub Copilot, also referred to as the AI Pair Programmer, has been trained on billions of lines of open source GitHub code, and is one of such tools that has been increasingly used since its launch in June 2021. However, little effort has been devoted to understanding the practices, challenges, and expected features of using Copilot in programming for auto-completed source code from the point of view of practitioners. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub Discussions. More specifically, we searched and manually collected 303 SO posts and 927 GitHub discussions related to the usage of Copilot. We identified the programming languages, Integrated Development Environments (IDEs), technologies used with Copilot, functions implemented, benefits, limitations, and challenges when using Copilot. The results show that when practitioners use Copilot: (1) The major programming languages used with Copilot are JavaScript and Python, (2) the main IDE used with Copilot is Visual Studio Code, (3) the most common used technology with Copilot is Node.js, (4) the leading function implemented by Copilot is data processing, (5) the main purpose of users using Copilot is to help generate code, (6) the significant benefit of using Copilot is useful code generation, (7) the main limitation encountered by practitioners when using Copilot is difficulty of integration, and (8) the most common expected feature is that Copilot can be integrated with more IDEs. Our results suggest that using Copilot is like a double-edged sword, which requires developers to carefully consider various aspects when deciding whether or not to use it. Our study provides empirically grounded foundations that could inform software developers and practitioners, as well as provide a basis for future investigations on the role of Copilot as an AI pair programmer in software development."
336,2022,"Partner-Aware Algorithms in Decentralized Cooperative Bandit Teams nan When humans collaborate with each other, they often make decisions by observing others and considering the consequences that their actions may have on the entire team, instead of greedily doing what is best for just themselves. We would like our AI agents to effectively collaborate in a similar way by capturing a model of their partners. In this work, we propose and analyze a decentralized Multi-Armed Bandit (MAB) problem with coupled rewards as an abstraction of more general multi-agent collaboration. We demonstrate that naive extensions of single-agent optimal MAB algorithms fail when applied for decentralized bandit teams. Instead, we propose a Partner-Aware strategy for joint sequential decision-making that extends the well-known single-agent Upper Confidence Bound algorithm. We analytically show that our proposed strategy achieves logarithmic regret, and provide extensive experiments involving human-AI and human-robot collaboration to validate our theoretical findings. Our results show that the proposed partner-aware strategy outperforms other known methods, and our human subject studies suggest humans prefer to collaborate with AI agents implementing our partner-aware strategy."
337,2023,"Joint Human and Autonomy Teaming for Defense: Status, Challenges, and Perspectives nan Human and Autonomy Teaming (HAT) involves humans, artificial intelligence and machine learning (AI/ML), and autonomous systems (AS) working together to enhance performance and efficiency across a variety of fields. The purpose of this paper is to outline the current status, challenges, and perspectives related to HAT in the defense applications, such as multi-domain operations (MDOs). This paper emphasizes that, although ASs have numerous benefits, humans, and machines need to work together to ensure effective communication and coordination. Furthermore, this paper addresses the issues of situation awareness, HAT bias, security, privacy, confidentiality, trust, transparency, and explainability as they relate to joint HAT in MDOs. Finally, this paper explores the potential of perspectives to overcome these challenges and enhance collaboration between humans, AI/ML, and AS in the context of defense."
338,2019,"Global Solutions vs. Local Solutions for the AI Safety Problem nan There are two types of artificial general intelligence (AGI) safety solutions: global and local. Most previously suggested solutions are local: they explain how to align or box a specific AI (Artificial Intelligence), but do not explain how to prevent the creation of dangerous AI in other places. Global solutions are those that ensure any AI on Earth is not dangerous. The number of suggested global solutions is much smaller than the number of proposed local solutions. Global solutions can be divided into four groups: 1. No AI: AGI technology is banned or its use is otherwise prevented; 2. One AI: the first superintelligent AI is used to prevent the creation of any others; 3. Net of AIs as AI police: a balance is created between many AIs, so they evolve as a net and can prevent any rogue AI from taking over the world; 4. Humans inside AI: humans are augmented or part of AI. We explore many ideas, both old and new, regarding global solutions for AI safety. They include changing the number of AI teams, different forms of AI Nanny (non-self-improving global control AI system able to prevent creation of dangerous AIs), selling AI safety solutions, and sending messages to future AI. Not every local solution scales to a global solution or does it ethically and safely. The choice of the best local solution should include understanding of the ways in which it will be scaled up. Human-AI teams or a superintelligent AI Service as suggested by Drexler may be examples of such ethically scalable local solutions, but the final choice depends on some unknown variables such as the speed of AI progress."
339,2023,"BO-Muse: A human expert and AI teaming framework for accelerated experimental design [arXiv] nan In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behaviour in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments."
340,2022,"An Empirical Evaluation of GitHub Copilot's Code Suggestions nan GitHub and OpenAI recently launched Copilot, an AI pair programmer that utilizes the power of Natural Language Processing, Static Analysis, Code Synthesis, and Artificial Intelligence. Given a natural language description of the target functionality, Copilot can generate corresponding code in several programming languages. In this paper, we perform an empirical study to evaluate the correctness and understandability of Copilot's suggested code. We use 33 LeetCode questions to create queries for Copilot in four different programming languages. We evaluate the correctness of the corresponding 132 Copilot solutions by running LeetCode's provided tests, and evaluate understandability using SonarQube's cyclomatic complexity and cognitive complexity metrics. We find that Copilot's Java suggestions have the highest correctness score (57%) while JavaScript is the lowest (27%). Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages. We also find some potential Copilot shortcomings, such as generating code that can be further simplified and code that relies on undefined helper methods."
341,2024,"Is Artificial Intelligence ageist? nan AimGenerative Artificial Intelligence (AI) is a technological innovation with wide applicability in daily life. However, it raises potential conflicts, such as biases and privacy infringements. Chatbots can offer support to older people, and it is necessary to explore whether their responses may perpetuate negative stereotypes of ageing.FindingsCopilot responded more ageistically, perpetuating negative stereotypes, especially regarding character and personality. Gemini was the least ageistic, mainly in the social area.MessageChatbots can offer support and facilitate the lives of older people, although some show negative stereotypes towards ageing. These findings suggest the need to address potential biases in AI to ensure equitable and respectful responses for all users.IntroductionGenerative Artificial Intelligence (AI) is a technological innovation with wide applicability in daily life, which could help elderly people. However, it raises potential conflicts, such as biases, omissions and errors.MethodsDescriptive study through the negative stereotypes towards aging questionnaire (CENVE) conducted on chatbots ChatGPT, Gemini, Perplexity, YOUChat, and Copilot was conducted.ResultsOf the chatbots studied, three were above 50% in responses with negative stereotypes, Copilot with high ageism level results, followed by Perplexity. In the health section, Copilot was the chatbot with the most negative connotations regarding old age (13 out of 20 points). In the personality section, Copilot scored 14 out of 20, followed by YOUChat.ConclusionThe Copilot chatbot responded to the statements more ageistically than the other platforms. These results highlight the importance of addressing any potential biases in AI to ensure that the responses provided are fair and respectful for all potential users."
342,2023,"Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools nan AI-based design tools are proliferating in professional software to assist engineering and industrial designers in complex manufacturing and design tasks. These tools take on more agentic roles than traditional computer-aided design tools and are often portrayed as co-creators. Yet, working effectively with such systems requires different skills than working with complex CAD tools alone. To date, we know little about how engineering designers learn to work with AI-based design tools. In this study, we observed trained designers as they learned to work with two AI-based tools on a realistic design task. We find that designers face many challenges in learning to effectively co-create with current systems, including challenges in understanding and adjusting AI outputs and in communicating their design goals. Based on our findings, we highlight several design opportunities to better support designer-AI co-creation."
343,2023,"From Manual to Automatic: The Evolution of Test Case Generation Methods and the Role of GitHub Copilot nan The emergence of AI tools, such as GitHub Copilot, introduces innovative approaches to software development tasks, particularly in the realm of test case generation. This research endeavors to evaluate the effectiveness of test cases generated by GitHub Copilot in comparison to manually crafted ones. Through the examination of four programming exercises, This study conducted a comparative analysis based on criteria including test passing rates, uniqueness, and equivalence. Preliminary findings suggest that the quality of test cases produced by Copilot is on par with manually created ones, indicating its potential to diversify the range of test cases when provided with precise prompts. While Copilot has the potential to enhance testing processes, it's important to consider certain limitations, such as constraints related to the range of test cases and potential prompt biases. Further research endeavors should explore a wider range of software development tasks and investigate how developer expertise influences the outcomes of test cases generated by AI tools."
344,2024,"Effective Lesson Planning and Assessment Design Using Leveraging Microsoft Copilot Implementation nan This study explores the beneficial uses of Microsoft Copilot as a support tool for Baliwag Polytechnic College instructors' lesson planning and activity design. Researchers evaluate the influence of Copilot on the creation of instructional content by examining the experiences and opinions of educators. The study demonstrates the advantages, difficulties, and opportunities for customization that come with incorporating Copilot into the curriculum. The results indicate that Copilot can significantly improve the effectiveness and caliber of lesson design, but also highlight certain implementation issues. This research offers insights into the future of technology-enhanced education and contributes to the expanding body of research on AI-assisted teaching strategies."
345,2024,"Optimizing Risk-averse Human-AI Hybrid Teams nan We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations."
346,2023,"Fair and equitable AI in biomedical research and healthcare: Social science perspectives nan Artificial intelligence (AI) offers opportunities but also challenges for biomedical research and healthcare. This position paper shares the results of the international conference Fair medicine and AI (online 3-5 March 2021). Scholars from science and technology studies (STS), gender studies, and ethics of science and technology formulated opportunities, challenges, and research and development desiderata for AI in healthcare. AI systems and solutions, which are being rapidly developed and applied, may have undesirable and unintended consequences including the risk of perpetuating health inequalities for marginalized groups. Socially robust development and implications of AI in healthcare require urgent investigation. There is a particular dearth of studies in human-AI interaction and how this may best be configured to dependably deliver safe, effective and equitable healthcare. To address these challenges, we need to establish diverse and interdisciplinary teams equipped to develop and apply medical AI in a fair, accountable and transparent manner. We formulate the importance of including social science perspectives in the development of intersectionally beneficent and equitable AI for biomedical research and healthcare, in part by strengthening AI health evaluation."
347,2020,"CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions nan Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions."
348,2024,"We Train AI, Why not Humans, Too? An Exploration of Human-AI Team Training for Future Workplace Viability nan nan"
349,2024,"Assessment of readability, reliability, and quality of ChatGPT, BARD, Gemini, Copilot, Perplexity responses on palliative care nan There is no study that comprehensively evaluates data on the readability and quality of palliative care information provided by artificial intelligence (AI) chatbots ChatGPT (R), Bard (R), Gemini (R), Copilot (R), Perplexity (R). Our study is an observational and cross-sectional original research study. In our study, AI chatbots ChatGPT (R), Bard (R), Gemini (R), Copilot (R), and Perplexity (R) were asked to present the answers of the 100 questions most frequently asked by patients about palliative care. Responses from each 5 AI chatbots were analyzed separately. This study did not involve any human participants. Study results revealed significant differences between the readability assessments of responses from all 5 AI chatbots (P < .05). According to the results of our study, when different readability indexes were evaluated holistically, the readability of AI chatbot responses was evaluated as Bard (R), Copilot (R), Perplexity (R), ChatGPT (R), Gemini (R), from easy to difficult (P < .05). In our study, the median readability indexes of each of the 5 AI chatbots Bard (R), Copilot (R), Perplexity (R), ChatGPT (R), Gemini (R) responses were compared to the recommended 6th grade reading level. According to the results of our study answers of all 5 AI chatbots were compared with the 6th grade reading level, statistically significant differences were observed in the all formulas (P < .001). The answers of all 5 artificial intelligence robots were determined to be at an educational level well above the 6th grade level. The modified DISCERN and Journal of American Medical Association scores was found to be the highest in Perplexity (R) (P < .001). Gemini (R) responses were found to have the highest Global Quality Scale score (P < .001). It is emphasized that patient education materials should have a readability level of 6th grade level. Of the 5 AI chatbots whose answers about palliative care were evaluated, Bard (R), Copilot (R), Perplexity (R), ChatGPT (R), Gemini (R), their current answers were found to be well above the recommended levels in terms of readability of text content. Text content quality assessment scores are also low. Both the quality and readability of texts should be brought to appropriate recommended limits."
350,2021,"Human-AI Collaboration with Bandit Feedback nan Human-machine complementarity is important when neither the algorithm nor the human yield dominant performance across all instances in a given domain. Most research on algorithmic decision-making solely centers on the algorithm's performance, while recent work that explores human-machine collaboration has framed the decision-making problems as classification tasks. In this paper, we first propose and then develop a solution for a novel human-machine collaboration problem in a bandit feedback setting. Our solution aims to exploit the human-machine complementarity to maximize decision rewards. We then extend our approach to settings with multiple human decision makers. We demonstrate the effectiveness of our proposed methods using both synthetic and real human responses, and find that our methods outperform both the algorithm and the human when they each make decisions on their own. We also show how personalized routing in the presence of multiple human decision-makers can further improve the human-machine team performance."
351,2023,"Alexa, How Can I Trust You Again? Trust Repair in Human-AI Teams nan nan"
352,2024,"Assessing the Security of GitHub Copilot's Generated Code - A Targeted Replication Study nan AI-powered code generation models have been developing rapidly, allowing developers to expedite code generation and thus improve their productivity. These models are trained on large corpora of code (primarily sourced from public repositories), which may contain bugs and vulnerabilities. Several concerns have been raised about the security of the code generated by these models. Recent studies have investigated security issues in AI-powered code generation tools such as GitHub Copilot and Amazon Code Whisperer, revealing several security weaknesses in the code generated by these tools. As these tools evolve, it is expected that they will improve their security protocols to prevent the suggestion of insecure code to developers. This paper replicates the study of Pearce et al., which investigated security weaknesses in Copilot and uncovered several weaknesses in the code suggested by Copilot across diverse scenarios and languages (Python, C, and Verilog). Our replication examines Copilot's security weaknesses using newer versions of Copilot and CodeQL (the security analysis framework). The replication focused on the presence of security vulnerabilities in Python code. Our results indicate that, with the improvements in newer versions of Copilot, the percentage of vulnerable code suggestions has reduced from 36.54% to 27.25%. Nonetheless, it remains evident that the model still suggests insecure code."
353,2024,"In human-machine trust, humans rely on a simple averaging strategy. nan With the growing role of artificial intelligence (AI) in our lives, attention is increasingly turning to the way that humans and AI work together. A key aspect of human-AI collaboration is how people integrate judgements or recommendations from machine agents, when they differ from their own judgements. We investigated trust in human-machine teaming using a perceptual judgement task based on the judge-advisor system. Participants ( n = 89 ) estimated a perceptual quantity, then received a recommendation from a machine agent. The participants then made a second response which combined their first estimate and the machine's recommendation. The degree to which participants shifted their second response in the direction of the recommendations provided a measure of their trust in the machine agent. We analysed the role of advice distance in people's willingness to change their judgements. When a recommendation falls a long way from their initial judgement, do people come to doubt their own judgement, trusting the recommendation more, or do they doubt the machine agent, trusting the recommendation less? We found that although some participants exhibited these behaviours, the most common response was neither of these tendencies, and a simple model based on averaging accounted best for participants' trust behaviour. We discuss implications for theories of trust, and human-machine teaming."
354,2023,"A Case Study on Scaffolding Exploratory Data Analysis for AI Pair Programmers nan Recent advances in automatic code generation have made tools like GitHub Copilot attractive for programmers, as they allow for the creation of code blocks by simply providing descriptive prompts to the AI. While researchers have studied the performance of these AI-based tools in general-purpose programming, their effectiveness in data analysis is understudied. Unlike general-purpose programming which focuses more on algorithm-driven tasks like building novel software, data analysis requires a data-driven approach to actually gain insights. It remains unclear how these tools could be utilized to help data scientists analyze real-world problems. In this paper, we conducted a qualitative user study with 5 participants to understand the use of GitHub Copilot in solving problems by scaffolding prompts at different levels of specificity among data scientists. We discovered that effective prompts require carefully selected terminology, properly arranged word order, and sufficiently established interaction between humans and GitHub Copilot. We also spot some potential flaws in GitHub Copilot that hinder data scientists from efficiently scaffolding prompts. Our work points out some improvement directions for both data scientists and GitHub Copilot in the future."
355,2024,"Intelligence as Agency nan In his 2009 AAAI article [10], Jonathan Grudin described AI and HCI as two fields divided by a common focus, noting how they competed for intellectual and economic resources (Fig. 1(a)). HCI is in its strongest position yet - with several senior HCI researchers leading human-centered AI teams, organizations, and institutes at major companies and universities. But, there continues to be the risk that history repeats itself: that HCI finds itself primarily reacting to advances in AI [19], rather than being a coequal discipline that exerts pressures that drive advances in AI as well. In this talk, I will propose two conceptual shifts that more explicitly center HCI values in an era of rapid progress in AI: (1) redefining intelligence as agency, the capacity to meaningfully act, rather than the capacity to perform a task; and (2) formulating design as the delegation of constrained agency, rather than solely the specification of affordances."
356,2019,"'Alexa, Do You Know Anything?' The Impact of an Intelligent Assistant on Team Interactions and Creative Performance Under Time Scarcity [arXiv] nan Human-AI collaboration is on the rise with the deployment of AI-enabled intelligent assistants (e.g. Amazon Echo, Cortana, Siri, etc.) across organizational contexts. It is claimed that intelligent assistants can help people achieve more in less time (Personal Digital Assistant - Cortana, n.d.). However, despite the increasing presence of intelligent assistants in collaborative settings, there is a void in the literature on how the deployment of this technology intersects with time scarcity to impact team behaviors and performance. To fill this gap in the literature, we collected behavioral data from 56 teams who participated in a between-subjects 2 (Intelligent Assistant: Available vs. Not Available) x 2 (Time: Scarce vs. Not Scarce/Control) lab experiment. The results show that teams with an intelligent assistant had significantly fewer interactions between its members compared to teams without an intelligent assistant. Teams who faced time scarcity also used the intelligent assistant more often to seek its assistance during task completion compared to those in the control condition. Lastly, teams with an intelligent assistant underperformed on a creative task compared to those without the device. We discuss implications of this technology from theoretical, empirical, and practical perspectives."
357,2024,"The Fine Balance Between Helping With Your Job and Taking It: AI Code Assistants Come to the Fore nan AI code generation tools are reshaping the software engineering landscape. We provide recommendations for practitioners interested in these tools based on narratives we have collected regarding two AI code generation tools, GitHub Copilot and Tabnine."
358,2024,"A Study on the Process of Creating Persona Using Generative AI - Focus on Imagination and Story nan This study aims to explore the potential and limitations of utilizing generative artificial intelligence in the persona creation process of user experience (UX) design by examining its impact. To achieve this, we conducted a comparative analysis of persona creation processes between a non-AI team and an AI team through protocol analysis and focus group interviews (FGI). The results showed that AI usage limited the designers' imagination, particularly reducing the consideration of different perspectives in persona creation. Additionally, AI usage decreased discussions about story construction, especially those describing the psychological and mental aspects of characters. Consequently, the use of AI hindered the formation of empathy towards personas. However, positive outcomes such as providing inspiration and preventing self-referential bias were also observed. This study suggests that using AI in design methodologies may weaken the human-centered aspects of UX design. It also emphasizes the need for a deeper consideration of enhancing human designers' creativity and empathy when integrating generative AI into the design process, beyond mere task automation."
359,2023,"Democratizing Content Creation and Consumption through Human-AI Copilot Systems nan Content creation and consumption play vital roles in our lives. However, creating high-quality content can be challenging for beginners, while navigating through and consuming vast amounts of media content can be overwhelming and cumbersome. My Ph.D. research focuses on democratizing content creation and improving content consumption experiences for everyday users. I achieve this by designing and evaluating interactive AI systems that serve as copilots, assisting users with tedious tasks. I explore various media modalities, such as video, audio, text, and images, and investigate how their interplay can address problems in individual modalities. This paper offers a comprehensive overview of my research agenda, including recent contributions, on-going progress, and future directions."
360,2024,"Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via Cheap and Efficient Prompt-Engineering nan AI assistants for coding are on the rise. However one of the reasons developers and companies avoid harnessing their full potential is the questionable security of the generated code. This paper first reviews the current state-of-the-art and identifies areas for improvement on this issue. Then, we propose a systematic approach based on prompt-altering methods to achieve better code security of (even proprietary black-box) AI-based code generators such as GitHub Copilot, while minimizing the complexity of the application from the user point-of-view, the computational resources, and operational costs. In sum, we propose and evaluate three prompt altering methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we discuss their combination. Contrary to the audit of code security, the latter two of the proposed methods require no expert knowledge from the user. We assess the effectiveness of the proposed methods on the GitHub Copilot using the OpenVPN project in realistic scenarios, and we demonstrate that the proposed methods reduce the number of insecure generated code samples by up to 16\% and increase the number of secure code by up to 8\%. Since our approach does not require access to the internals of the AI models, it can be in general applied to any AI-based code synthesizer, not only GitHub Copilot."
361,2024,"Assessing GitHub Copilot in Solidity Development: Capabilities, Testing, and Bug Fixing nan In the rapidly evolving landscape of blockchain technology, the development of reliable and secure smart contracts represents one of several crucial challenges. GitHub Copilot, an AI-powered code assistant, aims to enhance developer productivity by generating code snippets, facilitating testing, and assisting in program repair. This research examines Copilot's proficiency in generating functional and secure smart contracts, including token creation adhering to standards such as ERC20, ERC721, and ERC1155 with various optional features. Additionally, the study assesses its effectiveness in common development tasks, including the implementation of widely employed libraries such as SafeMath. Through controlled experiments, the accuracy, efficiency, and security of the code generated by Copilot are evaluated. This evaluation identifies both its strengths in expediting the development process and its limitations in managing complex blockchain-specific logic and security considerations. The findings contribute to an expanded understanding of the role of AI-assisted programming in blockchain development, offering insights into how developers can best leverage such tools in creating and testing smart contracts. This research aims to guide both practitioners and researchers in the blockchain domain, advancing the discussion on integrating AI into software development workflows in the context of Solidity and smart contract development, underscoring the need for further research to address the challenges and opportunities presented by AI in blockchain technology."
363,2024,"Generative AI for Code Generation: Software Reuse Implications nan Generative AI has lately started being used in the software engineering process. Developers are relying on ChatGPT, GitHub Copilot or other tools to accelerate the development process. Previous works have provided an overview of the tools and have compared their capabilities. Nevertheless, the relation with software reuse in the framework of Generative AI has not been examined extensively. In this work, we are studying how generative AI techniques respond to and affect software reuse, with an emphasis on software licensing issues and end-users' data privacy. We are using the following five tools: OpenAI ChatGPT, Google Gemini, GitHub Copilot, TabNine and Amazon CodeWhisperer. We provide an overview of the tools and previous works that have used them when examining code generation, discuss the implications on software reuse, and use a simple front-end use case to showcase how they respond on licensing and end-users' data privacy issues. This work introduces also a conceptual model that can help in improvements in the discussed reuse aspects."
364,2024,"Assessing the Quality of Patient Education Materials on Cardiac Catheterization From Artificial Intelligence Chatbots: An Observational Cross-Sectional Study. nan Background Health literacy empowers patients to participate in their own healthcare. Personal health literacy is one's ability to find, understand, and use information/resources to make well-informed health decisions. Artificial intelligence (AI) has become a source for the acquisition of health-related information through large language model (LLM)-driven chatbots. Assessment of the readability and quality of health information produced by these chatbots has been the subject of numerous studies to date. This study seeks to assess the quality of patient education materials on cardiac catheterization produced by AI chatbots. Methodology We asked a set of 10 questions about cardiac catheterization to four chatbots: ChatGPT (OpenAI, San Francisco, CA), Microsoft Copilot (Microsoft Corporation, Redmond, WA), Google Gemini (Google DeepMind, London, UK), and Meta AI (Meta, New York, NY). The questions and subsequent answers were utilized to make patient education materials on cardiac catheterization. The quality of these materials was assessed using two validated instruments for patient education materials: DISCERN and the Patient Education Materials Assessment Tool (PEMAT). Results The overall DISCERN scores were 4.5 for ChatGPT, 4.4 for Microsoft Copilot and Google Gemini, and 3.8 for Meta AI. ChatGPT, Microsoft Copilot, and Google Gemini tied for the highest reliability score at 4.6, while Meta AI had the lowest with 4.2. ChatGPT had the highest quality score at 4.4, while Meta AI had the lowest with 3.4. ChatGPT and Google Gemini had Understandability scores of 100%, while Meta AI had the lowest with 82%. ChatGPT, Microsoft Copilot, and Google Gemini all had Actionability scores of 75%, while Meta AI had one of 50%. Conclusions ChatGPT produced the most reliable and highest quality materials, followed closely by Google Gemini. Meta AI produced the lowest quality materials. Given the easy accessibility that chatbots provide patients and the high-quality responses that we obtained, they could be a reliable source for patients to obtain information about cardiac catheterization."
365,2021,"Working Alongside Non-Human Agents nan We coexist with non-human AI agents, and we now must plan for human and non-human-agent teaming, for cooperation and collaboration, as a means to expand collaborative intelligence in our ongoing quest for user advocacy. For practice and experimentation, we provide links to current non-human agents. We then distinguish automation and autonomy, and discuss humanness design, teaming. A deeper understanding of usability and ethical considerations for working alongside these systems, deploying robots and building bonds and trust with nonhuman agents, begins with differentiation of automation and autonomy, human-autonomy teaming, and a humanness design approach as a means to prevent undesirable autonomy. While TPC scholarship attends to privacy, accountability, safety and security, and transparency and explainability, we need additional vigilance regarding fairness and non-discrimination, human control of technology, TPC professional responsibility, and continued promotion of human values as we work alongside non-human agents."
366,2024,"AI in Teacher Education: An Introductory Training Session for Pre-service Teachers Involving Microsoft Copilot nan Artificial Intelligence (AI) has recently emerged as a transformative force due to the ability of large language models to generate surprisingly human-like responses to complex queries and tasks across various domains. Educators should embrace the technological affordances of AI to improve their teaching. However, it is first incumbent upon teacher educators to develop training programs that familiarize teachers with AI tools and their applications in educational settings. The aim of this study was to design a 45-min training session for pre-service teachers to use the AI chatbot Microsoft Copilot and identify ways AI can be used to assist with future teaching practices, particularly the creation of learning materials. The design of the intervention focused on demonstrating the usefulness and ease of use of the AI chatbot. Data from 43 pre-service teachers was collected and AI-assisted qualitative analysis performed. The results show that teachers perceive the AI tool to be most beneficial in the following categories: lesson planning, creating assessment questions and tasks, creating images, brainstorming and idea generation, creating educational games, generating text and rewriting text. This study provides initial insights on how AI tools like Microsoft Copilot can be introduced to teachers and how teacher education programs can help develop AI literacy among educators. More research is necessary to explore the long-term impacts of such training on teaching practices and student outcomes."
367,2024,"AI-Generated Test Scripts forWeb E2E Testing with ChatGPT and Copilot: A Preliminary Study nan Automated testing is vital for ensuring the reliability of web applications. This paper presents a preliminary study on leveraging artificial intelligence (AI) models, specifically ChatGPT and Github Copilot, to generate test scripts forweb end-to-end testing. Through experimentation, we evaluated the feasibility and effectiveness of AI language models in generating test scripts based on natural language descriptions of user interactions with web applications.Our preliminary results show that AI-based generation generally provides an advantage over fully manual test scripts development. Starting from test cases clearly defined in Gherkin, a reduction in development time is always observable. In some cases, this reduction is statistically significant (e.g., Manual vs. a particular use of ChatGPT). These results are valid provided that the tester has some skills in manual test script development and is therefore able to modify the code produced by the AI-generation tools. This study contributes to the exploration of AI-driven solutions in web test scripts generation and lays the foundation for future research in this domain."
368,2024,"Navigating Applications Development in Generative AI nan In its earliest prototypes, GitHub CoPilot stood apart from other tools. It demonstrated a remarkable ability to accelerate the work of intermediate coders by 20% to 40%, mainly with standardized languages like Python. This is a significant productivity gain in a labor-intensive activity, where such breakthroughs are rare."
369,2024,"AI in PCB Design: Insights from a Focused Case Study nan This paper explores the integration of Artificial Intelligence (AI) in Printed Circuit Board (PCB) design tools, highlighting the transformative impact AI can have on PCB design workflows. Based on the available documentation and secondary sources, nine AI-powered PCB design tools are analyzed, with a discussion of their features, strengths, and limitations. Due to access constraints, only Flux.ai was tested directly, serving as a focused case study. The case study demonstrates the capabilities of Flux.ai's AI Copilot feature, while also acknowledging its limitations. Our findings advocate for a future where AI-driven design tools enhance efficiency, precision, and user-friendliness in PCB design, setting new benchmarks for the field."
370,2024,Exploring the Use of AI Agents to Simulate Human Behavior in Group Decision-Making nan nan
371,2023,"On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Diferent Types of Interactive Explanations nan Explainability (XAI) has matured in recent years to provide more human-centered explanations of AI-based decision systems. While static explanations remain predominant, interactive XAI has gathered momentum to support the human cognitive process of explaining. However, the evidence regarding the benefts of interactive explanations is unclear. In this paper, we map existing fndings by conducting a detailed scoping review of 48 empirical studies in which interactive explanations are evaluated with human users. We also create a classifcation of interactive techniques specifc to XAI and group the resulting categories according to their role in the cognitive process of explanation: selective, mutable or dialogic. We identify the efects of interactivity on several user-based metrics. We fnd that interactive explanations improve perceived usefulness and performance of the human+AI team but take longer. We highlight conficting results regarding cognitive load and overconfdence. Lastly, we describe underexplored areas including measuring curiosity or learning or perturbing outcomes."
373,2024,"The impact of generative AI on collaborative open-source software development: evidence from github copilot nan Generative artificial intelligence (AI) has opened the possibility of automated content production, including coding in software development, which can significantly influence the participation and performance of software developers. To explore this impact, we investigate the role of GitHub Copilot, a generative AI pair programmer, on software development in open-source community, where multiple developers voluntarily collaborate on software projects. Using GitHub's dataset for open-source repositories and a generalized synthetic control method, we find that Copilot significantly enhances project-level productivity by 6.5%. Delving deeper, we dissect the key mechanisms driving this improvement. Our findings reveal a 5.5% increase in individual productivity and a 5.4% increase in participation. However, this is accompanied with a 41.6% increase in integration time, potentially due to higher coordination costs. Interestingly, we also observe the differential effects among developers. We discover that core developers achieve greater project-level productivity gains from using Copilot, benefiting more in terms of individual productivity and participation compared to peripheral developers, plausibly due to their deeper familiarity with software projects. We also find that the increase in project-level productivity is accompanied with no change in code quality. We conclude that AI pair programmers bring benefits to developers to automate and augment their code, but human developers' knowledge of software projects can enhance the benefits. In summary, our research underscores the role of AI pair programmers in impacting project-level productivity within the open-source community and suggests potential implications for the structure of open-source software projects."
374,2024,"Non-invasive brain-machine interface control with artificial intelligence copilots. nan Motor brain-machine interfaces (BMIs) decode neural signals to help people with paralysis move and communicate. Even with important advances in the last two decades, BMIs face key obstacles to clinical viability. Invasive BMIs achieve proficient cursor and robotic arm control but require neurosurgery, posing significant risk to patients. Non-invasive BMIs do not have neurosurgical risk, but achieve lower performance, sometimes being prohibitively frustrating to use and preventing widespread adoption. We take a step toward breaking this performance-risk tradeoff by building performant non-invasive BMIs. The critical limitation that bounds decoder performance in non-invasive BMIs is their poor neural signal-to-noise ratio. To overcome this, we contribute (1) a novel EEG decoding approach and (2) artificial intelligence (AI) copilots that infer task goals and aid action completion. We demonstrate that with this AI-BMI, in tandem with a new adaptive decoding approach using a convolutional neural network (CNN) and ReFIT-like Kalman filter (KF), healthy users and a paralyzed participant can autonomously and proficiently control computer cursors and robotic arms. Using an AI copilot improves goal acquisition speed by up to 4.3* in the standard center-out 8 cursor control task and enables users to control a robotic arm to perform the sequential pick-and-place task, moving 4 randomly placed blocks to 4 randomly chosen locations. As AI copilots improve, this approach may result in clinically viable non-invasive AI-BMIs."
375,2024,"Benchmarking ChatGPT, Codeium, and GitHub Copilot: A Comparative Study of AI-Driven Programming and Debugging Assistants nan With the increasing adoption of AI-driven tools in software development, large language models (LLMs) have become essential for tasks like code generation, bug fixing, and optimization. Tools like ChatGPT, GitHub Copilot, and Codeium provide valuable assistance in solving programming challenges, yet their effectiveness remains underexplored. This paper presents a comparative study of ChatGPT, Codeium, and GitHub Copilot, evaluating their performance on LeetCode problems across varying difficulty levels and categories. Key metrics such as success rates, runtime efficiency, memory usage, and error-handling capabilities are assessed. GitHub Copilot showed superior performance on easier and medium tasks, while ChatGPT excelled in memory efficiency and debugging. Codeium, though promising, struggled with more complex problems. Despite their strengths, all tools faced challenges in handling harder problems. These insights provide a deeper understanding of each tool's capabilities and limitations, offering guidance for developers and researchers seeking to optimize AI integration in coding workflows."
376,2023,"Naruto Mobile: AI Sparring Partner Using Heterogeneous Deep Reinforcement Learning nan Naruto Mobile is a popular mobile Fighting Game with over 100 million registered players. AI agents are deployed extensively to the game for a wide variety of applications such as level challenges and player training, which require them to fight like humans and imitate strong and weak players. Although deep reinforcement learning is an excellent approach to creating agents with diverse behaviors, it is difficult to apply to massive-scale games like Naruto Mobile which is built on a pool of more than 300 characters that have unique skills, speed, and attack range, as a traditional approach of self-play training at such scale may require a substantial computational cost and training time.In this paper, we present a new AI training approach called Heterogeneous Exploitation Self-Play (HESP) to improve AI agent generalization ability in Naruto Mobile and optimize its massive-scale self-play training so that the computational costs and train time are significantly reduced. The proposed algorithm has already been employed by the development team of Naruto Mobile to create AI agents, which, at the time of writing this paper, have been used in more than 300 million human-AI fighting matches. To the best of our knowledge, this is the first time that deep reinforcement learning has been employed by a commercial fighting game."
377,2024,"Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams [arXiv] nan Generative AI models are increasingly powering software applications, offering the capability to produce expressive content across varied contexts. However, unlike previous iterations of human-AI design, the emerging design process for generative capabilities primarily hinges on prompt engineering strategies. Given this fundamental shift in approach, our work aims to understand how collaborative software teams set up and apply design guidelines and values, iteratively prototype prompts, and evaluate prompts to achieve desired outcomes. We conducted design studies with 39 industry professionals, including designers, software engineers, and product managers. Our findings reveal a content-centric prototyping approach in which teams begin with the content they want to generate, then identify specific attributes, constraints, and values, and explore methods to give users the ability to influence and interact with those attributes. Based on associated challenges, such as the lack of model interpretability and overfitting the design to examples, we outline considerations for generative AI prototyping."
378,2024,"Rethinking software engineering in the foundation model era: from task-driven ai copilots to goal-driven AI pair programmers nan The advent of Foundation Models (FMs) and AI-powered copilots has transformed the landscape of software development, offering unprecedented code completion capabilities and enhancing developer productivity. However, the current task-driven nature of these copilots falls short in addressing the broader goals and complexities inherent in software engineering (SE). In this paper, we propose a paradigm shift towards goal-driven AI-powered pair programmers that collaborate with human developers in a more holistic and context-aware manner. We envision AI pair programmers that are goal-driven, human partners, SE-aware, and self-learning. These AI partners engage in iterative, conversation-driven development processes, aligning closely with human goals and facilitating informed decision-making. We discuss the desired attributes of such AI pair programmers and outline key challenges that must be addressed to realize this vision. Ultimately, our work represents a shift from AI-augmented SE to AI-transformed SE by replacing code completion with a collaborative partnership between humans and AI that enhances both productivity and software quality."
379,2023,Do You Own the Code AI Helps You Create? > GitHub Copilot kerfuffle spotlights legal gray area nan nan
380,2024,"Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot nan GitHub Copilot is an AI-enabled tool that automates program synthesis. It has gained significant attention since its launch in 2021. Recent studies have extensively examined Copilot's capabilities in various programming tasks, as well as its security issues. However, little is known about the effect of different natural languages on code suggestion. Natural language is considered a social bias in the field of NLP, and this bias could impact the diversity of software engineering. To address this gap, we conducted an empirical study to investigate the effect of three popular natural languages (Eng-lish, Japanese, and Chinese) on Copilot. We used 756 questions of varying difficulty levels from AtCoder contests for evaluation purposes. The results highlight that the capability varies across natural languages, with Chinese achieving the worst performance. Furthermore, regardless of the type of natural language, the performance decreases significantly as the difficulty of questions increases. Our work represents the initial step in comprehending the significance of natural languages in Copilot's capability and introduces promising opportunities for future endeavors."
382,2023,"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing [arXiv] nan Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications."
383,2024,"Dynamic Knowledge Injection for AIXI Agents nan Prior approximations of AIXI, a Bayesian optimality notion for general reinforcement learning, can only approximate AIXI's Bayesian environment model using an a-priori defined set of models. This is a fundamental source of epistemic uncertainty for the agent in settings where the existence of systematic bias in the predefined model class cannot be resolved by simply collecting more data from the environment. We address this issue in the context of Human-AI teaming by considering a setup where additional knowledge for the agent in the form of new candidate models arrives from a human operator in an online fashion. We introduce a new agent called DynamicHedgeAIXI that maintains an exact Bayesian mixture over dynamically changing sets of models via a time-adaptive prior constructed from a variant of the Hedge algorithm. The DynamicHedgeAIXI agent is the richest direct approximation of AIXI known to date and comes with good performance guarantees. Experimental results on epidemic control on contact networks validates the agent's practical utility."
386,2024,"MEDCO: Medical Education Copilots Based on A Multi-Agent Framework nan Large language models (LLMs) have had a significant impact on diverse research domains, including medicine and healthcare. However, the potential of LLMs as copilots in medical education remains underexplored. Current AI-assisted educational tools are limited by their solitary learning approach and inability to simulate the multi-disciplinary and interactive nature of actual medical training. To address these limitations, we propose MEDCO (Medical EDucation COpilots), a novel multi-agent-based copilot system specially developed to emulate real-world medical training environments. MEDCO incorporates three primary agents: an agentic patient, an expert doctor, and a radiologist, facilitating a multi-modal and interactive learning environment. Our framework emphasizes the learning of proficient question-asking skills, multi-disciplinary collaboration, and peer discussions between students. Our experiments show that simulated virtual students who underwent training with MEDCO not only achieved substantial performance enhancements comparable to those of advanced models, but also demonstrated human-like learning behaviors and improvements, coupled with an increase in the number of learning samples. This work contributes to medical education by introducing a copilot that implements an interactive and collaborative learning approach. It also provides valuable insights into the effectiveness of AI-integrated training paradigms."
387,2024,"Generative AI for Pull Request Descriptions: Adoption, Impact, and Developer Interventions [arXiv] nan GitHub's Copilot for Pull Requests (PRs) is a promising service aiming to automate various developer tasks related to PRs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. As this innovative technology gains traction in the Open Source Software (OSS) community, it is crucial to examine its early adoption and its impact on the development process. Additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. In our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 PRs in which parts of the descriptions were crafted by generative AI. Our findings indicate that: (1) Copilot for PRs, though in its infancy, is seeing a marked uptick in adoption. (2) PRs enhanced by Copilot for PRs require less review time and have a higher likelihood of being merged. (3) Developers using Copilot for PRs often complement the automated descriptions with their manual input. These results offer valuable insights into the growing integration of generative AI in software development."
388,2022,"Taking Flight with Copilot: Early insights and opportunities of AI-powered pair-programming tools nan Over the next five years, AI-powered tools likely will be helping developers in many diverse tasks. For example, such models may be used to improve code review, directing reviewers to parts of a change where review is most needed or even directly providing feedback on changes. Models such as Codex may suggest fixes for defects in code, build failures, or failing tests. These models are able to write tests automatically, helping to improve code quality and downstream reliability of distributed systems. This study of Copilot shows that developers spend more time reviewing code than actually writing code. As AI-powered tools are integrated into more software development tasks, developer roles will shift so that more time is spent assessing suggestions related to the task than doing the task itself."
389,2024,"Industrial Experience Report on AI-Assisted Coding in Professional Software Development nan AI-based tools for software development are widely discussed in academic literature. They promise to boost software development performance, especially in code creation. This paper collects insights from practitioners about the use and implications of AI assistance in industrial software development, with a focus on SMEs. Through interviews with five developers from three software development organization, we gathered and analyzed the experiences made in industrial practice, and we identified lessons learned and open challenges. ChatGPT and Copilot are used in industry projects. While they are considered useful for many code-related development activities, their integration in the development workflow remains mostly shallow. Contradicting observations about speed-ups due to AI support in development are reported. Legal issues are of minor concern although awareness exists."
390,2018,"Principles for Minimizing Cognitive Assistance Distraction in the Cockpit nan Over the last three years, the MITRE Corporation has been developing a cognitive assistant concept for pilots called Digital Copilot. Digital Copilot reduces pilot workload and increases safety by offloading pilot tasks, increasing task efficiency, and inferring pilot intent to provide the right information at the right time. In this paper, we will introduce the Digital Copilot concept and review three design principals for the cockpit cognitive assistance gleaned from the design process, flight testing, and simulator studies of Digital Copilot with General Aviation pilots."
391,2019,Error Discovery through Human-AI Collaboration nan nan
392,2024,"Harnessing the Potential of Gen-AI Coding Assistants in Public Sector Software Development nan The study on GitHub Copilot by GovTech Singapore's Engineering Productivity Programme (EPP) reveals significant potential for AI Code Assistant tools to boost developer productivity and improve application quality in the public sector. Highlighting the substantial benefits for the public sector, the study observed an increased productivity (coding / tasks speed increased by 21-28%), which translates into accelerated development, and quicker go-to-market, with a notable consensus (95%) that the tool increases developer satisfaction. Particularly, junior developers experienced considerable efficiency gains and reduced coding times, illustrating Copilot's capability to enhance job satisfaction by easing routine tasks. This advancement allows for a sharper focus on complex projects, faster learning, and improved code quality. Recognising the strategic importance of these tools, the study recommends the development of an AI Framework to maximise such benefits while cautioning against potential over-reliance without solid foundational programming skills. It also advises public sector developers to classify their code as Open to use Gen-AI Coding Assistant tools on the Cloud like GitHub Copilot and to consider self-hosted tools like Codeium or Code Llama for confidential code to leverage technology efficiently within the public sector framework. With up to 8,000 developers, comprising both public officers and vendors developing applications for the public sector and its customers, there is significant potential to enhance productivity."
393,2022,Artificial Intelligence in Organizations: Three Experiments on Human/Machine Interaction and Human Augmentation nan nan
394,2024,"Balancing Innovation and Ethics in AI-Driven Software Development nan This paper critically examines the ethical implications of integrating AI tools like GitHub Copilot and ChatGPT into the software development process. It explores issues such as code ownership, bias, accountability, privacy, and the potential impact on the job market. While these AI tools offer significant benefits in terms of productivity and efficiency, they also introduce complex ethical challenges. The paper argues that addressing these challenges is essential to ensuring that AI's integration into software development is both responsible and beneficial to society."
395,2023,"GitHub Copilot AI pair programmer: Asset or Liability? nan Automatic program synthesis is a long-lasting dream in software engineering. Recently, a promising Deep Learning (DL) based solution, called Copilot, has been proposed by OpenAI and Microsoft as an industrial product. Although some studies evaluate the correctness of Copilot solutions and report its issues, more empirical evaluations are necessary to understand how developers can benefit from it effectively. In this paper, we study the capabilities of Copilot in two different programming tasks: (i) generating (and reproducing) correct and efficient solutions for fundamental algorithmic problems, and (ii) comparing Copilot's proposed solutions with those of human programmers on a set of programming tasks. For the former, we assess the performance and functionality of Copilot in solving selected fundamental problems in computer science, like sorting and implementing data structures. In the latter, a dataset of programming problems with human-provided solutions is used. The results show that Copilot is capable of providing solutions for almost all fundamental algorithmic problems, however, some solutions are buggy and non-reproducible. Moreover, Copilot has some difficulties in combining multiple methods to generate a solution. Comparing Copilot to humans, our results show that the correct ratio of humans' solutions is greater than Copilot's suggestions, while the buggy solutions generated by Copilot require less effort to be repaired. Based on our findings, if Copilot is used by expert developers in software projects, it can become an asset since its suggestions could be comparable to humans' contributions in terms of quality. However, Copilot can become a liability if it is used by novice developers who may fail to filter its buggy or non-optimal solutions due to a lack of expertise. (c) 2023 Elsevier Inc. All rights reserved."
396,2023,"From Ban It TillWe Understand It to Resistance is Futile: How University Programming Instructors Plan to Adapt as More Students Use AI Code Generation and Explanation Tools such as ChatGPT and GitHub Copilot nan Over the past year (2022-2023), recently-released AI tools such as ChatGPT and GitHub Copilot have gained significant attention from computing educators. Both researchers and practitioners have discovered that these tools can generate correct solutions to a variety of introductory programming assignments and accurately explain the contents of code. Given their current capabilities and likely advances in the coming years, how do university instructors plan to adapt their courses to ensure that students still learn well? To gather a diverse sample of perspectives, we interviewed 20 introductory programming instructors (9 women + 11 men) across 9 countries (Australia, Botswana, Canada, Chile, China, Rwanda, Spain, Switzerland, United States) spanning all 6 populated continents. To our knowledge, this is the first empirical study to gather instructor perspectives about how they plan to adapt to these AI coding tools that more students will likely have access to in the future. We found that, in the short-term, many planned to take immediate measures to discourage AI-assisted cheating. Then opinions diverged about how to work with AI coding tools longer-term, with one side wanting to ban them and continue teaching programming fundamentals, and the other side wanting to integrate them into courses to prepare students for future jobs. Our study findings capture a rare snapshot in time in early 2023 as computing instructors are just starting to form opinions about this fast-growing phenomenon but have not yet converged to any consensus about best practices. Using these findings as inspiration, we synthesized a diverse set of open research questions regarding how to develop, deploy, and evaluate AI coding tools for computing education."
397,2024,"The Purposeful Presentation of AI Teammates: Impacts on Human Acceptance and Perception nan The paper reports on two empirical studies that provide the first examination into how the presentation of an AI teammate's identity, responsibility, and capability impacts humans' perception surrounding AI teammate adoption before interacting as teammates. Study 1's results indicated that AI teammates are accepted when they share equal responsibility on a task with humans, but other perceptions such as job security generally decline the more responsibility AI teammates have. Study 1 also revealed that identifying an AI as a tool instead of a teammate can have small benefits to human perceptions of job security and adoption. Study 2 revealed that the negative impacts of increasing responsibility can be mitigated by presenting AI teammates' capabilities as being endorsed by coworkers and one's own past experience. This paper discusses how to use these results to best balance the presentation of AI teammates' capabilities and responsibilities, as well as identifying AI as teammates."
398,2024,"Responses of Five Different Artificial Intelligence Chatbots to the Top Searched Queries About Erectile Dysfunction: A Comparative Analysis nan The aim of the study is to evaluate and compare the quality and readability of responses generated by five different artificial intelligence (AI) chatbots-ChatGPT, Bard, Bing, Ernie, and Copilot-to the top searched queries of erectile dysfunction (ED). Google Trends was used to identify ED-related relevant phrases. Each AI chatbot received a specific sequence of 25 frequently searched terms as input. Responses were evaluated using DISCERN, Ensuring Quality Information for Patients (EQIP), and Flesch-Kincaid Grade Level (FKGL) and Reading Ease (FKRE) metrics. The top three most frequently searched phrases were erectile dysfunction cause, how to erectile dysfunction, and erectile dysfunction treatment. Zimbabwe, Zambia, and Ghana exhibited the highest level of interest in ED. None of the AI chatbots achieved the necessary degree of readability. However, Bard exhibited significantly higher FKRE and FKGL ratings (p = 0.001), and Copilot achieved better EQIP and DISCERN ratings than the other chatbots (p = 0.001). Bard exhibited the simplest linguistic framework and posed the least challenge in terms of readability and comprehension, and Copilot's text quality on ED was superior to the other chatbots. As new chatbots are introduced, their understandability and text quality increase, providing better guidance to patients."
399,2024,"Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code [arXiv] nan As one of the most popular dynamic languages, Python experiences a decrease in readability and maintainability when code smells are present. Recent advancements in Large Language Models have sparked growing interest in AI-enabled tools for both code generation and refactoring. GitHub Copilot is one such tool that has gained widespread usage. Copilot Chat, released on September 2023, functions as an interactive tool aims at facilitating natural language-powered coding. However, limited attention has been given to understanding code smells in Copilot-generated Python code and Copilot's ability to fix the code smells it generates. To this end, we built a dataset comprising 102 code smells in Copilot-generated Python code. Our aim is to first explore the occurrence of code smells in Copilot-generated Python code and then evaluate the effectiveness of Copilot in fixing these code smells employing different prompts. The results show that 8 out of 10 types of Python smells can be detected in Copilot-generated Python code, among which Multiply-Nested Container is the most common one. For these code smells, Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing Python code smells generated by Copilot itself. Besides, the effectiveness of Copilot Chat in fixing these smells can be improved with the provision of more detailed prompts. However, using Copilot Chat to fix these smells might introduce new code smells."
400,2024,"Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects nan Generative AI technologies promise to transform the product development lifecycle. This study evaluates the efficiency gains, areas for improvement, and emerging challenges of using GitHub Copilot, an AI-powered coding assistant. We identified 15 software development tasks and assessed Copilot's benefits through real-world projects on large proprietary code bases. Our findings indicate significant reductions in developer toil, with up to 50% time saved in code documentation and autocompletion, and 30-40% in repetitive coding tasks, unit test generation, debugging, and pair programming. However, Copilot struggles with complex tasks, large functions, multiple files, and proprietary contexts, particularly with C/C++ code. We project a 33-36% time reduction for coding-related tasks in a cloud-first software development lifecycle. This study aims to quantify productivity improvements, identify underperforming scenarios, examine practical benefits and challenges, investigate performance variations across programming languages, and discuss emerging issues related to code quality, security, and developer experience."
401,2024,"Decoding medical jargon: The use of AI language models (ChatGPT-4, BARD, microsoft copilot) in radiology reports nan Objective: Evaluate Artificial Intelligence (AI) language models (ChatGPT-4, BARD, Microsoft Copilot) in simplifying radiology reports, assessing readability, understandability, actionability, and urgency classification. Methods: This study evaluated the effectiveness of these AI models in translating radiology reports into patient-friendly language and providing understandable and actionable suggestions and urgency classifications. Thirty radiology reports were processed using AI tools, and their outputs were assessed for readability (Flesch Reading Ease, Flesch-Kincaid Grade Level), understandability (PEMAT), and the accuracy of urgency classification. ANOVA and Chi -Square tests were performed to compare the models' performances. Results: All three AI models successfully transformed medical jargon into more accessible language, with BARD showing superior readability scores. In terms of understandability, all models achieved scores above 70 %, with ChatGPT-4 and BARD leading (p < 0.001, both). However, the AI models varied in accuracy of urgency recommendations, with no significant statistical difference (p = 0.284). Conclusion: AI language models have proven effective in simplifying radiology reports, thereby potentially improving patient comprehension and engagement in their health decisions. However, their accuracy in assessing the urgency of medical conditions based on radiology reports suggests a need for further refinement. Practice implications: Incorporating AI in radiology communication can empower patients, but further development is crucial for comprehensive and actionable patient support."
402,2024,"Assessing AI-Based Code Assistants in Method Generation Tasks nan AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality. This study compares four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method generation tasks, assessing their ability to produce accurate, correct, and efficient code. Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code."
403,1989,"The Electronic Copilot: towards airborne AI pilot aids nan The Electronic Copilot is an ambitious project, initiated by the Dret and AMD-BA, which aims to use artificial intelligence techniques to create an embedded system for decision aid in a single-seated fighter aircraft. The objective is to provide the pilot with relevant information to assist decision making and thus enhance the global effectiveness of the mission. The constraints due to the use of artificial intelligence in this context create a lot of theoretical and practical problems. The authors present the ways and means which they have used to tackle these difficulties."
404,2024,"Exploring AI Integration in Software Development: Case Studies and Insights nan The integration of Artificial Intelligence in software development has emerged as a promising solution to address the rising costs and scarcity of skilled developers. In this paper, we present a comprehensive evaluation of the impact of AI on software development. Specifically, we examine several AI tools, including GitHub Copilot, Vercel's v0, and Meta AI, through detailed case studies to demonstrate their effectiveness in automating tasks and reducing human resource dependency. Our analysis highlights the significant benefits of AI-driven tools in enhancing developer productivity by automating repetitive and mundane tasks, thereby allowing developers to focus on more complex and creative aspects of software development. Additionally, we discuss the challenges and limitations of integrating AI into the software development lifecycle, focusing on issues such as data quality and model interpretability. Overall, this paper aims to provide practical insights into the real-world applications and implications of AI tools in software development."
405,2024,"Quality of information about urologic pathology in English and Spanish from ChatGPT, BARD, and Copilot nan Introduction and objective: Generative artificial intelligence makes it possible to ask about medical pathologies in dialog boxes. Our objective was to analyze the quality of information about the most common urological pathologies provided by ChatGPT (OpenIA), BARD (Google), Methods: We analyzed information on the following pathologies and their treatments as provided by AI: prostate cancer, kidney cancer, bladder cancer, urinary lithiasis, and benign prostatic hypertrophy (BPH). Questions in English and Spanish were posed in dialog boxes; the answers were collected and analyzed with DISCERN questionnaires and the overall appropriateness of the response. Surgical procedures were performed with an informed consent questionnaire. Results: The responses from the three chatbots explained the pathology, detailed risk factors, and described treatments. The difference is that BARD and Copilot provide external information citations, which ChatGPT does not. The highest DISCERN scores, in absolute numbers, were obtained in Copilot; however, on the appropriacy scale it was noted that their responses were not the most appropriate. The best surgical treatment scores were obtained by BARD, followed Conclusions: The answers obtained from generative AI on urological diseases depended on the formulation of the question. The information provided had significant biases, depending on (c) 2024 AEU. Published by Elsevier Espa & ntilde;a, S.L.U. All rights reserved."
406,2022,"Cognitive Modeling of Anticipation: Unsupervised Learning and Symbolic Modeling of Pilots' Mental Representations. nan The ability to anticipate team members' actions enables joint action towards a common goal. Task knowledge and mental simulation allow for anticipating other agents' actions and for making inferences about their underlying mental representations. In human-AI teams, providing AI agents with anticipatory mechanisms can facilitate collaboration and successful execution of joint action. This paper presents a computational cognitive model demonstrating mental simulation of operators' mental models of a situation and anticipation of their behavior. The work proposes two successive steps: (1) A hierarchical cluster algorithm is applied to recognize patterns of behavior among pilots. These behavioral clusters are used to derive commonalities in situation models from empirical data (N = 13 pilots). (2) An ACT-R (adaptive control of thought - rational) cognitive model is implemented to mentally simulate different possible outcomes of action decisions and timing of a pilot. model tracing of ACT-R allows following up on operators' individual actions. Two models are implemented using the symbolic representations of ACT-R: one simulating normative behavior and the other by simulating individual differences and using subsymbolic learning. Model performance is analyzed by a comparison of both models. Results indicate the improved performance of the individual differences over the normative model and are discussed regarding implications for cognitive assistance capable of anticipating operatorbehavior."
407,2022,"An evaluation to determine if reading the mind in the eyes scores can be improved through training nan The Reading the Mind in the Eyes Test (RMET) has received attention due to its correlation with collective intelligence. If the RMET is a marker of collective intelligence, training to improve RMET could result in better teamwork, whether for human-human or human-AI (artificial intelligence) in composition. While training on related skills has proven effective in the literature, RMET training has not been studied. This research evaluates the development of RMET training, testing the impact of two training conditions (Naturalistic Training and Repeated RMET Practice) compared to a control. There were no significant differences in RMET scores due to training, but speed of response was positively correlated to RMET score for high-scoring participants. Both management professionals and AI creators looking to cultivate team skill through the application of the RMET may need to reconsider their tool selection."
408,2024,"The Recent Trends of Research on GitHub Copilot: A Systematic Review nan GitHub Copilot is an AI-powered code generation tool developed by OpenAI and GitHub that has gained significant attention from the software engineering community. Despite the significant attention received from the software engineering community, there is a lack of comprehensive examination of its effectiveness, reliability, and ethical implications on GitHub Copilot. The absence of a systematic review of GitHub Copilot's recent research trends hinders a thorough understanding of its current state of development and potential impact on software development practices. Therefore, this systematic review aims to analyze the recent trends of research on GitHub Copilot to assess its current state of development, identify gaps in existing knowledge, and provide insights into potential future research directions. This study used PRISMA for searching the relevant databases, screening and selecting eligible studies based on inclusion and exclusion criteria, data extraction and synthesis, and critical appraisal of the quality and relevance of the studies included. The results show that the trend of the studies is focusing on four main areas: developer productivity, code quality, code security and education."
409,2023,"Using GitHub Copilot to Solve Simple Programming Problems nan The teaching and assessment of introductory programming involves writing code that solves a problem described by text. Previous research found that OpenAI's Codex, a natural language machine learning model trained on billions of lines of code, performs well on many programming problems, often generating correct and readable Python code. GitHub's version of Codex, Copilot, is freely available to students. This raises pedagogic and academic integrity concerns. Educators need to know what Copilot is capable of, in order to adapt their teaching to AI-powered programming assistants. Previous research evaluated the most performant Codex model quantitatively, e.g. how many problems have at least one correct suggestion that passes all tests. Here I evaluate Copilot instead, to see if and how it differs from Codex, and look qualitatively at the generated suggestions, to understand the limitations of Copilot. I also report on the experience of using Copilot for other activities asked of students in programming courses: explaining code, generating tests and fixing bugs. The paper concludes with a discussion of the implications of the observed capabilities for the teaching of programming."
410,2023,"Demystifying Practices, Challenges and Expected Features of Using GitHub Copilot [arXiv] nan With the advances in machine learning, there is a growing interest in AI-enabled tools for autocompleting source code. GitHub Copilot has been trained on billions of lines of open source GitHub code, and is one of such tools that has been increasingly used since its launch in June 2021. However, little effort has been devoted to understanding the practices, challenges, and expected features of using Copilot in programming for auto-completed source code from the point of view of practitioners. To this end, we conducted an empirical study by collecting and analyzing the data from Stack Overflow (SO) and GitHub Discussions. We searched and manually collected 303 SO posts and 927 GitHub discussions related to the usage of Copilot. We identified the programming languages, Integrated Development Environments (IDEs), technologies used with Copilot, functions implemented, benefits, limitations, and challenges when using Copilot. The results show that when practitioners use Copilot: (1) The major programming languages used with Copilot are JavaScript and Python, (2) the main IDE used with Copilot is Visual Studio Code, (3) the most common used technology with Copilot is Node.js, (4) the leading function implemented by Copilot is data processing, (5) the main purpose of users using Copilot is to help generate code, (6) the significant benefit of using Copilot is useful code generation, (7) the main limitation encountered by practitioners when using Copilot is difficulty of integration, and (8) the most common expected feature is that Copilot can be integrated with more IDEs. Our results suggest that using Copilot is like a double-edged sword, which requires developers to carefully consider various aspects when deciding whether or not to use it. Our study provides empirically grounded foundations that could inform developers and practitioners, as well as provide a basis for future investigations."
411,2024,"Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot nan GitHub Copilot is an AI-enabled tool that automates program synthesis. It has gained significant attention since its launch in 2021. Recent studies have extensively examined Copilot's capabilities in various programming tasks, as well as its security issues. However, little is known about the effect of different natural languages on code suggestion. Natural language is considered a social bias in the field of NLP, and this bias could impact the diversity of software engineering. To address this gap, we conducted an empirical study to investigate the effect of three popular natural languages (English, Japanese, and Chinese) on Copilot. We used 756 questions of varying difficulty levels from AtCoder contests for evaluation purposes. The results highlight that the capability varies across natural languages, with Chinese achieving the worst performance. Furthermore, regardless of the type of natural language, the performance decreases significantly as the difficulty of questions increases. Our work represents the initial step in comprehending the significance of natural languages in Copilot's capability and introduces promising opportunities for future endeavors."
412,2024,"Why Would You Suggest That? Human Trust in Language Model Responses nan The emergence of Large Language Models (LLMs) has revealed a growing need for human-AI collaboration, especially in creative decision-making scenarios where trust and reliance are paramount. Through human studies and model evaluations on the open-ended News Headline Generation task from the LaMP benchmark, we analyze how the framing and presence of explanations affect user trust and model performance. Overall, we provide evidence that adding an explanation in the model response to justify its reasoning significantly increases self-reported user trust in the model when the user has the opportunity to compare various responses. Position and faithfulness of these explanations are also important factors. However, these gains disappear when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, equitably when they are shown in isolation. Our findings urge future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems."
413,2023,"Taking Flight with Copilot nan IN PAIR PROGRAMMING, two developers write code together. One takes the role of driver, writing the code needed for the task at hand, while the other assumes the role of navigator, directing the driver's work and reviewing the code. This allows the driver to focus on detailed coding-syntax and structure-while letting the navigator direct the flow of the work and review the code in real time. Proponents of pair programming say it improves code quality and readability and can"
414,2023,"Live Exploration of AI-Generated Programs [arXiv] nan AI-powered programming assistants are increasingly gaining popularity, with GitHub Copilot alone used by over a million developers worldwide. These tools are far from perfect, however, producing code suggestions that may be incorrect or incomplete in subtle ways. As a result, developers face a new set of challenges when they need to understand, validate, and choose between AI's suggestions. This paper explores whether Live Programming, a continuous display of a program's runtime values, can help address these challenges. We introduce Live Exploration of AI-Generated Programs, a new interaction model for AI programming assistants that supports exploring multiple code suggestions through Live Programming. We implement this interaction model in a prototype Python environment LEAP and evaluate it through a between-subject study. Our results motivate several design opportunities for future AI-powered programming tools."
415,2024,Generative AI Copilot to Support Safety Analyses of Human-Robot Collaborations: Hazard Operability Analysis and GPT-4 nan nan
416,2024,"Generating Java Methods: An Empirical Assessment of Four AI-Based Code Assistants nan AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers.In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes."
417,2023,"Human-Ai Collaborative Approaches to Supporting Multi-Stage, Stochastic Multi-Criteria Decision Making nan nan"
418,2024,"A common language nan Siemens showcased a production machine that is augmented with its Industrial Copilot for the first time as part of a collaboration with motion technology company Schaeffler. The Siemens Industrial Copilot helps automate repetitive tasks, freeing up engineering resources for other work. It is also expected to assist less- experienced shop floor employees to grow into engineering roles. To support engineers with various automation tasks, the artificial intelligence (AI) powered assistant is connected to Siemens's engineering framework Totally Integrated Automation (TIA) Portal via the open application programming interface TIA Portal Openness."
419,2020,"Human Digital Twin: Enabling Human-Multi Smart Machines Collaboration nan AI solutions are becoming ubiquitous in private and professional Domains. Soon, humans using AI will be relying on recommendations and actions from multiple smart machines to coordinate and manage their financial, professional, or health objectives. As the human's life aspects and objectives are dependent and as her resources are limited, the paper argues that to avoid conflicts, all smart machines supporting a human need to be adaptively aligned with her objectives. The paper introduces the concept of a Human Digital Twin (HDT), which is a human-specific smart machine dedicated to aligning human objectives with the smart machines supporting her. The HDT monitors the entire human-AI space and, based on the human's responses to the various machine actions, the HDT identifies stable human-machine interaction patterns, which can be used to anticipate human responses in given contexts and thus ensure the alignment of the various machines with her objectives. The HDT learns the human-machine interaction patterns by using the structure of the information used in the interactions (metadata), and not by relying on the content of that information. This enables HDTs to overcome the constraint of existing approaches, which require all smart solutions to operate on the same platform. In addition, by relying only on information structures, HDTs would not compromise human data or proprietary provider information."
420,2023,"Generative AI for Software Practitioners nan Generative artificial intelligence (AI) tools, such as Bard, ChatGPT, and CoPilot, have rapidly gained widespread usage. They also have the potential to boost software engineering productivity. In this article, we elaborate technologies and usage of generative AI in the software industry. We address questions, such as: How does generative AI improve software productivity? How to connect generative AI to software development, and what are the risks? Which technologies have what sorts of benefits? Practitioner guidance and case studies are shared from our industry context. I look forward to hearing from you about this column and the technologies that matter most for your work.-Christof Ebert"
421,2023,"Tasks, Copilots, and the Future of Search nan Tasks are central to information retrieval (IR) and drive interactions with search systems [2, 4, 10]. Understanding and modeling tasks helps these systems better support user needs [8, 9, 11]. This keynote focuses on search tasks, the emergence of generative artificial intelligence (AI), and the implications of recent work at their intersection for the future of search. Recent estimates suggest that half of Web search queries go unanswered, many of them connected to complex search tasks(1) that are ill-defined or multi-step and span several queries [6]. AI copilots, e.g., ChatGPT and Bing Chat, are emerging to address complex search tasks and many other challenges. These copilots are built on large foundation models such as GPT-4 and are being extended with skills and plugins. Copilots broaden the surface of tasks achievable via search, moving toward creation not just finding (e.g., interview preparation, email composition), and can make searchers more efficient and more successful.Users currently engage with AI copilots via natural language queries and dialog and the copilots generate answers with source attribution [7]. However, in delegating responsibility for answer generation, searchers also lose some control over aspects of the search process, such as directly manipulating queries and examining lists of search results [1]. The efficiency gains from auto-generating a single, synthesized answer may also reduce opportunities for user learning and serendipity. A wholesale move to copilots for all search tasks is neither practical nor necessary: model inference is expensive, conversational interfaces are unfamiliar to many users in a search context, and traditional search already excels for many types of task. Instead, experiences that unite search and chat are becoming more common, enabling users to adjust the modality and other aspects (e.g., answer tone) based on the task.The rise of AI copilots creates many opportunities for IR, including aligning generated answers with user intent, tasks, and applications via human feedback [3]; understanding copilot usage, including functional fixedness [5]; using context and data to tailor responses to people and situations (e.g., grounding, personalization); new search experiences (e.g., unifying search and chat); reliability and safety (e.g., accuracy, bias); understanding impacts on user learning and agency; and evaluation (e.g., model-based feedback, searcher simulations [12], repeatability). Research in these and related areas will enable search systems to more effectively utilize new copilot technologies together with traditional search to help searchers better tackle a wider variety of tasks."
422,2024,"The Disruptive Technologies of Search: Copilots, Assistants, Conversations, and Chats nan The major Web search engines, primarily represented by Google and Bing, are busily releasing new versions of search that incorporate GenAI. Newer websearch engines, particularly You.com, rely heavily on AI for their search results. AI-driven search also manifests in search engines geared toward the academic environment. In this category are Elicit and Consensus, among others. Change is a foot in the subscription databases librarians have long relied upon, with companies such as Clarivate, Elsevier,and Dimensions jumping into the AI search space."
423,2023,"PwnPilot: Reflections on Trusting Trust in the Age of Large Language Models and AI Code Assistants nan At the dawn of a new era in software engineering, one defined by large language models (LLMs) and AI code assistants like GitHub Copilot, new meaning can be found from a historic Turing Award Lecture that concluded one cannot trust source code they did not totally create themselves. In this paper, a targeted, systematic survey of the latest research results from 2019 to early 2023 highlights the possible risks of using AI code assistants that produce substantial source code contributions, and the potential for an AI Copilot to unknowingly become PwnPilot, a malevolent digital actor that introduces vulnerabilities and compromises trust. During a period of explosive growth for generative AI, renewed reflections on trusting trust point to conclusions similar to the original assertions of Ken Thompson in 1984. But despite a recent theoretical roadblock from proof of ability to plant undetectable backdoors in machine learning models, the potential for enhanced productivity from AI code assistants may still be realizable with an acceptable level of risk, perhaps even for safety critical and sensitive security relevant contexts. In support of that goal, a number of near-term risk management options and longer term research paths are identified as enablers for practitioners and inputs to potential research roadmaps toward more secure and trusted AI code generation."
424,2023,"Search Evolves Into Copilots, Chatbots, and Research Assistants nan Since its arrival last November, ChatGPT has monopolized the attention, not just of librarians, but also, seemingly, of every conscious person on the planet. Distinguishing between hype and reality is an ongoing adventure, as is the distinction between AI-generated content and search capabilities. At the 2023 ALA annual conference, librarians attending a huddle on generative AI, organized by Melissa Del Castillo, Florida International University, were mainly concerned about hallucinations. They told tales of time wasted tracking down spurious citations and freaked-out faculty worried about student use of ChatGPT for course assignments."
425,2024,"Comparative Analysis of Generative AI Tools in Enhancing Educational Engagement nan Technological development is continuously reshaping the traditional educational setting, presenting various possibilities for educational refinement. It is pivotal to motivate active engagement among all participants, and generative AI tools emerge as possible enablers in this pursuit. This paper presents survey insights from 246 students and 105 educators, collectively affirming the significance and potential benefits of AI integration within education. Within this study, 10 evaluative criteria have been established and applied to compare four publicly available generative AI tools ChatGPT 3.5 and 4, Google Bard and Microsoft Copilot, including data confidentiality and various recommendations on what, how, and when should certain AI tools be used. The findings spotlight educators' current preference for ChatGPT among the surveyed tools, however, they are also open to diverse AI tool adoption."
426,2023,"AI-Copilot for Business Optimisation: A Framework and A Case Study in Production Scheduling [arXiv] nan Business optimisation is the process of finding and implementing efficient and cost-effective means of operation to bring a competitive advantage for businesses. Synthesizing problem formulations is an integral part of business optimisation which is centred around human expertise, thus with a high potential of becoming a bottleneck. With the recent advancements in Large Language Models (LLMs), human expertise needed in problem formulation can potentially be minimized using Artificial Intelligence (AI). However, developing a LLM for problem formulation is challenging, due to training data requirements, token limitations, and the lack of appropriate performance metrics in LLMs. To minimize the requirement of large training data, considerable attention has recently been directed towards fine-tuning pre-trained LLMs for downstream tasks, rather than training a LLM from scratch for a specific task. In this paper, we adopt this approach and propose an AI-Copilot for business optimisation by fine-tuning a pre-trained LLM for problem formulation. To address token limitations, we introduce modularization and prompt engineering techniques to synthesize complex problem formulations as modules that fit into the token limits of LLMs. In addition, we design performance evaluation metrics that are more suitable for assessing the accuracy and quality of problem formulations compared to existing evaluation metrics. Experiment results demonstrate that our AI-Copilot can synthesize complex and large problem formulations for a typical business optimisation problem in production scheduling."
429,2020,"AI Song Contest: Human-AI Co-Creation in Songwriting [arXiv] nan Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation, or algorithmically ranked the samples. Ultimately, teams not only had to manage the flare and focus aspects of the creative process, but also juggle them with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression. [ISMIR 2020]."
430,2024,"Getting Back Together: HCI and Human Factors Joining Forces to Meet the AI Interaction Challenge nan The Human Factors Society and ACM SIGCHI jointly organized the first CHI conference in 1983, but during the remainder of the 1980s, Human-Computer Interaction (HCI) and Human Factors Engineering (HFE) increasingly diverged. The focus of HCI shifted from exploring systems for routinized activities of trained personnel, to a more general use of technology. HCI became predominantly design-oriented, concentrating on usability and user experience, moving further from HFE principles. However, the rapid growth of Artificial Intelligence (AI) applications posed unique and urgent challenges that call for a reestablishment of the connection between the two disciplines. We argue that by working as a team, HCI and HFE could more effectively address AI-posed challenges. We invite HCI and HFE researchers to take part in a full-day interactive hybrid workshop. With this workshop, we aim to initiate a collaboration between HCI and HFE and set a clear plan forward for a more united future."
431,2024,"Can Natural Language Processing (NLP) Provide Consultancy to Patients About Edentulism Teeth Treatment? nan Aim This study aimed to evaluate the accuracy and quality of the answers given by artificial intelligence (AI) applications to the questions directed at tooth deficiency treatments. Materials and methods Fifteenquestions asked by patients/ordinary people about missing tooth treatment were selected from the Quora platform. Questions were asked to the ChatGPT-4 (OpenAI Inc., San Francisco, California, United States) and Copilot (Microsoft Corporation, Redmond, Washington, United States) models. Responses were assessed by two expert physicians using a five-point Likert scale (LS) for accuracy and the Global Quality Scale (GQS) for quality. To assess the internal consistency and inter-rater agreement of ChatGPT-4 and Copilot, Cronbach's alpha, Spearman-Brown's coefficient, and Guttman's split-half coefficient were calculated to measure the reliability and internal consistency of both instruments (alpha=0.05). Results Copilot showed a mean LS value of 3.830.36 and ChatGPT-4 showed a lower mean value of 3.930.32. ChatGPT-4's GQS mean value (3.90.28) is also higher than Copilot (3.830.06) (p<0.001). Conclusion It can be said that AI chatbots gave highly accurate and consistent answers to questions about the treatment of toothlessness. With the ever-developing technology, AI chatbots can be used as consultants for dental treatments in the future."
432,2022,"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study nan This empirical study investigates the effectiveness of pair programming with GitHub Copilot in comparison to human pair-programming. Through an experiment with 21 participants we focus on code productivity and code quality. For experimental design, a participant was given a project to code, under three conditions presented in a randomized order. The conditions are pair-programming with Copilot, human pair-programming as a driver, and as a navigator. The codes generated from the three trials were analyzed to determine how many lines of code on average were added in each condition and how many lines of code on average were removed in the subsequent stage. The former measures the productivity of each condition while the latter measures the quality of the produced code. The results suggest that although Copilot increases productivity as measured by lines of code added, the quality of code produced is inferior by having more lines of code deleted in the subsequent trial."
434,2024,"A Comparison of the Effectiveness of ChatGPT and Co-Pilot for Generating Quality Python Code Solutions nan Artificial intelligence (AI) has become increasingly popular in software development to automate tasks and improve efficiency. AI has the potential to help while developing or maintaining software, in the sense that it can produce solutions out of a textual requirement specification, and understand code to provide suggestion on how a new requirement could be implemented. In this paper, we focus on the first scenario. Two AI-powered tools that have the potential to revolutionize the way software is developed are OpenAI's ChatGPT and GitHub's Copilot. In this paper, we used LeetCode, a popular platform for technical interview preparation and personal upskilling (self-learning), to evaluate the effectiveness of ChatGPT and Copilot on a set of coding problems, along with ChatGPT's ability to correct itself when provided with feedback. The analysis of the effectiveness can lead to various conclusions, such as on if these solutions are ready to take over coding roles, and to what extent several parameters (difficulty and quality requirements) influence this result. Solutions have been generated for 60 problems using ChatGPT and Copilot, for the Python programming language. We investigated the performance of the models, the recurrent kinds of errors, and the resulting code quality. The evaluation revealed that ChatGPT and Copilot can be effective tools for generating code solutions for easy problems while both models are prone to syntax and semantic errors. Small improvements are observed for ode quality metrics across iterations, although the improvement pattern is not consistently monotonic, questioning ChatGPT's awareness of the quality of its own solutions. Nevertheless, the improvement that was found along iterations, highlights the potential of AI and humans, acting as partners, in providing the optimal combination. The two models demonstrate a limited capacity for understanding context. Although AI-powered coding tools driven by large language models have the potential to assist developers in their coding tasks, they should be used with caution and in conjunction with human coding expertise. Developer intervention is necessary not only to debug errors but also to ensure high-quality and optimized code."
435,2024,"Collaborative Intelligence: A Scoping Review Of Current Applications nan This review provides a novel examination of the emerging field of collaborative intelligence and demonstrates the value that human-AI teams can deliver. Humans and artificial intelligence (AI) systems have complementary strengths. This complementarity creates the potential to achieve a step-change in performance by combining inputs from human and AI on a common task. We introduce the construct of collaborative intelligence and develop a set of criteria, for evaluating whether an AI system enables collaborative intelligence. Applications utilizing collaborative intelligence had to have (1) complementarity (i.e. the collaboration draws upon complementary human and AI capability to improve outcomes), (2) a shared objective and outcome, and (3) sustained, two-way task-related interaction between human and AI. A systematic review of 1,250 AI applications published between 2012 and 2021 was carried out to investigate whether real-world examples of collaborative intelligence could be identified. The review yielded 16 AI systems which met the criteria, demonstrating that collaboration between humans and AI systems is possible and that these systems offer a wide range of performance benefits including efficiency, quality, creativity, safety, and human enjoyment."
436,2024,"An Industry Case Study on Adoption of AI-based Programming Assistants nan Programming assistants based on artificial intelligence (AI), such as ChatGPT and GitHub Copilot, have gained worldwide popularity recently. Studies in software development have explored the adoption of these tools, investigating their characteristics and impacts and how practitioners interact and perceive them. To contribute to this growing body of knowledge, in this study, we aim to explore the adoption of AI-based programming assistants in the Brazilian industry. More specifically, we aim to understand how practitioners of a particular Brazilian agroindustry-related company perceive and use AI-based tools to develop software. Using an online survey, we collected and analyzed 72 responses from employees of the studied company. Our findings suggest that practitioners mainly adopt ChatGPT and GitHub Copilot, interacting with these tools to accelerate online searching, typing, and syntax recall. A recurrent difficulty is the lack of context in the suggestions provided by these tools, but participants work on detailed descriptions to contextualize and cope with this challenge. Among the reasons for not using AI-based tools, the most influential is that participants use a commercial programming language, i.e., Uniface, which these tools lack examples. Our results provide insights into the state of the practice related to AI-based programming assistants and discuss implications for practitioners and researchers."
437,2021,"Towards a systematic educational framework for human-machine teaming nan Artificial Intelligence (AI) and machine learning (ML) are having a great impact on all aspects of society. However, due to the technical competencies and mathematical understanding required for implementing solutions leveraging these technologies, access to the communities working on these technologies is limited to those having these skills. This limits the ability of domain experts to directly transfer their knowledge and contribute to the development of AI and ML systems. To address this problem, we propose the Human Education AI Teaming (HEAT) framework, in which we draw on human education to design an innovative education system to enable collaboration between humans and AI cognitive agents. The main aim of HEAT is to promote the social integration of AI by allowing domain experts to focus more on communicating a body of knowledge to the machine, and less on the computational, data, and engineering concepts associated with how the machine learns. We follow an educational theory-driven approach to derive the content knowledge and competencies required by each agent. We conclude the paper with a demonstration case study explaining how the complex autonomous guidance of a flock of sheep could leverage HEAT to make the technology accessible by empowering non-AI specialists, livestock farmers in our example."
438,2024,"Analyzing prompt influence on automated method generation: an empirical study with copilot [arXiv] nan Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering. In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result."
439,2022,"The Potential of Artificial Intelligence as a Method of Software Developer's Productivity Improvement nan Artificial Intelligence finds application at all stages of Software Engineering, and uses the Neural Networks, Machine Learning, Natural Language Processing concepts. This paper attempts to review the instance of such approach - neural network programmer's assistant Copilot, based on Codex, the AI system developed by OpenAI. The differences between Codex language model's versions and analogous systems were analyzed. The main problems and gaps of this innovation, such as correct commands formulation, copyrighting, safety issues, inefficient code, good practice examples and restrictions are also considered. Additionally, the opportunities for Copilot's growth, development and possible features' proposed recommendations were suggested."
440,2023,"Navigating Complex Search Tasks with AI Copilots [arXiv] nan As many of us in the information retrieval (IR) research community know and appreciate, search is far from being a solved problem. Millions of people struggle with tasks on search engines every day. Often, their struggles relate to the intrinsic complexity of their task and the failure of search systems to fully understand the task and serve relevant results. The task motivates the search, creating the gap/problematic situation that searchers attempt to bridge/resolve and drives search behavior as they work through different task facets. Complex search tasks require more than support for rudimentary fact finding or re-finding. Research on methods to support complex tasks includes work on generating query and website suggestions, personalizing and contextualizing search, and developing new search experiences, including those that span time and space. The recent emergence of generative artificial intelligence (AI) and the arrival of assistive agents, or copilots, based on this technology, has the potential to offer further assistance to searchers, especially those engaged in complex tasks. There are profound implications from these advances for the design of intelligent systems and for the future of search itself. This article, based on a keynote by the author at the 2023 ACM SIGIR Conference, explores these issues and charts a course toward new horizons in information access guided by AI copilots."
441,2024,"Relearning learning [Influence - AI in education] nan As soon as ChatGPT launched, it caused a major headache for teachers already becoming more experienced in the intricacies of plagiarism detection software than they ever desired. When students were simply at risk of copying online answers to finish coursework, picking them up seemed relatively easy, if time-consuming. Artificial intelligence (AI) models based on the extensive foundation models used by ChatGPT and similar tools made it easy to crib answers without them being spotted nearly as easily. The literature on using large language models to teach programming is just getting started, but software development is going to fundamentally change, said Daniel Zingaro,, pointing to tools such as Github's Copilot and Codex. Copilot was launched two years ago as part of a joint project with machine learning specialist OpenAI that created the underlying Codex model, which is based on the latter company's GPT-3 large language model. The companies launched Copilot as a tool intended to complete statements they write in an interactive development environment's editor. Using the more advanced GPT-4 model. Though the current generation of AI chatbots is clearly prone to fabrication some education experts believe the technology already has a role to play in education. And that includes legal education, just as long as you do not rely on the models to find accurate case law."
442,2024,"Exploring the Effect of Multiple Natural Languages on Code Suggestion Using GitHub Copilot [arXiv] nan GitHub Copilot is an AI-enabled tool that automates program synthesis. It has gained significant attention since its launch in 2021. Recent studies have extensively examined Copilot's capabilities in various programming tasks, as well as its security issues. However, little is known about the effect of different natural languages on code suggestion. Natural language is considered a social bias in the field of NLP, and this bias could impact the diversity of software engineering. To address this gap, we conducted an empirical study to investigate the effect of three popular natural languages (English, Japanese, and Chinese) on Copilot. We used 756 questions of varying difficulty levels from AtCoder contests for evaluation purposes. The results highlight that the capability varies across natural languages, with Chinese achieving the worst performance. Furthermore, regardless of the type of natural language, the performance decreases significantly as the difficulty of questions increases. Our work represents the initial step in comprehending the significance of natural languages in Copilot's capability and introduces promising opportunities for future endeavors."
443,2024,"Artificial Versus Human Intelligence in the Diagnostic Approach of Ophthalmic Case Scenarios: A Qualitative Evaluation of Performance and Consistency. nan PURPOSE: To evaluate the efficiency of three artificial intelligence (AI) chatbots (ChatGPT-3.5 (OpenAI, San Francisco, California, United States), Bing Copilot (Microsoft Corporation, Redmond, Washington, United States), Google Gemini (Google LLC, Mountain View, California, United States)) in assisting the ophthalmologist in the diagnostic approach and management of challenging ophthalmic cases and compare their performance with that of a practicing human ophthalmic specialist. The secondary aim was to assess the short- and medium-term consistency of ChatGPT's responses.METHODS: Eleven ophthalmic case scenarios of variable complexity were presented to the AI chatbots and to an ophthalmic specialist in a stepwise fashion. Advice regarding the initial differential diagnosis, the final diagnosis, further investigation, and management was asked for. One month later, the same process was repeated twice on the same day for ChatGPT only.RESULTS: The individual diagnostic performance of all three AI chatbots was inferior to that of the ophthalmic specialist; however, they provided useful complementary input in the diagnostic algorithm. This was especially true for ChatGPT and Bing Copilot. ChatGPT exhibited reasonable short- and medium-term consistency, with the mean Jaccard similarity coefficient of responses varying between 0.58 and 0.76.CONCLUSION: AI chatbots may act as useful assisting tools in the diagnosis and management of challenging ophthalmic cases; however, their responses should be scrutinized for potential inaccuracies, and by no means can they replace consultation with an ophthalmic specialist."
444,2024,"Modelling operator control work across traffic management domains: implications for interaction design nan Traffic management in aviation, shipping, and rail transport shows similarities and dissimilarities in the work process. For example, they share the temporal aspect, but different levels of urgency in the control work set different requirements on monitoring, decisions, and actions. However, few studies have been presented that model and compare the different domains in terms of temporal decision-making. The Joint Control Framework (JCF) is an approach to analyse and temporally model operators' control processes from a cognitive systems engineering perspective. In this study, we have used JCF to map, and compare, cognitive joints, such as perceptions, decisions, and actions, in temporally challenging control situations in air traffic control, maritime vessel traffic service, and train traffic management. Data was collected collaboratively with traffic operators, focusing on (1) identifying challenging traffic situations and (2) jointly modelling the temporal decision-making patterns of these situations using simplified JCF. Post-analysis was done by breaking down the results into different processes and comparing domains to ascertain how operators maintain control. An intermediate level of activity-between general monitoring and work with specific vehicles-was identified: processes-in-focus. A shared problem arises in the shift between general monitoring and the processes-in-focus. All processes-in-focus comprise cognitive joint cycles of perceptions, decisions, and actions. However, depending on the framing of processes-in-focus, the patterns of joints, such as temporal extension and complexity, differ. In the remainder of the article, implications for the interaction design, in particular the potential for human-AI/automation teaming with higher levels of automation and cognitive autonomy, are discussed."
445,2024,"Chatbots talk Strabismus: Can AI become the new patient Educator? nan Background: Strabismus is a common eye condition affecting both children and adults. Effective patient education is crucial for informed decision-making, but traditional methods often lack accessibility and engagement. Chatbots powered by AI have emerged as a promising solution. Aim: This study aims to evaluate and compare the performance of three chatbots (ChatGPT, Bard, and Copilot) and a reliable website (AAPOS) in answering real patient questions about strabismus. Method: Three chatbots (ChatGPT, Bard, and Copilot) were compared to a reliable website (AAPOS) using real patient questions. Metrics included accuracy (SOLO taxonomy), understandability/actionability (PEMAT), and readability (Flesch-Kincaid). We also performed a sentiment analysis to capture the emotional tone and impact of the responses. Results: The AAPOS achieved the highest mean SOLO score (4.14 f 0.47), followed by Bard, Copilot, and ChatGPT. Bard scored highest on both PEMAT-U (74.8 f 13.3) and PEMAT-A (66.2 f 13.6) measures. FleschKincaid Ease Scores revealed the AAPOS as the easiest to read (mean score: 55.8 f 14.11), closely followed by Copilot. ChatGPT, and Bard had lower scores on readability. The sentiment analysis revealed exciting differences. Conclusion: Chatbots, particularly Bard and Copilot, show promise in patient education for strabismus with strengths in understandability and actionability. However, the AAPOS website outperformed in accuracy and readability."
446,2023,"Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair [arXiv] nan During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful copilots in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI copilots (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot fixes 66 and 50 bugs, respectively, surpassing the best-performing baseline by 14 and 16 bugs fixed. More importantly, Repilot is capable of producing more valid and correct patches than the base LLM when given the same generation budget."
447,2022,"An investigation on trust in AI-enabled collaboration: Application of AI-Driven chatbot in accommodation-based sharing economy nan Several measures taken to control the spread of the COVID-19 pandemic have severely disrupted the accom-modation sharing sector. This study attempts to find solutions to aid the recovery of the accommodation sharing sector via team efforts. Accordingly, we focus on the integration of artificial intelligence (AI) and collaboration. Despite the significant developments in AI technologies, there exists no research considering the application of AI in team collaboration. Utilizing the design science research method and collaboration engineering, we developed an AI-driven prototype system, AI-Driven, for collaboration process recommendation. Qualitative results show that the newly developed tool for collaboration process recommendation has achieved satisfactory performance. Furthermore, we investigated the antecedents and outcomes of trust in the AI-driven collaboration context. From a practical perspective, we propose several solutions to the challenges looming over the accommodation sharing sector according to collaboration deliverables. Furthermore, a system prototype was developed to facilitate collaboration process recommendation and provide procedural guidance."
449,2025,"Practically implementing an LLM-supported collaborative vulnerability remediation process: A team-based approach nan Incorporating LLM into cybersecurity operations, a typical real-world high-stakes task, is critical but non-trivial in practice. Using cybersecurity as the study context, we conduct a three-step mix-method study to incorporate LLM into the vulnerability remediation process effectively. Specifically, we deconstruct the deficiencies in user satisfaction within the existing process (Study 1). This inspires us to design, implement, and empirically validate an LLM-supported collaborative vulnerability remediation process through a field study (Study 2). Given LLM's diverse contributions, we further investigate LLM's double-edge roles through the analysis of remediation reports and follow-up interviews (Study 3). In essence, our contribution lies in promoting an efficient LLM-supported collaborative vulnerability remediation process. These first-hand, real-world pieces of evidence suggest that when incorporating LLMs into practical processes, facilitating the collaborations among all associated stakeholders, reshaping LLMs' roles according to task complexity, as well as approaching the short-term side effects of improved user engagement facilitated by LLMs with a rational mindset."
450,2023,"Security Weaknesses of Copilot Generated Code in GitHub [arXiv] nan Modern code generation tools use AI models, particularly Large Language Models (LLMs), to generate functional and complete code. While such tools are becoming popular and widely available for developers, using these tools is often accompanied by security challenges. Therefore, it is important to assess the quality of the generated code, especially in terms of its security. Researchers have recently explored various aspects of code generation tools, including security. However, many open questions about the security of the generated code require further investigation, especially the security issues of automatically generated code in the wild. To this end, we conducted an empirical study by analyzing the security weaknesses in code snippets generated by GitHub Copilot that are found as part of publicly available projects hosted on GitHub. The goal is to investigate the types of security issues and their scale in real-world scenarios (rather than crafted scenarios). To this end, we identified 435 code snippets generated by Copilot from publicly available projects. We then conducted extensive security analysis to identify Common Weakness Enumeration (CWE) instances in these code snippets. The results show that (1) 35.8% of Copilot generated code snippets contain CWEs, and those issues are spread across multiple languages, (2) the security weaknesses are diverse and related to 42 different CWEs, in which CWE-78: OS Command Injection, CWE-330: Use of Insufficiently Random Values, and CWE-703: Improper Check or Handling of Exceptional Conditions occurred the most frequently, and (3) among the 42 CWEs identified, 11 of those belong to the currently recognized 2022 CWE Top-25. Our findings confirm that developers should be careful when adding code generated by Copilot (and similar AI code generation tools) and should also run appropriate security checks as they accept the suggested code."
451,2024,"Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot nan Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering.In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically consider the task of using Copilot with 124,800 prompts obtained by systematically combining the eight considered prompt features to generate the implementation of 200 Java methods. Results show how some prompt features, such as the presence of examples and the summary of the purpose of the method, can significantly influence the quality of the result."
452,2024,"Disrupting Test Development with AI Assistants nan Recent advancements in large language models, including GPT-4 and its variants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT, and Tabnine, have significantly transformed software development. This paper analyzes how these innovations impact productivity and software test development metrics. These tools enable developers to generate complete software programs with minimal human intervention before deployment. However, thorough review and testing by developers are still crucial. Utilizing the Test Pyramid concept, which categorizes tests into unit, integration, and end-to-end tests, we evaluate three popular AI coding assistants by generating and comparing unit tests for opensource modules. Our findings show that AI-generated tests are of equivalent quality to original tests, highlighting differences in usage and results among the tools. This research enhances the understanding and capabilities of AI-assistant tools in automated testing."
453,2024,"Validating AI-Generated Code with Live Programming nan AI-powered programming assistants are increasingly gaining popularity, with GitHub Copilot alone used by over a million developers worldwide. These tools are far from perfect, however, producing code suggestions that may be incorrect in subtle ways. As a result, developers face a new challenge: validating AI's suggestions. This paper explores whether Live Programming (LP), a continuous display of a program's runtime values, can help address this challenge. To answer this question, we built a Python editor that combines an AI-powered programming assistant with an existing LP environment. Using this environment in a between-subjects study (N = 17), we found that by lowering the cost of validation by execution, LP can mitigate over- and under-reliance on AI-generated programs and reduce the cognitive load of validation for certain types of tasks."
454,2023,"Towards Neuro-Symbolic AI for Assured and Trustworthy Human-Autonomy Teaming nan Despite the tremendous impact and potential of Artificial Intelligence (AI) for civilian and military applications, it has reached an impasse as learning and reasoning work well for certain applications and it generally suffers from a number of challenges such as hidden biases and causality. Next, symbolic AI (not as efficient as sub-symbolic AI), offers transparency, explainability, verifiability and trustworthiness. To address these limitations, neuro-symbolic AI has been emerged as a new AI field that combines efficiency of sub-symbolic AI with the assurance and transparency of symbolic AI. Furthermore, AI (that suffers from aforementioned challenges) will remain inadequate for operating independently in contested, unpredictable and complex multi-domain battlefield (MDB) environment for the foreseeable future and the AI enabled autonomous systems will require human in the loop to complete the mission in such a contested environment. Moreover, in order to successfully integrate AI enabled autonomous systems into military operations, military operators need to have assurance that these systems will perform as expected and in a safe manner. Most importantly, Human-Autonomy Teaming (HAT) for shared learning and understanding and joint reasoning is crucial to assist operations across military domains (space, air, land, maritime, and cyber) at combat speed with high assurance and trust. In this paper, we present a rough guide to key research challenges and perspectives of neuro symbolic AI for assured and trustworthy HAT."
455,2023,"Artificial Intelligence Versus Artists? nan Generative AI is seeing a burst of innovation, interest, and investments. This type of AI takes simple text in everyday language as input and produces output in different forms such as text, images, videos, audios, and 3D models. ChatGPTcurrently creating a lot of buzz (and controversy)is an example of a text-to-text generative AI tool. GitHub Copilot is a tool that can produce software code based on natural language inputs. DALLE 2, Midjourney, and Stable Diffusion are examples of text-to-image AI. With the right text inputs or prompts, the image generators can produce impressive images. In fact, one such image even won first prize at the Colorado State Fair's art competition last year. Artists using software tools as aids in their creative pursuits is nothing new, but generative AI tools are challenging some of our notions about artstarting with whether such computer-generated images can really be called art."
456,2024,"Examining the Performance of ChatGPT 3.5 and Microsoft Copilot in Otolaryngology: A Comparative Study with Otolaryngologists' Evaluation. nan To evaluate the response capabilities, in a public healthcare system otolaryngology job competition examination, of ChatGPT 3.5 and an internet-connected GPT-4 engine (Microsoft Copilot) with the real scores of otolaryngology specialists as the control group. In September 2023, 135 questions divided into theoretical and practical parts were input into ChatGPT 3.5 and an internet-connected GPT-4. The accuracy of AI responses was compared with the official results from otolaryngologists who took the exam, and statistical analysis was conducted using Stata 14.2. Copilot (GPT-4) outperformed ChatGPT 3.5. Copilot achieved a score of 88.5 points, while ChatGPT scored 60 points. Both AIs had discrepancies in their incorrect answers. Despite ChatGPT's proficiency, Copilot displayed superior performance, ranking as the second-best score among the 108 otolaryngologists who took the exam, while ChatGPT was placed 83rd. A chat powered by GPT-4 with internet access (Copilot) demonstrates superior performance in responding to multiple-choice medical questions compared to ChatGPT 3.5."
457,2024,"Digital pathology implementation in cancer diagnostics: towards informed decision-making. nan Digital pathology (DP) has become a part of the cancer healthcare system, creating additional value for cancer patients. DP implementation in clinical practice provides plenty of benefits but also harbors hidden ethical challenges affecting physician-patient relationships. This paper addresses the ethical obligation to transform the physician-patient relationship for informed and responsible decision-making when using artificial intelligence (AI)-based tools for cancer diagnostics. DP application allows to improve the performance of the Human-AI Team shifting focus from AI challenges towards the Augmented Human Intelligence (AHI) benefits. AHI enhances analytical sensitivity and empowers pathologists to deliver accurate diagnoses and assess predictive biomarkers for further personalized treatment of cancer patients. At the same time, patients' right to know about using AI tools, their accuracy, strengths and limitations, measures for privacy protection, acceptance of privacy concerns and legal protection defines the duty of physicians to provide the relevant information about AHI-based solutions to patients and the community for building transparency, understanding and trust, respecting patients' autonomy and empowering informed decision-making in oncology."
458,2024,AI-based Code Generation: Achievements and Open Problems nan nan
459,2024,"Toward human-centered shared autonomy AI paradigms for human-robot teaming in healthcare nan With recent advancements in AI and computation tools, intelligent paradigms emerged to empower different fields such as healthcare robots with new capabilities. Advanced AI robotic algorithms (e.g., reinforcement learning) can be trained and developed to autonomously make individual decisions to achieve a desired and usually fixed goal. However, such independent decisions and goal achievements might not be ideal for a healthcare robot that usually interacts with a dynamic end-user or a patient. In such a complex human-robot interaction (teaming) framework, the dynamic user continuously wants to be involved in decision-making as well as introducing new goals while interacting with their present environment in real-time. To address this challenge, an adaptive shared autonomy AI paradigm is required to be developed for the two interactive agents (Human & AI agents) with a foundation based on human-centered factors to avoid any possible ethical issues and guarantee no harm to humanity."
460,2023,"AI-based Code Generation: The Good, the Bad, and the Ugly nan Large language models (LLMs) have gained significant attention in the software engineering community. Nowadays developers have the possibility to exploit these models through industrial-grade tools providing a handy interface toward LLMs, such as GitHub Copilot. In this talk, I will discuss recent successful applications of LLMs for code generation, showing how they are changing the way in which developers approach coding (the Good). Then, I will present a few new issues arising from the usage of AI-based code generators (the Bad), closing with a general reflection about what we can improve as a research community when approaching this area (the Ugly)."
461,2020,"Crowd-Assisted Disaster Scene Assessment with Human-AI Interactive Attention nan The recent advances of mobile sensing and artificial intelligence (AI) have brought new revolutions in disaster response applications. One example is disaster scene assessment (DSA) which leverages computer vision techniques to assess the level of damage severity of the disaster events from images provided by eyewitnesses on social media. The assessment results are critical in prioritizing the rescue operations of the response teams. While AI algorithms can significantly reduce the detection time and manual labeling cost in such applications, their performance often falls short of the desired accuracy. Our work is motivated by the emergence of crowdsourcing platforms (e.g., Amazon Mechanic Turk, Waze) that provide unprecedented opportunities for acquiring human intelligence for AI applications. In this paper, we develop an interactive Disaster Scene Assessment (iDSA) scheme that allows AI algorithms to directly interact with humans to identify the salient regions of the disaster images in DSA applications. We also develop new incentive designs and active learning techniques to ensure reliable, timely, and cost-efficient responses from the crowdsourcing platforms. Our evaluation results on real-world case studies during Nepal and Ecuador earthquake events demonstrate that iDSA can significantly outperform state-of-the-art baselines in accurately assessing the damage of disaster scenes."
462,2024,"Assessing the Use of GitHub Copilot on Students of Engineering of Information Systems nan This study examines the impact of AI programming assistants like GitHub Copilot and ChatGPT on software engineering efficiency, an area that has seen limited empirical research. We experimentally evaluated the performance of programmers (n=16) in Python coding tasks with and without AI assistance, measuring time-to-completion and feature implementation. Results indicate that participants utilizing AI assistance completed tasks significantly faster (p = 0.033) and implemented more required features (p = 0.012) compared to those relying solely on unaided coding. These findings offer empirical insights into the integration of AI tools in software development workflows, highlighting their potential to enhance efficiency without compromising code quality or completeness, with implications for organizational pipelines and practitioner skills. Responses to exit surveys suggest that participants without IA tools assistance encountered frustrations related to code recall, time constraints, and problem-solving, while assisted participants reported no negative experiences, focusing instead on successful completion of tasks within the allotted time."
463,2024,"Assessing the Readability of Patient Education Materials on Cardiac Catheterization From Artificial Intelligence Chatbots: An Observational Cross-Sectional Study. nan BACKGROUND: Artificial intelligence (AI) is a burgeoning new field that has increased in popularity over the past couple of years, coinciding with the public release of large language model (LLM)-driven chatbots. These chatbots, such as ChatGPT, can be engaged directly in conversation, allowing users to ask them questions or issue other commands. Since LLMs are trained on large amounts of text data, they can also answer questions reliably and factually, an ability that has allowed them to serve as a source for medical inquiries. This study seeks to assess the readability of patient education materials on cardiac catheterization across four of the most common chatbots: ChatGPT, Microsoft Copilot, Google Gemini, and Meta AI.METHODOLOGY: A set of 10 questions regarding cardiac catheterization was developed using website-based patient education materials on the topic. We then asked these questions in consecutive order tofour of the most common chatbots: ChatGPT, Microsoft Copilot, Google Gemini, and Meta AI. The Flesch Reading Ease Score (FRES) was used to assess the readability score. Readability grade levels were assessed using six tools: Flesch-Kincaid Grade Level (FKGL), Gunning Fog Index (GFI), Coleman-Liau Index (CLI), Simple Measure of Gobbledygook (SMOG) Index, Automated Readability Index (ARI), and FORCAST Grade Level.RESULTS: The mean FRES across all four chatbots was 40.2, while overall mean grade levels for the four chatbots were 11.2, 13.7, 13.7, 13.3, 11.2, and 11.6 across the FKGL, GFI, CLI, SMOG, ARI, and FORCAST indices, respectively. Mean reading grade levels across the six tools were 14.8 for ChatGPT, 12.3 for Microsoft Copilot, 13.1 for Google Gemini, and 9.6 for Meta AI. Further, FRES values for the four chatbots were 31, 35.8, 36.4, and 57.7, respectively.CONCLUSIONS: This study shows that AI chatbots are capable of providing answers to medical questions regarding cardiac catheterization. However, the responses across the four chatbots had overall mean reading grade levels at the 11th-13th-grade level, depending on the tool used. This means that the materials were at the high school and even college reading level, which far exceeds the recommended sixth-grade level for patient education materials. Further, there is significant variability in the readability levels provided by different chatbots as, across all six grade-level assessments, Meta AI had the lowest scores andChatGPT generally had the highest."
464,2024,"Intelligent Tutor: Leveraging ChatGPT and Microsoft Copilot Studio to Deliver a Generative AI Student Support and Feedback System within Teams nan This study explores the integration of the ChatGPT API with GPT-4 model and Microsoft Copilot Studio on the Microsoft Teams platform to develop an intelligent tutoring system. Designed to provide instant support to students, the system dynamically adjusts educational content in response to the learners' progress and feedback. Utilizing advancements in natural language processing and machine learning, it interprets student inquiries, offers tailored feedback, and facilitates the educational journey. Initial implementation highlights the system's potential in boosting students' motivation and engagement, while equipping educators with critical insights into the learning process, thus promoting tailored educational experiences and enhancing instructional effectiveness."
465,2023,"Generative AI Assistants in Software Development Education [arXiv] nan The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations."
466,1994,"An intelligent copilot system in a real vehicle nan This paper presents the software architecture of an electronic copilot whose purpose is to give an appropriate support to the driver in all traffic situations. The objective is to increase the driver possibilities to take the right decision in a dynamic environment. Furthermore, the information presented to the driver must be carefully selected depending on the current traffic situation. After a global description of the copilot functional architecture, the authors present the Dynamic Data Manager in charge of updating the world model, and the Situation Analysis Module that diagnoses the situation, controls the sensors running mode and determines the messages to send to the driver. This work takes place in the European Prometheus Pro-Art project and will be integrated in a vehicle prototype called Prolab2. This French demonstrator integrates the works of nine research laboratories and two car companies, PSA and RENAULT."
467,2024,"Performance, Workload, Emotion, and Self-Efficacy of Novice Programmers Using AI Code Generation nan Artificial Intelligence-driven Development Environments (AIDEs) offer developers revolutionary computer programming assistance. There is great potential in incorporating AIDEs into Computer Science education; however, the effects of these tools should be fully examined before doing so. Here, a within-subjects study was conducted to compare the programming performance, workload, emotion, and self-efficacy of seventeen novices coding with and without use of the GitHub Copilot AIDE under time pressure. Results showed that using the AIDE significantly increased programming efficiency and reduced effort and mental workload but did not significantly impact emotion or self-efficacy. However, participants' performance improved with more experience using the AI, and their self-efficacy followed. The results suggest that students who try AIDEs will likely be tempted to use them for time-sensitive work. There is no evidence that providing AIDEs will aid struggling students, but there is a clear need for students to practice with AI to become competent and confident using it."
468,2024,"Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks [arXiv] nan Generative AI including large language models (LLMs) have recently gained significant interest in the geo-science community through its versatile task-solving capabilities including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification. So far, the assessment of LLMs for spatial tasks has primarily focused on ChatGPT, arguably the most prominent AI chatbot, whereas other chatbots received less attention. To narrow this research gap, this study evaluates the correctness of responses for a set of 54 spatial tasks assigned to four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot. Overall, the chatbots performed well on spatial literacy, GIS theory, and interpretation of programming code and given functions, but revealed weaknesses in mapping, code generation, and code translation. ChatGPT-4 outperformed other chatbots across most task categories."
469,2024,"Saltzer & Schroeder for 2030: Security engineering principles in a world of AI nan Writing secure code is challenging and so it is expected that, following the release of code-generative AI tools, such as ChatGPT and GitHub Copilot, developers will use these tools to perform security tasks and use security APIs. However, is the code generated by ChatGPT secure? How would the everyday software or security engineer be able to tell? As we approach the next decade we expect a greater adoption of code-generative AI tools and to see developers use them to write secure code. In preparation for this, we need to ensure security-by-design. In this paper, we look back in time to Saltzer & Schroeder's security design principles as they will need to evolve and adapt to the challenges that come with a world of AI-generated code."
470,2024,"The Emerging Role of Large Language Models in Improving Prostate Cancer Literacy nan This study assesses the effectiveness of chatbots powered by Large Language Models (LLMs)-ChatGPT 3.5, CoPilot, and Gemini-in delivering prostate cancer information, compared to the official Patient's Guide. Using 25 expert-validated questions, we conducted a comparative analysis to evaluate accuracy, timeliness, completeness, and understandability through a Likert scale. Statistical analyses were used to quantify the performance of each model. Results indicate that ChatGPT 3.5 consistently outperformed the other models, establishing itself as a robust and reliable source of information. CoPilot also performed effectively, albeit slightly less so than ChatGPT 3.5. Despite the strengths of the Patient's Guide, the advanced capabilities of LLMs like ChatGPT significantly enhance educational tools in healthcare. The findings underscore the need for ongoing innovation and improvement in AI applications within health sectors, especially considering the ethical implications underscored by the forthcoming EU AI Act. Future research should focus on investigating potential biases in AI-generated responses and their impact on patient outcomes."
471,2024,"Utilizing Large Language Models in Ophthalmology: The Current Landscape and Challenges nan A large language model (LLM) is an artificial intelligence (AI) model that uses natural language processing (NLP) to understand, interpret, and generate human-like language responses from unstructured text input. Its real-time response capabilities and eloquent dialogue enhance the interactive user experience in human-AI communication like never before. By gathering several sources on the internet, LLM chatbots can interact and respond to a wide range of queries, including problem solving, text summarization, and creating informative notes. Since ophthalmology is one of the medical fields integrating image analysis, telemedicine, AI, and other technologies, LLMs are likely to play an important role in eye care in the near future. This review summarizes the performance and potential applicability of LLMs in ophthalmology according to currently available publications."
472,2024,"Business Impact of Global Shifts on Indonesia's ICT Using AI Techniques nan This paper investigates the impact of dynamic global shifts on the information and communication technology (ICT) industry using advanced artificial intelligence (AI) techniques. The five global shifts examined are: altered environmental relationships, evolving economic and political power dynamics, increasing divergence and polarization, changing demographics, and sociocultural changes in the workplace. An AI-enabled qualitative analysis was conducted using five AI platformsBard, You.com, Copilot Microsoft, ChatGPT, and Perplexityto gather and analyze relevant data. The findings highlight the effects of these shifts on business revenue and operational costs. Based on the AI-generated insights, this paper provides strategic recommendations for ICT firms to adapt and thrive in the evolving business environment."
473,2024,"A multimodal generative AI copilot for human pathology nan Computational pathology1,2 has witnessed considerable progress in the development of both task-specific predictive models and task-agnostic self-supervised vision encoders3,4. However, despite the explosive growth of generative artificial intelligence (AI), there have been few studies on building general-purpose multimodal AI assistants and copilots5 tailored to pathology. Here we present PathChat, a vision-language generalist AI assistant for human pathology. We built PathChat by adapting a foundational vision encoder for pathology, combining it with a pretrained large language model and fine-tuning the whole system on over 456,000 diverse visual-language instructions consisting of 999,202 question and answer turns. We compare PathChat with several multimodal vision-language AI assistants and GPT-4V, which powers the commercially available multimodal general-purpose AI assistant ChatGPT-4 (ref. 6). PathChat achieved state-of-the-art performance on multiple-choice diagnostic questions from cases with diverse tissue origins and disease models. Furthermore, using open-ended questions and human expert evaluation, we found that overall PathChat produced more accurate and pathologist-preferable responses to diverse queries related to pathology. As an interactive vision-language AI copilot that can flexibly handle both visual and natural language inputs, PathChat may potentially find impactful applications in pathology education, research and human-in-the-loop clinical decision-making.PathChat, a multimodal generative AI copilot for human pathology, has been trained on a large dataset of visual-language instructions to interactively assist users with diverse pathology tasks."
474,2021,"K-level Reasoning for Zero-Shot Coordination in Hanabi nan The standard problem setting in cooperative multi-agent settings is self-play (SP), where the goal is to train a team of agents that works well together. However, optimal SP policies commonly contain arbitrary conventions (handshakes) and are not compatible with other, independently trained agents or humans. This latter desiderata was recently formalized by [18] as the zero-shot coordination (ZSC) setting and partially addressed with their Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance in the card game Hanabi. OP assumes access to the symmetries of the environment and prevents agents from breaking these in a mutually incompatible way during training. However, as the authors point out, discovering symmetries for a given environment is a computationally hard problem. Instead, we show that through a simple adaption of k-level reasoning (KLR) [7], synchronously training all levels, we can obtain competitive ZSC and ad-hoc teamplay performance in Hanabi, including when paired with a humanlike proxy bot. We also introduce a new method, synchronous-k-level reasoning with a best response (SyKLRBR), which further improves performance on our synchronous KLR by co-training a best response."
475,2023,"Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making nan In AI-assisted decision-making, it is critical for human decision-makers to know when to trust AI and when to trust themselves. However, prior studies calibrated human trust only based on AI confdence indicating AI's correctness likelihood (CL) but ignored humans' CL, hindering optimal team decision-making. To mitigate this gap, we proposed to promote humans' appropriate trust based on the CL of both sides at a task-instance level. We frst modeled humans' CL by approximating their decision-making models and computing their potential performance in similar instances. We demonstrated the feasibility and efectiveness of our model via two preliminary studies. Then, we proposed three CL exploitation strategies to calibrate users' trust explicitly/implicitly in the AI-assisted decision-making process. Results from a between-subjects experiment (N=293) showed that our CL exploitation strategies promoted more appropriate human trust in AI, compared with only using AI confdence. We further provided practical implications for more human-compatible AI-assisted decision-making."
476,2024,"Envisioning the Next-Generation AI Coding Assistants: Insights & Proposals nan As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants. AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses. We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants."
477,2024,"Improving Performance of Commercially Available AI Products in a Multi-Agent Configuration nan In recent years, with the rapid advancement of large language models (LLMs), multi-agent systems have become increasingly more capable of practical application. At the same time, the software development industry has had a number of new AI-powered tools developed that improve the software development lifecycle (SDLC). Academically, much attention has been paid to the role of multi-agent systems to the SDLC. And, while single-agent systems have frequently been examined in real-world applications, we have seen comparatively few real-world examples of publicly available commercial tools working together in a multi-agent system with measurable improvements. In this experiment we test context sharing between Crowdbotics PRD AI, a tool for generating software requirements using AI, and GitHub Copilot, an AI pair-programming tool. By sharing business requirements from PRD AI, we improve the code suggestion capabilities of GitHub Copilot by 13.8% and developer task success rate by 24.5% -- demonstrating a real-world example of commercially-available AI systems working together with improved outcomes."
478,2024,"Developing Critical Thinking Practices Interwoven with Generative AI Usage in an Introductory Programming Course nan Software development has evolved significantly. In the past, developers were required to have comprehensive understanding of programming languages, algorithms, and computer architecture. However, with the emergence of the Internet, software libraries, frameworks, and forums became widely available, which utilize reusable software components that can reduce development time and costs. The advent of Generative Artificial Intelligence (AI) tools, such as ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, has further enhanced the developer's toolkit, as these tools can be used for a wide variety of tasks such as code generation, documentation, commenting and reviewing. As programming is often slow and requires trial and error, novice programmers can be tempted to apply the first solution found on the Internet or proposed by an AI tool without much critical reflection or notion of responsibility. Hence, the advances of AI have raised both excitement and concerns among Information Technology (IT)/Computer Science (CS) students and educators. Yet, AI tools are here to stay, and students must learn to use them responsibly. The aim of this paper is to investigate how to design learning activities that introduce Generative AI tools (GitHub Copilot and ChatGPT) for programming while promoting critical thinking practices among students in an introductory programming course in the first semester. Students' opinions and customs were surveyed before and after the AI-based programming assignment. The results indicate that students' awareness of the possibilities and limitations of AI, as well as practices of critical thinking in programming increased. This is encouraging as critical thinking is an integral part of best programming practices."
479,2024,"SciSpace Copilot: Empowering Researchers through Intelligent Reading Assistance nan We introduce SciSpace Copilot, an AI research assistant that helps in understanding and reading research papers faster by providing a plethora of features. Answering questions from a document has recently become popular using the Retrieval Augmented Generation (RAG) approach. Our tool uses an advanced question-answering pipeline to give accurate answers while also citing sources from the paper. We also provide other valuable features on scientific text, including generating explanations, generating summaries, adding notes and highlights, and finding related papers from our 280 million corpus. Our tool supports 75+ languages, making research more accessible across language barriers. Thousands of users use SciSpace Copilot on a daily basis by uploading their articles to understand research faster and better. Our tool can be accessed at this link: https://typeset.io."
480,2024,"ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance [arXiv] nan In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learning to minimize ED wait time and maximize prediction accuracy of critical outcomes. On MIMIC-ED-Assist, ED-Copilot improves prediction accuracy over baselines while halving average wait time from four hours to two hours. Ablation studies demonstrate the importance of model scale and use of a bio-medical language model. Further analyses reveal the necessity of personalized laboratory test suggestions for diagnosing patients with severe cases, as well as the potential of ED-Copilot in providing ED clinicians with informative laboratory test recommendations. Our code is available at https://github.com/cxcscmu/ED-Copilot."
481,2024,"The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot nan Generative AI (GenAI) has been shown to enhance individual productivity in a guided setting. While it is also likely to transform processes in a collaborative work setting, it is unclear what trajectory this transformation will follow. Collaborative environment is characterized by a blend of origination tasks that involve building something from scratch and iteration tasks that involve refining on others' work. Whether GenAI affects these two aspects of collaborative work and to what extent is an open empirical question. We study this question within the open-source development landscape, a prime example of collaborative innovation, where contributions are voluntary and unguided. Specifically, we focus on the launch of GitHub Copilot in October 2021 and leverage a natural experiment in which GitHub Copilot (a programming-focused LLM) selectively rolled out support for Python, but not for R. We observe a significant jump in overall contributions, suggesting that GenAI effectively augments collaborative innovation in an unguided setting. Interestingly, Copilot's launch increased maintenance-related contributions, which are mostly iterative tasks involving building on others' work, significantly more than code-development contributions, which are mostly origination tasks involving standalone contributions. This disparity was exacerbated in active projects with extensive coding activity, raising concerns that, as GenAI models improve to accommodate richer context, the gap between origination and iterative solutions may widen. We discuss practical and policy implications to incentivize high-value innovative solutions."
482,2021,"AlphaMosaic: An Artificially Intelligent Battle Management Architecture nan Warfare is increasing in complexity, speed, and scale-not only due to enhanced technological capabilities but also from the employment methodologies associated with them. Incorporating artificial intelligence (AI) technology into this realm is a cogent solution to help address these complications because of the reduced cost, reduced risk to human life, and increased capability to rapidly adapt to changing environments. However, the introduction of AI comes with a host of new considerations. If AI is to be successfully integrated into air combat, humans must be included in the AI processing loop, and human interaction with AI decision loops must be frictionless. Additionally, AI-supported battle management systems must be designed for high and increasing human trust across dynamically changing scenarios. This paper presents AlphaMosaic, an AI battle manager developed as part of the Defense Advanced Research Projects Agency Air Combat Evolution program that is designed to incorporate human feedback in a manner conducive to true manned-unmanned aircraft teaming in beyond visual range air-combat scenarios."
483,2024,"How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study nan Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants."
484,2024,"OS-Copilot: Towards Generalist Computer Agents with Self-Improvement [arXiv] nan Autonomous interaction with the computer has been a longstanding challenge with great potential, and the recent proliferation of large language models (LLMs) has markedly accelerated progress in building digital agents. However, most of these agents are designed to interact with a narrow domain, such as a specific software or website. This narrow focus constrains their applicability for general computer tasks. To this end, we introduce OS-Copilot, a framework to build generalist agents capable of interfacing with comprehensive elements in an operating system (OS), including the web, code terminals, files, multimedia, and various third-party applications. We use OS-Copilot to create FRIDAY, a self-improving embodied agent for automating general computer tasks. On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods by 35%, showcasing strong generalization to unseen applications via accumulated skills from previous tasks. We also present numerical and quantitative evidence that FRIDAY learns to control and self-improve on Excel and Powerpoint with minimal supervision. Our OS-Copilot framework and empirical findings provide infrastructure and insights for future research toward more capable and general-purpose computer agents."
485,2024,"Comparative accuracy of ChatGPT-4, Microsoft Copilot and Google Gemini in the Italian entrance test for healthcare sciences degrees: a cross-sectional study nan Background Artificial intelligence (AI) chatbots are emerging educational tools for students in healthcare science. However, assessing their accuracy is essential prior to adoption in educational settings. This study aimed to assess the accuracy of predicting the correct answers from three AI chatbots (ChatGPT-4, Microsoft Copilot and Google Gemini) in the Italian entrance standardized examination test of healthcare science degrees (CINECA test). Secondarily, we assessed the narrative coherence of the AI chatbots' responses (i.e., text output) based on three qualitative metrics: the logical rationale behind the chosen answer, the presence of information internal to the question, and presence of information external to the question. Methods An observational cross-sectional design was performed in September of 2023. Accuracy of the three chatbots was evaluated for the CINECA test, where questions were formatted using a multiple-choice structure with a single best answer. The outcome is binary (correct or incorrect). Chi-squared test and a post hoc analysis with Bonferroni correction assessed differences among chatbots performance in accuracy. A p-value of < 0.05 was considered statistically significant. A sensitivity analysis was performed, excluding answers that were not applicable (e.g., images). Narrative coherence was analyzed by absolute and relative frequencies of correct answers and errors. Results Overall, of the 820 CINECA multiple-choice questions inputted into all chatbots, 20 questions were not imported in ChatGPT-4 (n = 808) and Google Gemini (n = 808) due to technical limitations. We found statistically significant differences in the ChatGPT-4 vs Google Gemini and Microsoft Copilot vs Google Gemini comparisons (p-value < 0.001). The narrative coherence of AI chatbots revealed Logical reasoning as the prevalent correct answer (n = 622, 81.5%) and Logical error as the prevalent incorrect answer (n = 40, 88.9%). Conclusions Our main findings reveal that: (A) AI chatbots performed well; (B) ChatGPT-4 and Microsoft Copilot performed better than Google Gemini; and (C) their narrative coherence is primarily logical. Although AI chatbots showed promising accuracy in predicting the correct answer in the Italian entrance university standardized examination test, we encourage candidates to cautiously incorporate this new technology to supplement their learning rather than a primary resource. Trial registration Not required."
486,2022,"Beyond tracking: using deep learning to discover novel interactions in biological swarms nan Most deep-learning frameworks for understanding biological swarms are designed to fit perceptive models of group behavior to individual-level data (e.g., spatial coordinates of identified features of individuals) that have been separately gathered from video observations. Despite considerable advances in automated tracking, these methods are still very expensive or unreliable when tracking large numbers of animals simultaneously. Moreover, this approach assumes that the human-chosen features include sufficient features to explain important patterns in collective behavior. To address these issues, we propose training deep network models to predict system-level states directly from generic graphical features from the entire view, which can be relatively inexpensive to gather in a completely automated fashion. Because the resulting predictive models are not based on human-understood predictors, we use explanatory modules (e.g., Grad-CAM) that combine information hidden in the latent variables of the deep-network model with the video data itself to communicate to a human observer which aspects of observed individual behaviors are most informative in predicting group behavior. This represents an example of augmented intelligence in behavioral ecology-knowledge co-creation in a human-AI team. As proof of concept, we utilize a 20-day video recording of a colony of over 50 Harpegnathos saltator ants to showcase that, without any individual annotations provided, a trained model can generate an importance map across the video frames to highlight regions of important behaviors, such as dueling (which the AI has no a priori knowledge of), that play a role in the resolution of reproductive-hierarchy re-formation. Based on the empirical results, we also discuss the potential use and current challenges to further develop the proposed framework as a tool to discover behaviors that have not yet been considered crucial to understand complex social dynamics within biological collectives."
487,2024,"Exploring Generative AI's Impact on Facilitating the Transition of On-Premises Applications to the Cloud nan In this study, we investigated the transformative potential of generative AI in facilitating the migration of on-premises applications to the cloud. We discovered multiple benefits after experimenting with various IDEs, such as Visual Studio Enterprise and Visual Studio Code, and migration methodologies recommended by global cloud providers such as AWS or Azure. These include a 20% increase in coding speed, improvements and migrations to numerous CI/CD pipelines, and the value added by GitHub Copilot in describing and creating code for Class Diagrams. Our investigation involves testing code capabilities and explanations across various frameworks and projects. We discussed the benefits of using more efficient rapid engineering practices and uncovered GitHub's capacity to create code in multiple languages. By utilizing Generative AI, we demonstrated its ability to improve time efficiency, eliminate errors, and preserve consistency. AI may dramatically expedite migration through cost savings and faster development times. Throughout our journey, we also used Google's Gemini to turn legacy Jenkins code into GitHub actions, demonstrating the versatility and potential of AI in cloud migration."
488,2024,"Appraisal of AI-generated dermatology literature reviews nan Background: Artificial intelligence (AI) tools have the potential to revolutionize many facets of medicine and medical sciences research. Numerous AI tools have been developed and are in continuous states of iterative improvement in their functionality. Objectives: This study aimed to assess the performance of three AI tools: The Literature, Microsoft's Copilot and Google's Gemini in performing literature reviews on a range of dermatology topics. Methods: Each tool was asked to write a literature review on five topics. The topics chosen have recently had peer-reviewed systematic reviews published. The outputs of each took were graded on their evidence and analysis, conclusions and references on a 5-point Likert scale by three dermatologists who are working in clinical practice, have completed the UK dermatology postgraduate training examination and are partaking in continued professional development. Results: Across all five topics chosen, the literature reviews written by Gemini scored the highest. The mean score for Gemini for each review was 10.53, significantly higher than the mean scores achieved by The Literature (7.73) and Copilot (7.4) (p < 0.001). Conclusions: This paper shows that AI-generated literature reviews can provide real-time summaries of medical literature across a range of dermatology topics, but limitations to their comprehensiveness and accuracy are apparent."
489,2024,"Do Generative AI Tools Ensure Green Code? An Investigative Study nan Software sustainability is emerging as a primary concern, aiming to optimize resource utilization, minimize environmental impact, and promote a greener, more resilient digital ecosystem. The sustainability or 'greenness' of software is typically determined by the adoption of sustainable coding practices. With a maturing ecosystem around generative AI, many software developers now rely on these tools to generate code using natural language prompts. Despite their potential advantages, there is a significant lack of studies on the sustainability aspects of AI-generated code. Specifically, how environmentally friendly is the AI-generated code based upon its adoption of sustainable coding practices? In this paper, we present the results of an early investigation into the sustainability aspects of AI-generated code across three popular generative AI tools ChatGPT, BARD, and Copilot. The results highlight the default non-green behavior of tools for generating code, across multiple rules and scenarios. It underscores the need for further in-depth investigations and effective remediation strategies."
491,2022,"From captain to button-presser: operators' perspectives on navigating highly automated ferries nan Teaming with Artificial Intelligence (AI) is changing the way seafarers work. We show that a new kind of seafaring is emerging, characterized by cooperation with AI systems. In this format of seafaring, navigation tasks are controlled automatically while human operators manage the automation, always in the loop and ready to take over control if necessary. Ideally, this arrangement sees improvements in overall system performance and safety. However, little is known today about how this format of work will unfold in real-world operations. We investigate this topic by interviewing operators (n = 5) on ferries recently outfitted with state-of-the-art automated navigation technology. We used a mixed-methods approach to analyze the case study interviews, combining quantitative text analysis with Grounded Theory qualitative analysis techniques. The results show that operators perceived a shift towards a backup role coincident with increasing agency of machine autonomy. This role shift was characterized by button-pressing to start the machine autonomy and subsequently intervening to stop it when things go wrong. We observed that this shift led to boredom, deskilling, stretched resources, and compromised organizational harmony - effects running counter to the intention of improved system performance and safety. Synthesizing the findings, we present (1) effects across three operational dimensions: (i) tasks, (ii) human-computer interaction, (iii) organization; and (2) a definition of collaborative (human-AI) seafaring. By identifying issues in the early implementation of highly automated ship navigation, we hope to guide designers of Maritime Autonomous Surface Ships (MASSs) away from potential pitfalls and towards development more in tune with real-world demands of collaborative work."
492,2024,"Navigating Autonomy: Unveiling Security Experts' Perspectives on Augmented Intelligence in Cybersecurity nan The rapidly evolving cybersecurity threat landscape and shortage of skilled professionals are amplifying the need for technical support. AI tools offer great opportunities to support security experts by augmenting their intelligence and allowing them to focus on their unique human skills and expertise. For the successful design of AI tools and expert-AI interfaces, however, it is essential to understand the specialised security-critical context and the experts' requirements. To this end, 27 in-depth interviews with security experts, mostly in high-level managerial roles, were conducted and analysed using a grounded theory approach. The interviews showed that experts assigned tasks to AI, humans, or the human-AI team according to the skills they attributed to them. However, deciding how autonomously an AI tool should be able to perform tasks is a challenge that requires experts to weigh up factors such as trust, type of task, benefits, and risks. The resulting decision framework enhances understanding of the interplay between trust in AI, especially influenced by its transparency, and different levels of autonomy. As these factors affect the adoption of AI and the success of expert-AI collaboration in cybersecurity, it is important to further investigate them in the context of experts' AI-related decision-making processes."
493,2023,"Comment nan When CHATGPT launched last year, it took the tech world by storm due to the role it can play both as a smart coding assistant and, more generally, to help speed up the writing process for people in almost any walk of life. Since then, generative AI systems for a variety of uses have gained fans the world over. However, there are some potential legal issues on the horizon.In the US, an AI-based software platform owned by Microsoft - GitHub Copilot - and OpenAI, which supplied the open-source code for training purposes, are facing a class-action lawsuit over alleged copyright infringement. The case was filed by programmer and lawyer Matthew Butterick and several anonymous members of the open-source community."
494,2024,"The Role of Generative AI Tools in Application Development: A Comprehensive Review of Current Technologies and Practices nan This paper provides a comprehensive review of the role of Generative AI (GenAI) tools in modern software application development. It highlights the advancements in machine learning, natural language processing, and neural network architectures that have enabled the evolution of GenAI tools from basic code generation and assistance to fully autonomous software development capabilities. Key GenAI tools like GitHub Copilot, TabNine, Cursor AI and Devin AI are examined, with a focus on their functionalities such as code generation, testing, debugging, and deployment. The paper also discusses the productivity benefits, limitations, and ethical considerations of integrating GenAI tools into the development lifecycle. The aim is to equip software developers, managers, and organizations with a deep understanding of the practical applications and future directions of GenAI in streamlining development processes."
495,2024,"AI-Powered Western Blot Interpretation: A Novel Approach to Studying the Frameshift Mutant of Ubiquitin B (UBB+1) in Schizophrenia nan The application of artificial intelligence (AI) in the analysis of molecular biology data is becoming increasingly widespread. The Western Blot (WB) technique, a cornerstone in proteomic research, facilitates the identification and analysis of proteins, such as the frameshift mutant of ubiquitin B (UBB+1). In our study, we attempted to assess the potential of four different AI models-Gemini, Gemini Advanced, Microsoft Copilot, and ChatGPT 4-in the analysis of WB imagery containing UBB+1, derived from peripheral blood studies of patients suffering from schizophrenia. Participants, all male and diagnosed with schizophrenia, were recruited from the Specialist Psychiatric Care Team of Babinski Hospital in Lodz. After obtaining their informed consent, blood samples were collected and transported to the laboratory of the Department of Medical Biochemistry at the Medical University of Lodz. The samples were processed, synthesis of Ub-(UBB+1)-U-48 dimers was performed, and the WB technique was applied. The result of the WB analysis, in the form of a photograph with basic labels but without a legend (JPG format), was implemented into ChatGPT 4, Microsoft Copilot, Gemini and Gemini Advanced. Following the implementation of the image, the command 'Could you analyze the attached photo?' was added, along with the protocol from Sample Preparation and Synthesis of Ub-(UBB+1)-U-48 Dimers. The AI models effectively analyzed and interpreted WB images, with variations in their approaches and depth. Gemini excelled in detailing the WB process and biological significance of bands, while Gemini Advanced focused on specific band identification, especially Ub-(UBB+1)-U-48 dimers. Microsoft Copilot provided a basic overview with less technicality, and ChatGPT 4 offered comprehensive band interpretations, linking them to patient samples and standards, thus confirming the hypothesis about the differing capabilities of these models. This discovery demonstrates the advanced capabilities of ChatGPT 4 and highlights the growing role of AI in scientific research, including the interpretation of results."
496,2023,"Examining the Implications of Automaticity Theory in the Construction Industry nan This study investigated the development of automaticity during repetitive construction activities. Twenty- eight subjects were recruited to participate in a total of 22 trials of simulated roofing installations for one month in a laboratory. The performance metric considered in this research for feature-based diagnosis of automaticity development was the roofing task accuracy. The results revealed that the roofing task accuracy significantly improved with practice as the trial days progressed. Given that practitioners are interested in training workers to achieve automaticity to increase their productivity and multi-tasking skills, the results of this study provide measures to test training effectiveness and the extent to which workers have developed automaticity. Also, by better understanding the model of humans, this study's results will help improve human-AI teaming as the AI will better understand the cognitive state of its human counterpart and can adapt to him/her more accurately."
497,2024,"Pioneering Research on a Neurodiverse ADHD Workforce in the Future Construction Industry nan While incorporating novel technologies aims to facilitate the inclusion of the future construction industry, the empirical investigation of diverse workers' performance is necessary to better understand the strategies to broaden their participation. The literature has rarely focused on inclusivity in terms of involving workers with various disabilities in the construction sector. Thus, this study aims to pioneer the discussion on inclusivity in construction by focusing on workers with attention-deficit/hyperactivity disorder (ADHD). A future bricklaying worker-AI collaborative task was developed to examine workers' attentional allocation, situational awareness (SA), and productivity in future works. The results indicated that ADHD workers exhibited a lower SA of dynamic objects by focusing on the primary task with no increase in productivity, leading to lower overall performance. This study provided insights into (1) understanding ADHD workers' performance on human-AI teaming of future jobsites, and (2) proposing strategies to facilitate inclusivity in the future construction industry."
498,2017,"Super-Human AI for Strategic Reasoning: Beating Top Pros in Heads-Up No-Limit Texas Hold'em nan Poker has been a challenge problem in AI and game theory for decades. As a game of imperfect information it involves obstacles not present in games like chess and Go, and requires totally different techniques. No program had been able to beat top players in large poker games. Until now! In January 2017, our AI, Libratus, beat a team of four top specialist professionals in headsup no-limit Texas hold'em, which has 10(161) decision points. This game is the main benchmark challenge for imperfect-information game solving. Libratus is the only AI that has beat top humans at this game. Libratus is powered by new algorithms in each of its three main modules:1. computing blueprint (approximate Nash equilibrium) strategies before the event,2. novel nested endgame solving during play, and3. fixing its own strategy to play even closer to equilibrium based on what holes the opponents have been able to identify and exploit.These domain-independent algorithms have potential applicability to a variety of real-world imperfect-information games such as negotiation, business strategy, cybersecurity, physical security, military applications, strategic pricing, product portfolio planning, certain areas of finance, auctions, political campaigns, and steering biological adaptation and evolution, for example, for medical treatment planning."
499,2024,"Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow nan Generative AI (GenAI) has fundamentally changed how knowledge workers, such as software developers, solve tasks and collaborate to build software products. Introducing innovative tools like ChatGPT and Copilot has created new opportunities to assist and augment software developers across various problems. We conducted an empirical study involving interviews with 13 data scientists, managers, developers, designers, and frontend developers to investigate the usage of GenAI. Our study reveals that ChatGPT signifies a paradigm shift in the workflow of software developers. The technology empowers developers by enabling them to work more efficiently, speed up the learning process, and increase motivation by reducing tedious and repetitive tasks. Moreover, our results indicate a change in teamwork collaboration due to software engineers using GenAI for help instead of asking co-workers which impacts the learning loop in agile teams."
500,2024,"Can artificial intelligence models serve as patient information consultants in orthodontics? nan Background To evaluate the accuracy, reliability, quality, and readability of responses generated by ChatGPT-3.5, ChatGPT-4, Gemini, and Copilot in relation to orthodontic clear aligners. Methods Frequently asked questions by patients/laypersons about clear aligners on websites were identified using the Google search tool and these questions were posed to ChatGPT-3.5, ChatGPT-4, Gemini, and Copilot AI models. Responses were assessed using a five-point Likert scale for accuracy, the modified DISCERN scale for reliability, the Global Quality Scale (GQS) for quality, and the Flesch Reading Ease Score (FRES) for readability. Results ChatGPT-4 responses had the highest mean Likert score (4.5 +/- 0.61), followed by Copilot (4.35 +/- 0.81), ChatGPT-3.5 (4.15 +/- 0.75) and Gemini (4.1 +/- 0.72). The difference between the Likert scores of the chatbot models was not statistically significant (p > 0.05). Copilot had a significantly higher modified DISCERN and GQS score compared to both Gemini, ChatGPT-4 and ChatGPT-3.5 (p < 0.05). Gemini's modified DISCERN and GQS score was statistically higher than ChatGPT-3.5 (p < 0.05). Gemini also had a significantly higher FRES compared to both ChatGPT-4, Copilot and ChatGPT-3.5 (p < 0.05). The mean FRES was 38.39 +/- 11.56 for ChatGPT-3.5, 43.88 +/- 10.13 for ChatGPT-4 and 41.72 +/- 10.74 for Copilot, indicating that the responses were difficult to read according to the reading level. The mean FRES for Gemini is 54.12 +/- 10.27, indicating that Gemini's responses are more readable than other chatbots. Conclusions All chatbot models provided generally accurate, moderate reliable and moderate to good quality answers to questions about the clear aligners. Furthermore, the readability of the responses was difficult. ChatGPT, Gemini and Copilot have significant potential as patient information tools in orthodontics, however, to be fully effective they need to be supplemented with more evidence-based information and improved readability."
501,2024,"Can AI Answer My Questions? Utilizing Artificial Intelligence in the Perioperative Assessment for Abdominoplasty Patients nan Background Abdominoplasty is a common operation, used for a range of cosmetic and functional issues, often in the context of divarication of recti, significant weight loss, and after pregnancy. Despite this, patient-surgeon communication gaps can hinder informed decision-making. The integration of large language models (LLMs) in healthcare offers potential for enhancing patient information. This study evaluated the feasibility of using LLMs for answering perioperative queries.Methods This study assessed the efficacy of four leading LLMs-OpenAI's ChatGPT-3.5, Anthropic's Claude, Google's Gemini, and Bing's CoPilot-using fifteen unique prompts. All outputs were evaluated using the Flesch-Kincaid, Flesch Reading Ease score, and Coleman-Liau index for readability assessment. The DISCERN score and a Likert scale were utilized to evaluate quality. Scores were assigned by two plastic surgical residents and then reviewed and discussed until a consensus was reached by five plastic surgeon specialists.Results ChatGPT-3.5 required the highest level for comprehension, followed by Gemini, Claude, then CoPilot. Claude provided the most appropriate and actionable advice. In terms of patient-friendliness, CoPilot outperformed the rest, enhancing engagement and information comprehensiveness. ChatGPT-3.5 and Gemini offered adequate, though unremarkable, advice, employing more professional language. CoPilot uniquely included visual aids and was the only model to use hyperlinks, although they were not very helpful and acceptable, and it faced limitations in responding to certain queries.Conclusion ChatGPT-3.5, Gemini, Claude, and Bing's CoPilot showcased differences in readability and reliability. LLMs offer unique advantages for patient care but require careful selection. Future research should integrate LLM strengths and address weaknesses for optimal patient education.Level of Evidence V This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266."
502,2024,"Robust Testing of AI Language Model Resiliency with Novel Adversarial Prompts nan In the rapidly advancing field of Artificial Intelligence (AI), this study presents a critical evaluation of the resilience and cybersecurity efficacy of leading AI models, including ChatGPT-4, Bard, Claude, and Microsoft Copilot. Central to this research are innovative adversarial prompts designed to rigorously test the content moderation capabilities of these AI systems. This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses. Additionally, the research spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation. The empirical results from this investigation are pivotal for assessing AI models' current robustness and security. They highlight the necessity for ongoing development and meticulous testing to bolster AI defenses against various adversarial challenges. Notably, this study also delves into the ethical and societal implications of employing advanced jailbreak techniques in AI testing. The findings are significant for understanding AI vulnerabilities and formulating strategies to enhance AI technologies' reliability and ethical soundness, paving the way for safer and more secure AI applications."
503,2024,"Trust and Reliance in Evolving Human-AI Workflows (TREW) nan State-of-the-art AIs, including Large Language Models (LLMs) like GPT-4, now possess capabilities once unique to humans, such as coding, idea generation, and planning. Advanced AIs are now integrated into a plethora of platforms and tools, including GitHub Copilot, Bing Chat, Bard, ChatGPT, and Advanced Data Analytics. In contrast to conventional, specialized AIs that typically offer singular solutions, these LLMs redefine human-AI dynamics, with a growing trend toward humans viewing them as collaborative counterparts. This shift leads to enhanced dialogues, negotiations, and task delegation between humans and AI. With these rapid advancements, the nature of human roles in the AI collaboration spectrum is evolving. While our previous workshops CHI TRAIT 2022 and 2023 delved into the trust and reliance concerning traditional AIs, the pressing question now is: how should we measure trust and reliance with these emerging AI technologies? As these systems witness widespread adoption, there's also a need to assess their impact on human skill development. Does AI assistance amplify human skill progression, or does it inadvertently inhibit it? Considering the multifaceted challenges and solutions that revolve around human-AI interactions, we invite experts from diverse fields, including HCI, AI, ML, psychology, and social science. Our aim is to bridge communication gaps and facilitate rich collaborations across these domains."
504,2023,"Commands as AI Conversations [arXiv] nan Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? ai-cli, an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, ai-cli transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The ai-cli tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platform applicability."
505,2024,"CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging LLMs' (Lack of) Multicultural Knowledge nan Frontier large language models (LLMs) are developed by researchers and practitioners with skewed cultural backgrounds and on datasets with skewed sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively assessed with current methods for developing benchmarks. Existing multicultural evaluations primarily rely on expensive and restricted human annotations or potentially outdated internet resources. Thus, they struggle to capture the intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks are promising, yet risk propagating the same biases they are meant to measure. To synergize the creativity and expert cultural knowledge of human annotators and the scalability and standardizability of LLM-based automation, we introduce CulturalTeaming, an interactive red-teaming system that leverages human-AI collaboration to build truly challenging evaluation dataset for assessing the multicultural knowledge of LLMs, while improving annotators' capabilities and experiences. Our study reveals that CulturalTeaming's various modes of AI assistance support annotators in creating cultural questions, that modern LLMs fail at, in a gamified manner. Importantly, the increased level of AI assistance (e.g., LLM-generated revision hints) empowers users to create more difficult questions with enhanced perceived creativity of themselves, shedding light on the promises of involving heavier AI assistance in modern evaluation dataset creation procedures. Through a series of 1-hour workshop sessions, we gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with users' red-teaming attempts, that different families of modern LLMs perform with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs' multicultural proficiency."
506,2023,"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow [arXiv] nan Various industries such as finance, meteorology, and energy generate vast amounts of heterogeneous data every day. There is a natural demand for humans to manage, process, and display data efficiently. However, it necessitates labor-intensive efforts and a high level of expertise for these data-related tasks. Considering that large language models (LLMs) have showcased promising capabilities in semantic understanding and reasoning, we advocate that the deployment of LLMs could autonomously manage and process massive amounts of data while displaying and interacting in a human-friendly manner. Based on this belief, we propose Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end. Acting like an experienced expert, Data-Copilot autonomously transforms raw data into visualization results that best match the user's intent. Specifically, Data-Copilot autonomously designs versatile interfaces (tools) for data management, processing, prediction, and visualization. In real-time response, it automatically deploys a concise workflow by invoking corresponding interfaces step by step for the user's request. The interface design and deployment processes are fully controlled by Data-Copilot itself, without human assistance. Besides, we create a Data-Copilot demo that links abundant data from different domains (stock, fund, company, economics, and live news) and accurately respond to diverse requests, serving as a reliable AI assistant."
507,2023,"On the Design of AI-powered Code Assistants for Notebooks nan AI-powered code assistants, such as Copilot, are quickly becoming a ubiquitous component of contemporary coding contexts. Among these environments, computational notebooks, such as Jupyter, are of particular interest as they provide rich interface afordances that interleave code and output in a manner that allows for both exploratory and presentational work. Despite their popularity, little is known about the appropriate design of code assistants in notebooks. We investigate the potential of code assistants in computational notebooks by creating a design space (reifed from a survey of extant tools) and through an interview-design study (with 15 practicing data scientists). Through this work, we identify challenges and opportunities for future systems in this space, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specifc tools (like linters), and the importance of polite assistants."
508,2023,"An Infinity of Pong: A Raspberry Pi Pico W handheld writes its own games nan There is currently a lot of interest in AI tools designed to help programmers write software. GitHub's Copilot and Amazon's CodeWhisperer apply deep-learning techniques originally developed for generating natural-language text by adapting it to generate source code. The idea is that programmers can use these tools as a kind of auto-complete on steroids, using prompts to produce chunks of code that developers can integrate into their software."
509,2023,"Social AI and the Challenges of the Human-AI Ecosystem [arXiv] nan The rise of large-scale socio-technical systems in which humans interact with artificial intelligence (AI) systems (including assistants and recommenders, in short AIs) multiplies the opportunity for the emergence of collective phenomena and tipping points, with unexpected, possibly unintended, consequences. For example, navigation systems' suggestions may create chaos if too many drivers are directed on the same route, and personalised recommendations on social media may amplify polarisation, filter bubbles, and radicalisation. On the other hand, we may learn how to foster the wisdom of crowds and collective action effects to face social and environmental challenges. In order to understand the impact of AI on socio-technical systems and design next-generation AIs that team with humans to help overcome societal problems rather than exacerbate them, we propose to build the foundations of Social AI at the intersection of Complex Systems, Network Science and AI. In this perspective paper, we discuss the main open questions in Social AI, outlining possible technical and scientific challenges and suggesting research avenues."
510,2023,"GeoDeepShovel: A platform for building scientific database from geoscience literature with AI assistance nan With the rapid development of big data science, the research paradigm in the field of geosciences has also begun to shift to big data-driven scientific discovery. Researchers need to read a huge amount of literature to locate, extract and aggregate relevant results and data that are published and stored in PDF format for building a scientific database to support the big data-driven discovery. In this paper, based on the findings of a study about how geoscientists annotate literature and extract and aggregate data, we proposed GeoDeepShovel, a publicly available AI-assisted data extraction system to support their needs. GeoDeepShovel leverages state-of-the-art neural network models to support researcher(s) easily and accurately annotate papers (in the PDF format) and extract data from tables, figures, maps, etc., in a human-AI collaboration manner. As a part of the Deep-Time Digital Earth (DDE) program, GeoDeepShovel has been deployed for 8 months, and there are already 400 users from 44 geoscience research teams within the DDE program using it to construct scientific databases on a daily basis, and more than 240 projects and 50,000 documents have been processed for building scientific databases."
511,2022,"Computer copilots for endoscopic diagnosis nan Artificial intelligence (AI) tools for endoscopy are now entering clinical practice after demonstrating substantial improvements to polyp detection on colonoscopy. As this technology continues to mature, efforts to develop and validate a new frontier of possibilities-including diagnostic classification, risk stratification, and clinical outcomes assessment-are now underway. In npj Digital Medicine, scientists from Cosmo AI/Linkverse and collaborators report an extension to the first FDA-cleared AI tool for colonoscopy that goes beyond polyp detection to enable video-based diagnostic characterization."
512,2023,"PCR-Chain: Partial Code Reuse Assisted by Hierarchical Chaining of Prompts on Frozen Copilot nan API documentation, technical blogs and programming Q&A sites contain a large amount of partial code that can be reused in programming tasks. However, due to unresolved simple names and last-mile syntax errors, such partial code is frequently not compilable. To facilitate partial code reuse, we develop PCR-Chain for resolving FQNs and fixing last-mile syntax errors in partial code based on a giant pre-trained code model (e.g., Copilot). Methodologically, PCR-Chain is backed up by the underlying global-level prompt architecture (which combines three design ideas: hierarchical task breakdown, prompt composition including sequential and conditional structures, and a mix of prompt-based AI and non-AI units) and the locallevel prompt design. Technically, we propose PCR-Chain, which employs in-context learning rather than supervised fine-tuning with gradient updates on downstream task data. This approach enables the frozen, giant pre-trained code model to learn the desired behavior for a specific task through behavior-describing prompts and imitate it to complete the task. Experimental results show that PCR-Chain automatically resolves the FQNs and fixes last-mile syntax errors in 50 partial code samples collected from Stack Overflow with high success rates, without requiring any program analysis. The correct execution of the unit, module, and PCR-Chain demonstrates the effectiveness of the prompt design, prompt composition, and prompt architecture."
513,2023,"How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study [arXiv] nan Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision-making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts' workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts' preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants."
514,2024,"Communicating AI for Architectural and Interior Design: Reinterpreting Traditional Iznik Tile Compositions through AI Software for Contemporary Spaces nan Artificial intelligence (AI), which has a strong potential to assist architects in conceptual and visualization stages, has been increasingly used in the field of design and architecture. This study, focusing on the AI tools that generate images from texts and offer innovative solutions to design problems, aims to evaluate the use of AI for the reinterpretation of traditional Iznik tile patterns and colors in the context of architectural design and modern interiors. The methodology consists of four stages, which are the selection of AI tools (Copilot, DALL-E 2, DALL-E 3, Midjourney), the preparation of textual prompts for testing & ccedil;ini (tile) expression, testing of the AI tools' perception of the concepts related to Iznik tile motifs, and the creation of prompt series. The findings of our study provide evidence that current AI tools exhibit distinct features in terms of variety, conceptualization, artistic visualization, and image production, while they are hardly equipped with the necessary conceptual background to communicate with the designers for the interpretation of the traditional Iznik tiles in contemporary architectural design. Specifically, Midjourney, which could produce historically referenced contemporary designs in response to textual expressions, was more successful than other AI tools. DALL-E 2 could not visualize the expressions concerning the placement of the Iznik tile surfaces in interior spaces but was quite inspiring in terms of the images regarding the tile pattern and color. DALL-E 3 and Copilot tools produced similar images in terms of color palette and patterns, whereas DALL-E 3 was better at visualizing spatial data. Our results reveal that AI tools still need to be developed for analyzing traditional patterns, styles, and forms for contemporary design purposes. On the other hand, AI tools can develop innovative approaches, optimize the tile production procedure, and have the potential to accelerate the design process for designers by generating new and diverse ideas."
515,2024,"Towards AI-Native Software Engineering (SE 3.0): A Vision and a Challenge Roadmap nan The rise of AI-assisted software engineering (SE 2.0), powered by Foundation Models (FMs) and FM-powered copilots, has shown promise in improving developer productivity. However, it has also exposed inherent limitations, such as cognitive overload on developers and inefficiencies. We propose a shift towards Software Engineering 3.0 (SE 3.0), an AI-native approach characterized by intent-first, conversation-oriented development between human developers and AI teammates. SE 3.0 envisions AI systems evolving beyond task-driven copilots into intelligent collaborators, capable of deeply understanding and reasoning about software engineering principles and intents. We outline the key components of the SE 3.0 technology stack, which includes Teammate.next for adaptive and personalized AI partnership, IDE.next for intent-first conversation-oriented development, Compiler.next for multi-objective code synthesis, and Runtime.next for SLA-aware execution with edge-computing support. Our vision addresses the inefficiencies and cognitive strain of SE 2.0 by fostering a symbiotic relationship between human developers and AI, maximizing their complementary strengths. We also present a roadmap of challenges that must be overcome to realize our vision of SE 3.0. This paper lays the foundation for future discussions on the role of AI in the next era of software engineering."
516,2024,"Impact of AI-tooling on the Engineering Workspace nan To understand the impacts of AI-driven coding tools on engineers' workflow and work environment, we utilize the Jellyfish platform to analyze indicators of change. Key indicators are derived from Allocations, Coding Fraction vs. PR Fraction, Lifecycle Phases, Cycle Time, Jira ticket size, PR pickup time, PR comments, PR comment count, interactions, and coding languages. Significant changes were observed in coding time fractions among Copilot users, with an average decrease of 3% with individual decreases as large as 15%. Ticket sizes decreased by an average of 16% across four companies, accompanied by an 8% decrease in cycle times, whereas the control group showed no change. Additionally, the PR process evolved with Copilot usage, featuring longer and more comprehensive comments, despite the weekly number of PRs reviewed remaining constant. Not all hypothesized changes were observed across all participating companies. However, some companies experienced a decrease in PR pickup times by up to 33%, indicating reduced workflow bottlenecks, and one company experienced a shift of up to 17% of effort from maintenance and support work towards product growth initiatives. This study is the first to utilize data from more than one company and goes beyond simple productivity and satisfaction measures, considering real-world engineering settings instead. By doing so, we highlight that some companies seem to benefit more than others from the use of Copilot and that changes can be subtle when investigating aggregates rather than specific aspects of engineering work and workflows - something that will be further investigated in the future."
517,2023,"ReadingQuizMaker: A Human-NLP Collaborative System that Supports Instructors to Design High-Quality Reading Quiz Questions nan Despite that reading assignments are prevalent, methods to encourage students to actively read are limited. We propose a system ReadingQuizMaker that supports instructors to conveniently design high-quality questions to help students comprehend readings. ReadingQuizMaker adapts to instructors' natural workflows of creating questions, while providing NLP-based process-oriented support. ReadingQuizMaker enables instructors to decide when and which NLP models to use, select the input to the models, and edit the outcomes. In an evaluation study, instructors found the resulting questions to be comparable to their previously designed quizzes. Instructors praised ReadingQuizMaker for its ease of use, and considered the NLP suggestions to be satisfying and helpful. We compared ReadingQuizMaker with a control condition where instructors were given automatically generated questions to edit. Instructors showed a strong preference for the human-AI teaming approach provided by ReadingQuizMaker. Our findings suggest the importance of giving users control and showing an immediate preview of AI outcomes when providing AI support."
518,2024,"Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making [arXiv] nan Artificial Intelligence (AI) is increasingly employed in various decision-making tasks, typically as a Recommender, providing recommendations that the AI deems correct. However, recent studies suggest this may diminish human analytical thinking and lead to humans' inappropriate reliance on AI, impairing the synergy in human-AI teams. In contrast, human advisors in group decision-making perform various roles, such as analyzing alternative options or criticizing decision-makers to encourage their critical thinking. This diversity of roles has not yet been empirically explored in AI assistance. In this paper, we examine three AI roles: Recommender, Analyzer, and Devil's Advocate, and evaluate their effects across two AI performance levels. Our results show each role's distinct strengths and limitations in task performance, reliance appropriateness, and user experience. Notably, the Recommender role is not always the most effective, especially if the AI performance level is low, the Analyzer role may be preferable. These insights offer valuable implications for designing AI assistants with adaptive functional roles according to different situations."
519,2024,"Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and Humans nan This chapter introduces a research project titled Analyzing the Political Discourse: A Collaboration Between Humans and Artificial Intelligence, which was initiated in preparation for Greece's 2023 general elections. The project focused on the analysis of political leaders' campaign speeches, employing Artificial Intelligence (AI), in conjunction with an interdisciplinary team comprising journalists, a political scientist, and data scientists. The chapter delves into various aspects of political discourse analysis, including sentiment analysis, polarization, populism, topic detection, and Named Entities Recognition (NER). This experimental study investigates the capabilities of large language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing political speech, evaluates its strengths and weaknesses, and highlights the essential role of human oversight in using AI in journalism projects and potentially other societal sectors. The project stands as an innovative example of human-AI collaboration (known also as hybrid intelligence) within the realm of digital humanities, offering valuable insights for future initiatives."
520,2021,"How Clinicians Perceive Artificial Intelligence-Assisted Technologies in Diagnostic Decision Making: Mixed Methods Approach nan Background: With the rapid development of artificial intelligence (AI) and related technologies, AI algorithms are being embedded into various health information technologies that assist clinicians in clinical decision making.Objective: This study aimed to explore how clinicians perceive AI assistance in diagnostic decision making and suggest the paths forward for AI-human teaming for clinical decision making in health care.Methods: This study used a mixed methods approach, utilizing hierarchical linear modeling and sentiment analysis through natural language understanding techniques.Results: A total of 114 clinicians participated in online simulation surveys in 2020 and 2021. These clinicians studied family medicine and used AI algorithms to aid in patient diagnosis. Their overall sentiment toward AI-assisted diagnosis was positive and comparable with diagnoses made without the assistance of AI. However, AI-guided decision making was not congruent with the way clinicians typically made decisions in diagnosing illnesses. In a quantitative survey, clinicians reported perceiving current AI assistance as not likely to enhance diagnostic capability and negatively influenced their overall performance (beta=-0.421, P=.02). Instead, clinicians' diagnostic capabilities tended to be associated with well-known parameters, such as education, age, and daily habit of technology use on social media platforms.Conclusions: This study elucidated clinicians' current perceptions and sentiments toward AI-enabled diagnosis. Although the sentiment was positive, the current form of AI assistance may not be linked with efficient decision making, as AI algorithms are not well aligned with subjective human reasoning in clinical diagnosis. Developers and policy makers in health could gather behavioral data from clinicians in various disciplines to help align AI algorithms with the unique subjective patterns of reasoning that humans employ in clinical diagnosis."
521,2024,"User-centric AI: evaluating the usability of generative AI applications through user reviews on app stores nan This article presents a usability evaluation and comparison of generative AI applications through the analysis of user reviews from popular digital marketplaces, specifically Apple's App Store and Google Play. The study aims to bridge the research gap in real-world usability assessments of generative AI tools. A total of 11,549 reviews were extracted and analyzed from January to March 2024 for five generative AI apps: ChatGPT, Bing AI, Microsoft Copilot, Gemini AI, and Da Vinci AI. The dataset has been made publicly available, allowing for further analysis by other researchers. The evaluation follows ISO 9241 usability standards, focusing on effectiveness, efficiency, and user satisfaction. This study is believed to be the first usability evaluation for generative AI applications using user reviews across digital marketplaces. The results show that ChatGPT achieved the highest compound usability scores among Android and iOS users, with scores of 0.504 and 0.462, respectively. Conversely, Gemini AI scored the lowest among Android apps at 0.016, and Da Vinci AI had the lowest among iOS apps at 0.275. Satisfaction scores were critical in usability assessments, with ChatGPT obtaining the highest rates of 0.590 for Android and 0.565 for iOS, while Gemini AI had the lowest satisfaction rate at -0.138 for Android users. The findings revealed usability issues related to ease of use, functionality, and reliability in generative AI tools, providing valuable insights from user opinions and feedback. Based on the analysis, actionable recommendations were proposed to enhance the usability of generative AI tools, aiming to address identified usability issues and improve the overall user experience. This study contributes to a deeper understanding of user experiences and offers valuable guidance for enhancing the usability of generative AI applications."
522,2024,"Developing Critical Thinking Practices Interwoven with Generative AI usage in an Introductory Programming Course nan Software development has evolved significantly. In the past, developers were required to have comprehensive understanding of programming languages, algorithms, and computer architecture. However, with the emergence of the Internet, software libraries, frameworks, and forums became widely available, which utilize reusable software components that can reduce development time and costs. The advent of Generative Artificial Intelligence (AI) tools, such as ChatGPT, GitHub Copilot, and Amazon CodeWhisperer, has further enhanced the developer's toolkit, as these tools can be used for a wide variety of tasks such as code generation, documentation, commenting and reviewing. As programming is often slow and requires trial and error, novice programmers can be tempted to apply the first solution found on the Internet or proposed by an AI tool without much critical reflection or notion of responsibility. Hence, the advances of AI have raised both excitement and concerns among Information Technology (IT)/Computer Science (CS) students and educators. Yet, AI tools are here to stay, and students must learn to use them responsibly. The aim of this paper is to investigate how to design learning activities that introduce Generative AI tools (GitHub Copilot and ChatGPT) for programming while promoting critical thinking practices among students in an introductory programming course in the first semester. Students' opinions and customs were surveyed before and after the AI-based programming assignment. The results indicate that students' awareness of the possibilities and limitations of AI, as well as practices of critical thinking in programming increased. This is encouraging as critical thinking is an integral part of best programming practices."
523,2024,"Exploring the Efficacy of GenAI in Grading SQL Query Tasks: A Case Study nan Numerous techniques, including problem-solving, seeking clarification, and creating questions, have been employed to utilize generative Artificial Intelligence (AI) in education. This study investigates the possibility of using Generate AI (GenAI) to grade Structured Query Language (SQL) queries automatically. Three models were used which are ChatGPT, Gemini, and Copilot. The study uses an experimental approach to assess how well the models perform in evaluating student responses by comparing the models' accuracy with those of human experts. The results showed that despite some inconsistencies, GenAI holds great promise for streamlining. Thus, further research is required in light of inconsistent GenAI performance. If these issues were resolved, GenAI can be utilized in education. However, human oversight and ethical issues must always come first."
524,2023,"Commands as AI Conversations nan Developers and data scientists often struggle to write command-line inputs, even though graphical interfaces or tools like ChatGPT can assist. The solution? ai-cli, an open-source system inspired by GitHub Copilot that converts natural language prompts into executable commands for various Linux command-line tools. By tapping into OpenAI's API, which allows interaction through JSON HTTP requests, ai-cli transforms user queries into actionable command-line instructions. However, integrating AI assistance across multiple command-line tools, especially in open source settings, can be complex. Historically, operating systems could mediate, but individual tool functionality and the lack of a unified approach have made centralized integration challenging. The ai-cli tool, by bridging this gap through dynamic loading and linking with each program's Readline library API, makes command-line interfaces smarter and more user-friendly, opening avenues for further enhancement and cross-platform applicability."
525,2024,"It's Weird That it KnowsWhat I Want: Usability and Interactions with Copilot for Novice Programmers nan Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support and scaffold the novice programming experience."
526,2024,"Recommender System Powered by Large Language Models nan The rapid development of AI technologies has made machine interaction a daily norm, such as OpenAI's ChatGPT and Microsoft's Copilot, leading us to develop a personalized book recommender system utilizing Large Language Models (LLMs). By analyzing user behavior, employing natural language processing, and applying a fine-tuned recommender model, this system notably enhances the accuracy of its book suggestions and user satisfaction. Our comprehensive evaluations reveal that this framework significantly surpasses traditional models in delivering personalized content that aligns with users' unique reading preferences and dialogue histories. This research delves into the capabilities of LLMs to offer tailored book recommendations, highlighting the system's ability to synergize user data with book content for improved recommendation precision. It also examines user interactions with LLMs, offering valuable insights for future AI-driven recommender systems."
527,2018,"Artificial Intelligence and the Role of Leadership nan A future artificial intelligence (AI) leadership position will likely include a new follower, the AI machine. The continued rise in retired-aged individuals illustrates the need to replace a traditional workforce with alternatives. Robots tend to be non-functional without leaders. With this new position, AI leaders will engage in processes that focus on leading the programmers of the AI machine as well as influencing decisions made by AI machines post-programming. Communication standards should be set for both the human and machine members. It is unclear if this new position will be referred to as leadership or management, but standards should be in place for proper supervision of these machines. Leadership research has found that behaviors such as charismatic influences and relationship building are important for leading humans. AI leadership may require adjustments to current influences used for humans. Building relationships with AI machines is expected to be altered with greater focus needed on ethical and moral mentoring. Such a focus may utilize top-down and bottom-up roboethics, with decreased focus on getting machine followers to feel part of an in-group. The current paper provides ideas for leading teams of AI machines and their programmers."
528,2024,"Impacts of the Usage of Generative Artificial Intelligence on Software Development Process nan Context: Over the years, tools have been created to improve the execution of development process activities. The emergence of generative Artificial Intelligence (AI) and, more recently, the launch and dissemination of Copilot, ChatGPT-3 and other generative tools, have broadened the discussion about the possibility of using conversational generative AI tools in diverse development tasks. Problem: There is still a lack of secondary studies to map the literature about how software development process activities can be affected by the usage of generative AI tools. Solution: This study aims to identify in which activities of the software development process Natural Language (NL) generative AI tools have been used and how they can impact requirements specification, design/architecture, development and testing activities. IS Theory: The study was developed under the aegis of the Task Technology Fit theory. Method: This work presents the results of a Systematic Mapping Review (SMR) carried out to collect research results that investigate the application of generative AI tools in the software development process. Results: Results indicate that the main activities affected are development and testing and that, although there are still some issues to be addressed, there are benefits in using AI generative tools compared to using more traditional methods like human-human pair programming and code testing made by software engineering professionals. Contribution: It was possible to collect studies to identify in which activities of the software development process generative AI tools can be applied and what are the impacts of using this technology."
529,2023,"Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair nan During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful copilots in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a general code generation framework to further copilot the AI copilots (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthesizes a candidate patch through the interaction between an LLM and a Completion Engine, which 1) prunes away infeasible tokens suggested by the LLM and 2) proactively completes the token based on the suggestions provided by the Completion Engine. Our evaluation on a subset of the widely-used Defects4j 1.2 and 2.0 datasets shows that Repilot outperforms state-of-the-art techniques by fixing 27% and 47% more bugs, respectively. Moreover, Repilot produces more valid and correct patches than the base LLM with the same budget. While we focus on leveraging Repilot for APR in this work, the overall approach is also generalizable to other code generation tasks."
530,2023,"Search Evolves Into Copilots, Chatbots, and Research Assistants nan ChatGPT has monopolized the attention, not just of librarians, but also, seemingly, of every conscious person on the planet. Distinguishing between hype and reality is an ongoing adventure, as is the distinction between AI-generated content and search capabilities. The annual conference, librarians attending a huddle on generative AI, organized by Melissa Del Castillo, Florida International University, were mainly concerned about hallucinations. When considering generative AI, information professionals tend to be wary. They see value but worry about the misinformation it is all too prone to create. OpenAI, the creator of ChatGPT, recognizing the misinformation problem, shut down its AI classifier on July 20, 2023, due to its low rate of accuracy. ChatGPT is now available on Android devices, following its debut on iOS last May. Working with governments, the Frontier Model Forum recognizes that guardrails are necessary, given the immense potential of AI for both good and evil. It plans to identify best practices for safety standards and safety practices, advance AI safety research."
531,2021,"WILL HUMANS-IN-THE-LOOP BECOME BORGS? MERITS AND PITFALLS OF WORKING WITH AI nan We analyze how advice from an AI affects complementarities between humans and AI, in particular what humans know that an AI does not know: unique human knowledge. In a multi-method study consisting of an analytical model, experimental studies, and a simulation study, our main finding is that human choices converge toward similar responses improving individual accuracy. However, as overall individual accu-racy of the group of humans improves, the individual unique human knowledge decreases. Based on this finding, we claim that humans interacting with AI behave like Borgs, that is, cyborg creatures with strong individual performance but no human individuality. We argue that the loss of unique human knowledge may lead to several undesirable outcomes in a host of human-AI decision environments. We demonstrate this harmful impact on the wisdom of crowds. Simulation results based on our experimental data suggest that groups of humans interacting with AI are far less effective as compared to human groups without AI assistance. We suggest mitigation techniques to create environments that can provide the best of both worlds (e.g., by personalizing AI advice). We show that such interventions perform well individually as well as in wisdom of crowds settings."
532,2024,"Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization nan The rapid advancement of artificial intelligence systems has brought the challenge of AI alignment to the forefront of research, particularly in complex decision-making and task execution. As these systems surpass human-level performance in sophisticated problems, ensuring their alignment with human values, intentions, and ethical guidelines becomes crucial. Building on previous work in explanation generation for human-agent alignment, we address the more complex dynamics of multi-agent systems and human-AI teams. This paper introduces a novel approach to model alignment through weak-to-strong generalization in the context of language models. We present a framework where a strong model facilitates the improvement of a weaker model, bridging the gap between explanation generation and model alignment. Our method, formalized as a facilitation function, allows for the transfer of capabilities from advanced models to less capable ones without direct access to extensive training data. Our results suggest that this facilitation-based approach not only enhances model performance but also provides insights into the nature of model alignment and the potential for scalable oversight of AI systems."
533,2023,"Large Language Models and Simple, Stupid Bugs nan With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding prompt. Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase the possibility of producing known, verbatim fixes."
534,2024,"Operationalizing AI Explainability Using Interpretability Cues in the Cockpit: Insights from User-Centered Development of the Intelligent Pilot Advisory System (IPAS) nan This paper presents a concept for operationalizing Artificial Intelligence (AI) explainability for the Intelligent Pilot Advisory System (IPAS) as requested in the European Aviation Safety Agency's AI Roadmap 2.0 in order to meet the requirement of Trustworthy AI. The IPAS is currently being developed to provide AI-based decision support in commercial aircraft to assist the flight crew, especially in emergency situations. The development of the IPAS is following a user-centred and exploratory design approach, with the active involvement of airline pilots in the early stages of development to iteratively tailor the system to their requirements. The concept presented in this paper aims to provide interpretability cues to achieve operational explainability of AI, which should enable commercial aircraft pilots to understand and adequately trust the recommendations generated by AI when making decisions in emergencies. Focus of the research was to identify initial interpretability requirements and to answer the question of what interpretation cues pilots need from the AI-based system. Based on a user study with airline pilots, four requirements for interpretation cues were formulated. These results will form the basis for the next iteration of the IPAS, where the requirements will be implemented."
535,2022,"Invisible clinical labor driving the successful integration of AI in healthcare nan Artificial Intelligence and Machine Learning (AI/ML) tools are changing the landscape of healthcare decision-making. Vast amounts of data can lead to efficient triage and diagnosis of patients with the assistance of ML methodologies. However, more research has focused on the technological challenges of developing AI, rather than the system integration. As a result, clinical teams' role in developing and deploying these tools has been overlooked. We look to three case studies from our research to describe the often invisible work that clinical teams do in driving the successful integration of clinical AI tools. Namely, clinical teams support data labeling, identifying algorithmic errors and accounting for workflow exceptions, translating algorithmic output to clinical next steps in care, and developing team awareness of how the tool is used once deployed. We call for detailed and extensive documentation strategies (of clinical labor, workflows, and team structures) to ensure this labor is valued and to promote sharing of sociotechnical implementation strategies. Copyright  2022 Ulloa, Rothrock, Ahmad and Jacobs."
537,2023,"PwR: Exploring the Role of Representations in Conversational Programming [arXiv] nan Large Language Models (LLMs) have revolutionized programming and software engineering. AI programming assistants such as GitHub Copilot X enable conversational programming, narrowing the gap between human intent and code generation. However, prior literature has identified a key challenge--there is a gap between user's mental model of the system's understanding after a sequence of natural language utterances, and the AI system's actual understanding. To address this, we introduce Programming with Representations (PwR), an approach that uses representations to convey the system's understanding back to the user in natural language. We conducted an in-lab task-centered study with 14 users of varying programming proficiency and found that representations significantly improve understandability, and instilled a sense of agency among our participants. Expert programmers use them for verification, while intermediate programmers benefit from confirmation. Natural language-based development with LLMs, coupled with representations, promises to transform software development, making it more accessible and efficient."
538,2024,"Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI Collaboration [arXiv] nan Bayesian optimization (BO) with Gaussian processes (GP) has become an indispensable algorithm for black box optimization problems. Not without a dash of irony, BO is often considered a black box itself, lacking ways to provide reasons as to why certain parameters are proposed to be evaluated. This is particularly relevant in human-in-the-loop applications of BO, such as in robotics. We address this issue by proposing ShapleyBO, a framework for interpreting BO's proposals by game-theoretic Shapley values.They quantify each parameter's contribution to BO's acquisition function. Exploiting the linearity of Shapley values, we are further able to identify how strongly each parameter drives BO's exploration and exploitation for additive acquisition functions like the confidence bound. We also show that ShapleyBO can disentangle the contributions to exploration into those that explore aleatoric and epistemic uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human machine interface (HMI), allowing users to interfere with BO in case proposals do not align with human reasoning. We demonstrate this HMI's benefits for the use case of personalizing wearable robotic devices (assistive back exosuits) by human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO can achieve lower regret than teams without."
539,2023,"Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue nan Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the partner's missing knowledge is a more viable approach than predicting one's own. We show that explicit modeling of the partner's dialogue moves and mental states produces improved and more stable results than without. These results provide insight for future AI agents that can predict what knowledge their partner is missing and, therefore, can proactively communicate such information to help their partner acquire such missing knowledge toward a common understanding of joint tasks."
540,2023,Triple Helix: AI-Artist-Audience Collaboration in a Performative Art Experience nan nan
541,2024,"How Do Information Technology Professionals Use Generative Artificial Intelligence? nan Context: The emergence of generative Artificial Intelligence (AI) and, more recently, the dissemination of Copilot, ChatGPT-3 and similar tools have broadened the discussion about the possibility of using generative AI tools in many professional segments such as health, education, and technological area. Problem: Although some studies explore the potential of generative AI tools to assist Information Technology (IT) professionals in executing specific tasks, they do not delve into the professionals' characteristics or collect information about multiple generative AI tools usage. Solution: Considering the possibilities brought by generative AI, this study aims to shed light on the perception of IT professionals about generative AI tools and characterize these professionals' profiles. IS Theory: This research is based on the Technology Acceptance Model. Method: A survey research was carried out with IT professionals so as to identify how these professionals are using generative AI and gather information about these professionals' profiles. Results: Results show that 70,5% (43 out of 61) of the respondents use some generative AI tool, the majority of whom are software development professionals, and, despite the problems faced when using these tools, 86% of these professionals recommend using them. Contribution: In this study the profile of the IT professionals using generative AI was identified, it was then possible to evaluate the acceptance of such tools among these professionals and identify the main reasons why some of them are not yet using generative AI."
542,2024,"Assessing the Accuracy of Artificial Intelligence Models in Scoliosis Classification and Suggested Therapeutic Approaches nan Background: Open-source artificial intelligence models (OSAIMs) are increasingly being applied in various fields, including IT and medicine, offering promising solutions for diagnostic and therapeutic interventions. In response to the growing interest in AI for clinical diagnostics, we evaluated several OSAIMs-such as ChatGPT 4, Microsoft Copilot, Gemini, PopAi, You Chat, Claude, and the specialized PMC-LLaMA 13B-assessing their abilities to classify scoliosis severity and recommend treatments based on radiological descriptions from AP radiographs. Methods: Our study employed a two-stage methodology, where descriptions of single-curve scoliosis were analyzed by AI models following their evaluation by two independent neurosurgeons. Statistical analysis involved the Shapiro-Wilk test for normality, with non-normal distributions described using medians and interquartile ranges. Inter-rater reliability was assessed using Fleiss' kappa, and performance metrics, like accuracy, sensitivity, specificity, and F1 scores, were used to evaluate the AI systems' classification accuracy. Results: The analysis indicated that although some AI systems, like ChatGPT 4, Copilot, and PopAi, accurately reflected the recommended Cobb angle ranges for disease severity and treatment, others, such as Gemini and Claude, required further calibration. Particularly, PMC-LLaMA 13B expanded the classification range for moderate scoliosis, potentially influencing clinical decisions and delaying interventions. Conclusions: These findings highlight the need for the continuous refinement of AI models to enhance their clinical applicability."
543,2023,"Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review nan This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming capabilities to Apple's Xcode for mobile software development. This paper also presents the challenges of and opportunities for incorporating NLP techniques with software naturalness, empowering developers with advanced coding assistance and streamlining the software development process."
544,2023,"On the Design of AI-powered Code Assistants for Notebooks nan AI-powered code assistants, such as Copilot, are quickly becoming a ubiquitous component of contemporary coding contexts. Among these environments, computational notebooks, such as Jupyter, are of particular interest as they provide rich interface affordances that interleave code and output in a manner that allows for both exploratory and presentational work. Despite their popularity, little is known about the appropriate design of code assistants in notebooks. We investigate the potential of code assistants in computational notebooks by creating a design space (reified from a survey of extant tools) and through an interview-design study (with 15 practicing data scientists). Through this work, we identify challenges and opportunities for future systems in this space, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specific tools (like linters), and the importance of polite assistants."
545,2024,"Can large language models provide accurate and quality information to parents regarding chronic kidney diseases? nan RationaleArtificial Intelligence (AI) large language models (LLM) are tools capable of generating human-like text responses to user queries across topics. The use of these language models in various medical contexts is currently being studied. However, the performance and content quality of these language models have not been evaluated in specific medical fields.Aims and objectivesThis study aimed to compare the performance of AI LLMs ChatGPT, Gemini and Copilot in providing information to parents about chronic kidney diseases (CKD) and compare the information accuracy and quality with that of a reference source.MethodsIn this study, 40 frequently asked questions about CKD were identified. The accuracy and quality of the answers were evaluated with reference to the Kidney Disease: Improving Global Outcomes guidelines. The accuracy of the responses generated by LLMs was assessed using F1, precision and recall scores. The quality of the responses was evaluated using a five-point global quality score (GQS).ResultsChatGPT and Gemini achieved high F1 scores of 0.89 and 1, respectively, in the diagnosis and lifestyle categories, demonstrating significant success in generating accurate responses. Furthermore, ChatGPT and Gemini were successful in generating accurate responses with high precision values in the diagnosis and lifestyle categories. In terms of recall values, all LLMs exhibited strong performance in the diagnosis, treatment and lifestyle categories. Average GQ scores for the responses generated were 3.46 +/- 0.55, 1.93 +/- 0.63 and 2.02 +/- 0.69 for Gemini, ChatGPT 3.5 and Copilot, respectively. In all categories, Gemini performed better than ChatGPT and Copilot.ConclusionAlthough LLMs provide parents with high-accuracy information about CKD, their use is limited compared with that of a reference source. The limitations in the performance of LLMs can lead to misinformation and potential misinterpretations. Therefore, patients and parents should exercise caution when using these models."
546,2024,"Knowledge Prompting: How Knowledge Engineers Use Large Language Models nan Despite many advances in knowledge engineering (KE), challenges remain in areas such as engineering knowledge graphs (KGs) at scale, keeping up with evolving domain knowledge, multilingualism, and multimodality. Recently, KE has used LLMs to support semi-automatic tasks, but the most effective use of LLMs to support knowledge engineers across the KE activites is still in its infancy. To explore the vision of LLM copilots for KE and change existing KE practices, we conducted a multimethod study during a KE hackathon. We investigated participants' views on the use of LLMs, the challenges they face, the skills they may need to integrate LLMs into their practices, and how they use LLMs responsibly. We found participants felt LLMs could contribute to improving efficiency when engineering KGs, but presented increased challenges around the already complex issues of evaluating the KE tasks. We discovered prompting to be a useful but undervalued skill for knowledge engineers working with LLMs, and note that natural language processing skills may become more relevant across more roles in KG construction. Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI. Given the limited ethical training, most knowledge engineers receive solutions such as our suggested `KG cards' based on data cards could be a useful guide for KG construction. Our findings can support designers of KE AI copilots, KE researchers, and practitioners using advanced AI to develop trustworthy applications, propose new methodologies for KE and operate new technologies responsibly."
547,2024,"Software Development and Education: Transitioning Towards AI Enhanced Teaching nan This paper investigates the impact of large language model (LLM) AI tools, such as ChatGPT and Copilot, on software development education, focusing on usability, efficiency, and effectiveness in real-world scenarios. The research employs a quantitative approach, utilizing a survey of 50 software developers with varying levels of experience. Preliminary findings suggest that AI tools have a positive influence on expediting coding tasks and automating text generation, particularly in the early stages of product development. Challenges related to customization, accuracy, and transparency, as well as concerns about their potential impacts on employment, personal privacy, and ethical boundaries, have been identified. Pointers and initial recommendations for transitioning to AI-enhanced teaching and optimizing interactions between learners and generative AI practices are provided."
548,2024,"Towards Education 4.0: The role of Large Language Models as virtual tutors in chemical engineering nan Recent years have seen the rise of Artificial Intelligence (AI) powered generative chatbots, such as OpenAI's ChatGPT or Microsoft's Copilot. These tools have simultaneously positively surprised and taken aback people worldwide, raising the question of whether they can or should be used in education, as well as how to properly guide students and teachers on using them safely and ethically. To this end, this work provides (i) a brief overview of the current applications of AI in Higher Education (HE), (ii) a discussion of the ethical and societal concerns associated with the usage of AI models, (iii) the initial steps of the implementation of a chatbot used at the Technical University of Denmark (DTU) able to perform audits for Good Manufacturing Practice (GMP), and (iv) an investigation of the need and opportunities of AI in chemical engineering education. The latter is achieved through quantitative and qualitative analyses of the responses given by both Master's students and academia/ industry practitioners on the introduction and use of AI in education. This paves the way for discussing current perceptions, expectations and concerns of AI models, as well as their limitations and the opportunities they could provide."
549,2023,"Six Opportunities for Scientists and Engineers to Learn Programming Using AI Tools Such as ChatGPT nan This article demonstrates how scientists and engineers can use modern artificial intelligence (AI) tools such as ChatGPT and GitHub Copilot to learn computer programming skills that are relevant to their jobs. It begins by summarizing common ways that AI tools can already help people learn programming in general and then presents six new opportunities catered to the needs of scientists and engineers: 1) create customized programming tutorials for one's own domain of work, 2) learn complex data visualization libraries, 3) learn to refactor exploratory code into more maintainable software, 4) learn about inherited legacy code, 5) learn new programming languages on demand within the context of one's workflow, and 6) question the assumptions that one's scientific code is making. Taken together, these opportunities point toward a future where AI can help scientists and engineers learn programming on demand within the context of their existing real-world workflows."
550,2024,Are Prompt Engineering and TODO Comments Friends or Foes? An Evaluation on GitHub Copilot nan nan
551,2023,"Single-case learning analytics: Feasibility of a human-centered analytics approach to support doctoral education nan Recent advances in machine learning and natural language processing have the potential to transform human activity in many domains. The field of learning analytics has applied these techniques successfully to many areas of education but has not been able to permeate others, such as doctoral education. Indeed, doctoral education remains an under-researched area with widespread problems (high dropout rates, low mental well-being) and lacks technological support beyond very specialized tasks. The inherent uniqueness of the doctoral journey may help explain the lack of generalized solutions (technological or otherwise) to these challenges. We propose a novel approach to apply the aforementioned advances in computation to support doctoral education. Single-case learning analytics defines a process in which doctoral students, researchers, and computational elements collaborate to extract insights about a single (doctoral) learner's experience and learning process (e.g., contextual cues, behaviors and trends related to the doctoral student's sense of progress). The feasibility and added value of this approach are demonstrated using an authentic dataset collected by nine doctoral students over a period of at least two months. The insights from this feasibility study also serve to spark a research agenda for future technological support of doctoral education, which is aligned with recent calls for more human-centered approaches to designing and implementing learning analytics technologies."
552,2024,"Unveiling livestock trade trends: A beginner's guide to generative AI-powered visualization nan This tutorial, rooted in the context of livestock research, is designed to assist novice or non-programmers in visualizing trends in livestock exports between the US and Japan using Python and generative AI systems such as Microsoft's Copilot and Google's Gemini. The analysis of these trends plays a pivotal role in optimizing livestock production. The tutorial offers a thorough guide on preparing data using reliable federal datasets, generating Python code, and tackling potential issues such as overlapping data points. It effectively simplifies complex tasks into manageable steps and includes Python code in the appendices for easy reference. By enabling researchers to extract insights and make predictions from livestock data, this tutorial addresses a significant void in the existing literature. This innovative approach has the potential to transform the way researchers engage with and interpret livestock data, thereby making a substantial contribution to the field."
553,2023,"Programming Languages for AI Programing Agents (Invited Talk) nan Over the past decade software development has shifted from a process centered around writing code to a process that increasingly involves composition of external packages and managing the integration of code from other team members. The next decade-plus will be defined by the shift from a process where humans are the central developers of code into one where AI agents, likely Large Language Model (LLM) based, will be the major creators of code and humans will shift to a supervisory role as curators, integrating rich framework-functionality and code developed by AI programming agents. In this new world we must ask ourselves - are programming languages as they exist today fit for purpose and how do they evolve to meet the needs of this future programming model. This talk represents an opinionated take on the question and attempts to outline specific areas of investigation that need to be addressed by the PL community as part of this journey including: What programming language features help/hinder AI agents when understanding and generating code? What programming language features help/hinder human agents when working with an AI Copilot? What programming language tools are needed to empower AI agents in creating grounded and reliable outputs? How can intents be expressed as part of the program representation - examples, constraints, natural language, external documents? How do we empower end-users as part of this transformation? What programming language features are needed to support new AI driven workflows - live coding, interactive requirement gathering, AI TDD? Effectively answering these questions plays a key role in determining if AI driven programming represents a revolution in how software is developed or is limited to being a programming productivity aid for existing development workflows. As such our community should play a central role in understanding this space and leading in the development of this technological transformation!"
555,2023,"Artificial Intelligence and Public Health: An Exploratory Study. nan Artificial intelligence (AI) has the potential to revolutionize research by automating data analysis, generating new insights, and supporting the discovery of new knowledge. The top 10 contribution areas of AI towards public health were gathered in this exploratory study. We utilized the text-davinci-003 model of GPT-3, using OpenAI playground default parameters. The model was trained with the largest training dataset any AI had, limited to a cut-off date in 2021. This study aimed to test the ability of GPT-3 to advance public health and to explore the feasibility of using AI as a scientific co-author. We asked the AI asked for structured input, including scientific quotations, and reviewed responses for plausibility. We found that GPT-3 was able to assemble, summarize, and generate plausible text blocks relevant for public health concerns, elucidating valuable areas of application for itself. However, most quotations were purely invented by GPT-3 and thus invalid. Our research showed that AI can contribute to public health research as a team member. According to authorship guidelines, the AI was ultimately not listed as a co-author, as it would be done with a human researcher. We conclude that good scientific practice also needs to be followed for AI contributions, and a broad scientific discourse on AI contributions is needed."
556,2024,"Whodunit: Classifying Code as Human Authored or GPT-4 generated- A case study on CodeChef problems nan Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI-generated code as their work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier on the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code."
557,2024,"Large Language Models Meet User Interfaces: The Case of Provisioning Feedback nan Incorporating Generative AI (GenAI) and Large Language Models (LLMs) in education can enhance teaching efficiency and enrich student learning. Current LLM usage involves conversational user interfaces (CUIs) for tasks like generating materials or providing feedback. However, this presents challenges including the need for educator expertise in AI and CUIs, ethical concerns with high-stakes decisions, and privacy risks. CUIs also struggle with complex tasks. To address these, we propose transitioning from CUIs to user-friendly applications leveraging LLMs via API calls. We present a framework for ethically incorporating GenAI into educational tools and demonstrate its application in our tool, Feedback Copilot, which provides personalized feedback on student assignments. Our evaluation shows the effectiveness of this approach, with implications for GenAI researchers, educators, and technologists. This work charts a course for the future of GenAI in education."
558,2024,"Minimap: An interactive dynamic decision making game for search and rescue missions. nan Many aspects of humans' dynamic decision-making (DDM) behaviors have been studied with computer-simulated games called microworlds. However, most microworlds only emphasize specific elements of DDM and are inflexible in generating a variety of environments and experimental designs. Moreover, despite the ubiquity of gridworld games for Artificial Intelligence (AI) research, only some tools exist to aid in the development of browser-based gridworld environments for studying the dynamics of human decision-making behavior. To address these issues, we introduce Minimap, a dynamic interactive game to examine DDM in search and rescue missions, which incorporates all the essential characteristics of DDM and offers a wide range of flexibility regarding experimental setups and the creation of experimental scenarios. Minimap specifically allows customization of dynamics, complexity, opaqueness, and dynamic complexity when designing a DDM task. Minimap also enables researchers to visualize and replay recorded human trajectories for the analysis of human behavior. To demonstrate the utility of Minimap, we present a behavioral experiment that examines the impact of different degrees of structural complexity coupled with the opaqueness of the environment on human decision-making performance under time constraints. We discuss the potential applications of Minimap in improving productivity and transparent replications of human behavior and human-AI teaming research. We made Minimap an open-source tool, freely available at https://github.com/DDM-Lab/MinimapInteractiveDDMGame ."
559,2022,"INVESTIGATING HOW ENGINEERS AND DESIGNERS COMMUNICATE DESIGN RATIONALE nan Design rationale is the explicit documentation of the reasons behind decisions made in designing a product or system. Typically, design rationale is captured using a combination of written reports and oral presentations. Research shows that the structure and information used to communicate rationale significantly influence human behavior. To better understand the influence that communication of design rationale has on the design process, a detailed understanding of the information and techniques used to communicate design rationale needs to be studied. This research aims to identify how engineers and designers communicate this information in written form and the implications that their communication patterns have in engineering design. Eight hundred and forty-six pages of technical engineering design reports from 28 teams representing 116 individuals were analyzed using a mixed-methods approach and then compared across project types. The data were coded into categories using a schema we developed. The findings highlight the range of clarity that designers use in their rationales to support their design actions. Instead of clear, logical reasoning trends, designers often use techniques to fill gaps in design rationale through making assumptions, inserting oneself, or redirecting focus. The results suggest a need for improving design communication in engineering education and practice, perhaps through existing design reasoning frameworks or design rationale capturing tools. By capturing design rationale clearly between human designers, human-AI systems can leverage these findings to increase human confidence in and acceptance of a design agent's recommendations."
560,2024,"Exploratory Models of Human-AI Teams: Leveraging Human Digital Twins to Investigate Trust Development nan As human-agent teaming (HAT) research continues to grow, computational methods for modeling HAT behaviors and measuring HAT effectiveness also continue to develop. One rising method involves the use of human digital twins (HDT) to approximate human behaviors and socio-emotional-cognitive reactions to AI-driven agent team members. In this paper, we address three research questions relating to the use of digital twins for modeling trust in HATs. First, to address the question of how we can appropriately model and operationalize HAT trust through HDT HAT experiments, we conducted causal analytics of team communication data to understand the impact of empathy, socio-cognitive, and emotional constructs on trust formation. Additionally, we reflect on the current state of the HAT trust science to discuss characteristics of HAT trust that must be replicable by a HDT such as individual differences in trust tendencies, emergent trust patterns, and appropriate measurement of these characteristics over time. Second, to address the question of how valid measures of HDT trust are for approximating human trust in HATs, we discuss the properties of HDT trust: self-report measures, interaction-based measures, and compliance type behavioral measures. Additionally, we share results of preliminary simulations comparing different LLM models for generating HDT communications and analyze their ability to replicate human-like trust dynamics. Third, to address how HAT experimental manipulations will extend to human digital twin studies, we share experimental design focusing on propensity to trust for HDTs vs. transparency and competency-based trust for AI agents."
562,2024,"Whodunit: Classifying Code as Human Authored or GPT-4 Generated - A case study on CodeChef problems nan Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI-generated code as their work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier on the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code."
563,2023,"Artificial intelligence in healthcare: Complementing, not replacing, doctors and healthcare providers nan The utilization of artificial intelligence (AI) in clinical practice has increased and is evidently contributing to improved diagnostic accuracy, optimized treatment planning, and improved patient outcomes. The rapid evolution of AI, especially generative AI and large language models (LLMs), have reignited the discussions about their potential impact on the healthcare industry, particularly regarding the role of healthcare providers. Concerning questions, can AI replace doctors? and will doctors who are using AI replace those who are not using it? have been echoed. To shed light on this debate, this article focuses on emphasizing the augmentative role of AI in healthcare, underlining that AI is aimed to complement, rather than replace, doctors and healthcare providers. The fundamental solution emerges with the human-AI collaboration, which combines the cognitive strengths of healthcare providers with the analytical capabilities of AI. A human-in-the-loop (HITL) approach ensures that the AI systems are guided, communicated, and supervised by human expertise, thereby maintaining safety and quality in healthcare services. Finally, the adoption can be forged further by the organizational process informed by the HITL approach to improve multidisciplinary teams in the loop. AI can create a paradigm shift in healthcare by complementing and enhancing the skills of healthcare providers, ultimately leading to improved service quality, patient outcomes, and a more efficient healthcare system."
565,2024,"Optimizing athletic performance through advanced nutrition strategies: can AI and digital platforms have a role in ultra-endurance sports? nan Nutrition is vital for athletic performance, especially in ultra-endurance sports, which pose unique nutritional challenges. Despite its importance, there exist gaps in the nutrition knowledge among athletes, and emerging digital tools could potentially bridge this gap. The ULTRA-Q, a sports nutrition questionnaire adapted for ultra-endurance athletes, was used to assess the nutritional knowledge of ChatGPT-3.5, ChatGPT-4, Google Bard, and Microsoft Copilot. Their performance was compared with experienced ultra-endurance athletes, registered sports nutritionists and dietitians, and the general population. ChatGPT-4 demonstrated the highest accuracy (93%), followed by Microsoft Copilot (92%), Bard (84%), and ChatGPT-3.5 (83%). The averaged AI model achieved an overall score of 88%, with the highest score in Body Composition (94%) and the lowest in Nutrients (84%). The averaged AI model outperformed the general population by 31% points and ultra-endurance athletes by 20% points in overall knowledge. The AI model exhibited superior knowledge in Fluids, outperforming registered dietitians by 49% points, the general population by 42% points, and ultra-endurance athletes by 32% points. In Body Composition, the AI model surpassed the general population by 31% points and ultraendurance athletes by 24% points. In Supplements, it outperformed registered dietitians by 58% points and the general population by 55% points. Finally, in Nutrients and in Recovery, it outperformed the general population only, by 24% and 29% points, respectively. AI models show high proficiency in sports nutrition knowledge, potentially serving as valuable tools for nutritional education and advice. AI-generated insights could be integrated with expert human judgment for effective athlete performance optimization."
566,2020,Human-AI Collaboration for Natural Language Generation with Interpretable Neural Networks nan nan
568,2021,Towards Human-Machine Symbiosis: Design for Effective AI Facilitation nan nan
569,2024,"An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project nan Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration."
570,2024,"Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task nan Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG. However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information. To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities. By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence. Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG. We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques."
571,2024,"Building Multi-Agent Copilot towards Autonomous Agricultural Data Management and Analysis nan Current agricultural data management and analysis paradigms are to large extent traditional, in which data collecting, curating, integration, loading, storing, sharing and analyzing still involve too much human effort and know-how. The experts, researchers and the farm operators need to understand the data and the whole process of data management pipeline to make fully use of the data. The essential problem of the traditional paradigm is the lack of a layer of orchestrational intelligence which can understand, organize and coordinate the data processing utilities to maximize data management and analysis outcome. The emerging reasoning and tool mastering abilities of large language models (LLM) make it a potentially good fit to this position, which helps a shift from the traditional user-driven paradigm to AI-driven paradigm. In this paper, we propose and explore the idea of a LLM based copilot for autonomous agricultural data management and analysis. Based on our previously developed platform of Agricultural Data Management and Analytics (ADMA), we build a proof-of-concept multi-agent system called ADMA Copilot, which can understand user's intent, makes plans for data processing pipeline and accomplishes tasks automatically, in which three agents: a LLM based controller, an input formatter and an output formatter collaborate together. Different from existing LLM based solutions, by defining a meta-program graph, our work decouples control flow and data flow to enhance the predictability of the behaviour of the agents. Experiments demonstrates the intelligence, autonomy, efficacy, efficiency, extensibility, flexibility and privacy of our system. Comparison is also made between ours and existing systems to show the superiority and potential of our system."
572,2024,"Empirical Study of Symmetrical Reasoning in Conversational Chatbots nan This work explores the capability of conversational chatbots powered by large language models (LLMs), to understand and characterize predicate symmetry, a cognitive linguistic function traditionally believed to be an inherent human trait. Leveraging in-context learning (ICL), a paradigm shift enabling chatbots to learn new tasks from prompts without re-training, we assess the symmetrical reasoning of five chatbots: ChatGPT 4, Huggingface chat AI, Microsoft's Copilot AI, LLaMA through Perplexity, and Gemini Advanced. Using the Symmetry Inference Sentence (SIS) dataset by Tanchip et al. (2020), we compare chatbot responses against human evaluations to gauge their understanding of predicate symmetry. Experiment results reveal varied performance among chatbots, with some approaching human-like reasoning capabilities. Gemini, for example, reaches a correlation of 0.85 with human scores, while providing a sounding justification for each symmetry evaluation. This study underscores the potential and limitations of LLMs in mirroring complex cognitive processes as symmetrical reasoning."
573,1988,"Outline of COPILOT, an expert system for reactor operational assistance, using a Bayesian diagnostic module nan The authors explore the possibility of building an expert system for nuclear plant operational assistance around the idea of Bayesian diagnosis. They lay out the general structure of such an expert system and, within that, the more specific structure of the Bayesian diagnostic module and knowledge base. An example is given in which this module is used to diagnose the cause of a reactor trip through three levels of successively increasing detail."
574,2022,Three Maxims for Developing Human-Centered AI for Decision Making nan nan
575,2022,"Group Chat Ecology in Enterprise Instant Messaging: How Employees Collaborate Through Multi-User Chat Channels on Slack nan Despite the long history of studying instant messaging usage, we know very little about how today's people participate in group chat channels and interact with others inside a real-world organization. In this short paper, we aim to update the existing knowledge on how group chat is used in the context of today's organizations. The knowledge is particularly important for the new norm of remote works under the COVID-19 pandemic. We have the privilege of collecting two valuable datasets: a total of 4,300 group chat channels in Slack from an R&D department in a multinational IT company; and a total of 117 groups' performance data. Through qualitative coding of 100 randomly sampled group channels from the 4,300 channels dataset, we identified and reported 9 categories such as Project channels, IT-Support channels, and Event channels. We further defined a feature metric with 21 meta-features (and their derived features) without looking at the message content to depict the group communication style for these group chat channels, with which we successfully trained a machine learning model that can automatically classify a given group channel into one of the 9 categories. In addition to the descriptive data analysis, we illustrated how these communication metrics can be used to analyze team performance. We cross-referenced 117 project teams and their team-based Slack channels and identified 57 teams that appeared in both datasets, then we built a regression model to reveal the relationship between these group communication styles and the project team performance. This work contributes an updated empirical understanding of human-human communication practices within the enterprise setting, and suggests design opportunities for the future of human-AI communication experience."
576,2024,"The performance of artificial intelligence large language model-linked chatbots in surgical decision-making for gastroesophageal reflux disease nan BackgroundLarge language model (LLM)-linked chatbots may be an efficient source of clinical recommendations for healthcare providers and patients. This study evaluated the performance of LLM-linked chatbots in providing recommendations for the surgical management of gastroesophageal reflux disease (GERD).MethodsNine patient cases were created based on key questions addressed by the Society of American Gastrointestinal and Endoscopic Surgeons (SAGES) guidelines for the surgical treatment of GERD. ChatGPT-3.5, ChatGPT-4, Copilot, Google Bard, and Perplexity AI were queried on November 16th, 2023, for recommendations regarding the surgical management of GERD. Accurate chatbot performance was defined as the number of responses aligning with SAGES guideline recommendations. Outcomes were reported with counts and percentages.ResultsSurgeons were given accurate recommendations for the surgical management of GERD in an adult patient for 5/7 (71.4%) KQs by ChatGPT-4, 3/7 (42.9%) KQs by Copilot, 6/7 (85.7%) KQs by Google Bard, and 3/7 (42.9%) KQs by Perplexity according to the SAGES guidelines. Patients were given accurate recommendations for 3/5 (60.0%) KQs by ChatGPT-4, 2/5 (40.0%) KQs by Copilot, 4/5 (80.0%) KQs by Google Bard, and 1/5 (20.0%) KQs by Perplexity, respectively. In a pediatric patient, surgeons were given accurate recommendations for 2/3 (66.7%) KQs by ChatGPT-4, 3/3 (100.0%) KQs by Copilot, 3/3 (100.0%) KQs by Google Bard, and 2/3 (66.7%) KQs by Perplexity. Patients were given appropriate guidance for 2/2 (100.0%) KQs by ChatGPT-4, 2/2 (100.0%) KQs by Copilot, 1/2 (50.0%) KQs by Google Bard, and 1/2 (50.0%) KQs by Perplexity.ConclusionsGastrointestinal surgeons, gastroenterologists, and patients should recognize both the promise and pitfalls of LLM's when utilized for advice on surgical management of GERD. Additional training of LLM's using evidence-based health information is needed."
577,2024,"Exploring the Potential Use of Generative AI in Software Engineering Education nan The integration of Generative AI into software engineering education marks a transformative shift in teaching methodologies. This paper explores its potential, highlighting the benefits of enhancing student engagement, creativity, and efficiency while preparing them for industry challenges. Through a comprehensive analysis of 13 popular generative AI tools, we examine their roles in various software engineering tasks such as requirements analysis, design, coding, debugging, and testing. This paper contributes to the broader discourse on the future of software engineering education by offering evidence-based recommendations for leveraging generative AI to create adaptive and forward-thinking instructional strategies."
578,2022,"Towards Human-centered Explainable AI: User Studies for Model Explanations [arXiv] nan Explainable AI (XAI) is widely viewed as a sine qua non for ever-expanding AI research. A better understanding of the needs of XAI users, as well as human-centered evaluations of explainable models are both a necessity and a challenge. In this paper, we explore how HCI and AI researchers conduct user studies in XAI applications based on a systematic literature review. After identifying and thoroughly analyzing 85 core papers with human-based XAI evaluations over the past five years, we categorize them along the measured characteristics of explanatory methods, namely trust, understanding, fairness, usability, and human-AI team performance. Our research shows that XAI is spreading more rapidly in certain application domains, such as recommender systems than in others, but that user evaluations are still rather sparse and incorporate hardly any insights from cognitive or social sciences. Based on a comprehensive discussion of best practices, i.e., common models, design choices, and measures in user studies, we propose practical guidelines on designing and conducting user studies for XAI researchers and practitioners. Lastly, this survey also highlights several open research directions, particularly linking psychological science and human-centered XAI."
579,2024,"Does Co-Development with AI Assistants Lead to More Maintainable Code? A Registered Report nan AI assistants like GitHub Copilot are transforming software engineering; several studies have highlighted productivity improvements. However, their impact on code quality, particularly in terms of maintainability, requires further investigation. [Objective/Aim] This study aims to examine the influence of AI assistants on software maintainability, specifically assessing how these tools affect the ability of developers to evolve code. [Method] We will conduct a two-phased controlled experiment involving professional developers. In Phase 1, developers will add a new feature to a Java project, with or without the aid of an AI assistant. Phase 2, a randomized controlled trial, will involve a different set of developers evolving random Phase 1 projects - working without AI assistants. We will employ Bayesian analysis to evaluate differences in completion time, perceived productivity, code quality, and test coverage."
580,2024,"RUBICON: Rubric-Based Evaluation of Domain-Specific Human AI Conversations nan Evaluating conversational assistants, such as GitHub Copilot Chat, poses a significant challenge for tool builders in the domain of Software Engineering. These assistants rely on language models and chat-based user experiences, rendering their evaluation with respect to the quality of the Human-AI conversations complicated. Existing general-purpose metrics for measuring conversational quality found in literature are inadequate for appraising domainspecific dialogues due to their lack of contextual sensitivity.In this paper, we present RUBICON, a technique for evaluating domain-specific Human-AI conversations. RUBICON leverages large language models to generate candidate rubrics for assessing conversation quality and employs a selection process to choose the subset of rubrics based on their performance in scoring conversations. In our experiments, RUBICON effectively learns to differentiate conversation quality, achieving higher accuracy and yield rates than existing baselines."
581,2023,"On the assessment of generative AI in modeling tasks: an experience report with ChatGPT and UML nan Most experts agree that large language models (LLMs), such as those used by Copilot and ChatGPT, are expected to revolutionize the way in which software is developed. Many papers are currently devoted to analyzing the potential advantages and limitations of these generative AI models for writing code. However, the analysis of the current state of LLMs with respect to software modeling has received little attention. In this paper, we investigate the current capabilities of ChatGPT to perform modeling tasks and to assist modelers, while also trying to identify its main shortcomings. Our findings show that, in contrast to code generation, the performance of the current version of ChatGPT for software modeling is limited, with various syntactic and semantic deficiencies, lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role that LLMs can play in the software modeling discipline in the short term, and how the modeling community can help to improve the current capabilities of ChatGPT and the coming LLMs for software modeling."
582,2024,"Significant Productivity Gains through Programming with Large Language Models nan Large language models like GPT and Codex drastically alter many daily tasks, including programming, where they can rapidly generate code from natural language or informal specifications. Thus, they will change what it means to be a programmer and how programmers act during software development. This work explores how AI assistance for code generation impacts productivity. In our user study (N=24), we asked programmers to complete Python programming tasks supported by a) an auto-complete interface using GitHub Copilot, b) a conversational system using GPT-3, and c) traditionally with just the web browser. Aside from significantly increasing productivity metrics, participants displayed distinctive usage patterns and strategies, highlighting that the form of presentation and interaction affects how users engage with these systems. Our findings emphasize the benefits of AI-assisted coding and highlight the different design challenges for these systems."
583,2024,"Better Understanding of Humans for Cooperative AI through Clustering nan Cooperative AI and AI alignment research are increasingly important fields of study as machine learning models are becoming more prevalent in society. Applications such as self-driving cars, realistic AI in games, and human-AI teams, all require further advancement in cooperative and alignment research before more widespread applications can be achieved. However, research in these fields has typically lagged behind other machine learning applications due to the difficulty of creating models that are robust to and can adapt to novel human partners. We attempt to address this through the creation of a framework that uses Archetypal Analysis, a unique clustering algorithm that finds extremal 'archetype' points in a dataset and expresses each other point as a convex combination of these archetypes. This framework creates understandable archetypes of players which a reinforcement learning agent can use to adapt accordingly to unseen partners. We show that this framework not only results in performance comparable to other cooperative benchmark models but also achieves higher levels of perceived cooperativeness without the need for human involvement during the training process. As such, we demonstrate that the use of clustering techniques to better model different types of human behaviour and strategies can be an effective approach in improving the ability of AI models to adapt to and improve cooperation with novel partners."
584,2024,"Assessing AI-Based Code Assistants in Method Generation Tasks [arXiv] nan AI-based code assistants are increasingly popular as a means to enhance productivity and improve code quality. This study compares four AI-based code assistants, GitHub Copilot, Tabnine, ChatGPT, and Google Bard, in method generation tasks, assessing their ability to produce accurate, correct, and efficient code. Results show that code assistants are useful, with complementary capabilities, although they rarely generate ready-to-use correct code."
585,2024,"Artificial intelligence (AI) and process safety: Some cautionary observations nan Artificial intelligence (AI) has become a vogue topic in the press, and descriptions of its potential impact range from apocalyptic to salvational. Interest in the topic will no doubt stimulate the search for applications to support both the technical and management systems aspects of process safety management. Within our industries, maintaining institutional memory and technical capability is made increasingly challenging by more frequent job movement among younger staff and the loss to the retirement of more senior staff. One would hope that AI could help fill the gaps caused by these factors. However, the author's sampling of current AI capabilities suggests that AI is not yet ready to do so. This paper provides some examples of errors and insufficiencies identified when seeking AI assistance in addressing process safety issues. It also suggests some existing challenges to better training of AI to support the needs of the process safety community. It concludes that caution should be applied, especially by less experienced personnel, when seeking AI assistance in addressing process safety-related technical matters."
586,2023,"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models [arXiv] nan Recently, large language models for code generation have achieved breakthroughs in several programming language tasks. Their advances in competition-level programming problems have made them an emerging pillar in AI-assisted pair programming. Tools such as GitHub Copilot are already part of the daily programming workflow and are used by more than a million developers. The training data for these models is usually collected from open-source repositories (e.g., GitHub) that contain software faults and security vulnerabilities. This unsanitized training data can lead language models to learn these vulnerabilities and propagate them in the code generation procedure. Given the wide use of these models in the daily workflow of developers, it is crucial to study the security aspects of these models systematically. In this work, we propose the first approach to automatically finding security vulnerabilities in black-box code generation models. To achieve this, we propose a novel black-box inversion approach based on few-shot prompting. We evaluate the effectiveness of our approach by examining code generation models in the generation of high-risk security weaknesses. We show that our approach automatically and systematically finds 1000s of security vulnerabilities in various code generation models, including the commercial black-box model GitHub Copilot."
587,2021,"More Similar Values, More Trust? - the Effect of Value Similarity on Trust in Human-Agent Interaction nan As AI systems are increasingly involved in decision making, it also becomes important that they elicit appropriate levels of trust from their users. To achieve this, it is first important to understand which factors influence trust in AI. We identify that a research gap exists regarding the role of personal values in trust in AI. Therefore, this paper studies how human and agent Value Similarity (VS) influences a human's trust in that agent. To explore this, 89 participants teamed up with five different agents, which were designed with varying levels of value similarity to that of the participants. In a within-subjects, scenario-based experiment, agents gave suggestions on what to do when entering the building to save a hostage. We analyzed the agent's scores on subjective value similarity, trust and qualitative data from open-ended questions. Our results show that agents rated as having more similar values also scored higher on trust, indicating a positive effect between the two. With this result, we add to the existing understanding of human-agent trust by providing insight into the role of value-similarity."
588,2023,"Modeling, Replicating, and Predicting Human Behavior: A Survey nan Given the popular presupposition of human reasoning as the standard for learning and decision making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. As such, topics including Game Theory, Theory of Mind, and Machine Learning, among others, integrate concepts that are assumed components of human reasoning. These serve as techniques to replicate and understand the behaviors of humans. In addition, next-generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, allowing them not only to replicate human models as a technique to learn but also to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this article is to provide a succinct yet systematic review of important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques that learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without necessarily learning via trial and error."
589,2024,"What Kind of Support Do Astronauts Need for Maintenance Tasks in Space Habitats? A Survey Study nan NASA's renewed human spaceflight exploration efforts, starting with the Artemis Program, are accelerating the development of deep space habitats. However, there are still many unresolved challenges to achieving viable operations for deep space habitats, mainly due to communication latencies. These latencies will limit the ability of mission control on Earth to command and control these space habitats and will also prevent mission control from assisting astronauts in real time. For example, astronauts occasionally require assistance while following procedures to perform maintenance tasks. Traditionally, mission controllers provide real-time support while monitoring astronauts during the execution of these maintenance tasks in low Earth orbit habitats (e.g., International Space Station). To mitigate the loss of this type of real-time support for deep space habitats, the use of Autonomous systems powered by artificial intelligence (AI) methods has been proposed. To develop these autonomous systems, we first need to understand the current state of the art for human task support in low earth orbit. To improve our understanding, we designed and conducted a survey study with ten former astronauts who performed space habitat maintenance tasks to identify and quantify feedback options (or suggestions) to be given during task execution. Although assistance from mission control is far more complex than just simple status feedback, simplifying assistance as a limited set of feedback options may make the implementation of human-AI teams easier. In this paper, we present the design process and results of the survey study regarding the preferred feedback options as a means of assistance for maintenance tasks in space habitats."
590,2024,"Comparing answers of artificial intelligence systems and clinical toxicologists to questions about poisoning: Can their answers be distinguished? nan OBJECTIVE: To present questions about poisoning to 4 artificial intelligence (AI) systems and 4 clinical toxicologists and determine whether readers can identify the source of the answers. To evaluate and compare text quality and level of knowledge found in the AI and toxicologists' responses.METHODS: Ten questions about toxicology were presented to the following AI systems: Copilot, Bard, Luzia, and ChatGPT. Four clinical toxicologists were asked to answer the same questions. Twenty-four recruited experts in toxicology were sent a pair of answers (1 from an AI system and one from a toxicologist) for each of the 10 questions. For each answer, the experts had to identify the source, evaluate text quality, and assess level of knowledge reflected. Quantitative variables were described as mean (SD) and qualitative ones as absolute frequency and proportion. A value of P .05 was considered significant in all comparisons.RESULTS: Of the 240 evaluated AI answers, the expert evaluators thought that 21 (8.8%) and 38 (15.8%), respectively, were certainly or probably written by a toxicologist. The experts were unable to guess the source of 13 (5.4%) AI answers. Luzia and ChatGPT were better able to mislead the experts than Bard (P = .036 and P = .041, respectively). Text quality was judged excellent in 38.8% of the AI answers. ChatGPT text quality was rated highest (61.3% excellent) vs Bard (34.4%), Luzia (31.7%), and Copilot (26.3%) (P .001, all comparisons). The average score for the level of knowledge perceived in the AI answers was 7.23 (1.57) out of 10. The highest average score was achieved by ChatGPT at 8.03 (1.26) vs Luzia (7.02 [1,63]), Bard (6.91 [1.64]), and Copilot (6.91 [1.46]) (P .001, all comparisons).CONCLUSIONS: Luzia and ChatGPT answers to the toxicology questions were often thought to resemble those of clinical toxicologists. ChatGPT answers were judged to be very well-written and reflect a very high level of knowledge.OBJETIVO: Formular preguntas sobre intoxicaciones a cuatro sistemas de inteligencia artificial (IA) y a cuatro toxicologos clinicos (TC) y constatar si un grupo de observadores es capaz de identificar el origen de las respuestas. Valorar la calidad del texto y el nivel de conocimientos ofrecidos por estas IA y compararlos con el de los TC.METODO: Se prepararon 10 preguntas de toxicologia y se introdujeron en cuatro sistemas de IA (Copilot, Bard, LuzIA y ChatGPT). Se solicito a cuatro TC que respondiesen a las mismas preguntas. Se consiguieron 24 observadores expertos en toxicologia y se les remitio un cuestionario con 10 preguntas y cada una de ellas con una respuesta procedente de una IA y otra de un TC. Cada observador tenia que decidir la procedencia de las respuestas, valorar la calidad del texto y cuantificar el nivel de conocimientos sobre el tema.RESULTADOS: De las 240 respuestas que analizaron los observadores y que procedian de alguna IA, en 21 ocasiones (8,8%) opinaron que con certeza provenian de un TC, en 38 (15,8%) que procedian probablemente de un TC y en 13 (5,4%) reconocian que no podian establecer el origen de la respuesta. LuzIA y ChatGPT mostraron una mayor capacidad de engano a los observadores, con diferencias significativas respecto a Bard (p = 0,036 y p = 0,041, respectivamente). Con relacion a la calidad de los textos de las respuestas ofrecidas por las IA, la valoracion de los observadores fue de excelente en el 38,8% de las ocasiones, con una diferencia significativa en favor de ChatGPT (61,3% de respuestas excelentes) respecto a Bard (34,4%, p 0,001), LuzIA (31,7%, p 0,001) y Copilot (26,3%, p 0,001). Respecto a la percepcion de conocimientos sobre el tema por parte de las IA, la puntuacion media de fue de 7,23 (DE 1,57) sobre 10, obteniendo ChatGPT una puntuacion de 8,03 (DE 1,26) que fue mayor a la obtenida por Luzia [7,02 (DE 1,63), p 0,001], Bard [6,91 (1,64), p 0,001] y Copilot [6,91 (1,46), p 0,001].CONCLUSIONES: LuzIA y ChatGPT son sistemas de IA capaces de generar respuestas a preguntas de toxicologia que, con frecuencia, parecen haber sido respondidas por un TC. La calidad de los textos generados y la percepcion de conocimientos que ofrece ChatGPT es muy elevada."
591,2024,"Contextual Augmented Multi-Model Programming (CAMP): A Hybrid Local-Cloud Copilot Framework nan The advancements in cloud-based Large Languages Models (LLMs) have revolutionized AI-assisted programming. However, their integration into certain local development environments like ones within the Apple software ecosystem (e.g., iOS apps, macOS) remains challenging due to computational demands and sandboxed constraints. This paper presents CAMP, a multi-model AI-assisted programming framework that consists of a local model that employs Retrieval-Augmented Generation (RAG) to retrieve contextual information from the codebase to facilitate context-aware prompt construction thus optimizing the performance of the cloud model, empowering LLMs' capabilities in local Integrated Development Environments (IDEs). The methodology is actualized in Copilot for Xcode, an AI-assisted programming tool crafted for Xcode that employs the RAG module to address software constraints and enables diverse generative programming tasks, including automatic code completion, documentation, error detection, and intelligent user-agent interaction. The results from objective experiments on generated code quality and subjective experiments on user adoption collectively demonstrate the pilot success of the proposed system and mark its significant contributions to the realm of AI-assisted programming."
592,2024,"You're on a bicycle with a little motor: Benefits and Challenges of Using AI Code Assistants nan AI code assistants, such as Tabnine, GitHub CoPilot, and ChatGPT, employ Large Language Models (LLMs) trained on extensive source code and other documents. They receive prompts and generate code suggestions aimed to facilitate programming tasks. Previous research in this field has explored the correctness, complexity, quality, and security of the code suggestions. Software developers' experiences have been studied in the context of controlled experiments. Based on 14 interviews with software developers, this paper describes the developers' daily and continuous experiences with AI code assistants, presenting benefits and challenges grounded in actual development work, along with strategies to address these challenges."
593,2023,"Responsible use of AI in military systems: prospects and challenges nan Artificial Intelligence (AI) holds great potential for the military domain but is also seen as prone to data bias and lacking transparency and explainability. In order to advance the trustworthiness of AI-enabled systems, a dynamic approach to the development, deployment and use of AI systems is required. This approach, when incorporating ethical principles such as lawfulness, traceability, reliability and bias mitigation, is called 'Responsible AI'. This article describes the challenges of using AI responsibly in the military domain from a human factors and ergonomics perspective. Many of the ironies of automation originally described by Bainbridge still apply in the field of AI, but there are also some unique challenges and requirements that need to be considered, such as a larger emphasis on ethical risk analyses and validation and verification up-front, as well as moral situation awareness during deployment and use of AI in military systems.'Responsible AI' is a relatively novel transdisciplinary field incorporating ethical principles in the development and use of AI in military systems. I describe the prospects and challenges with Responsible AI from a human factors and ergonomics perspective. There is in particular a need for new methods for testing and evaluation, validation and verification, explainability and transparency of AI, as well as for new ways of Human-AI Teaming."
594,2024,"AI-Driven Guided Response for Security Operation Centers with Microsoft Copilot for Security nan Security operation centers contend with a constant stream of security incidents, ranging from straightforward to highly complex. To address this, we developed Copilot Guided Response (CGR), an industry-scale ML architecture that guides security analysts across three key tasks -- (1) investigation, providing essential historical context by identifying similar incidents; (2) triaging to ascertain the nature of the incident -- whether it is a true positive, false positive, or benign positive; and (3) remediation, recommending tailored containment actions. CGR is integrated into the Microsoft Defender XDR product and deployed worldwide, generating millions of recommendations across thousands of customers. Our extensive evaluation, incorporating internal evaluation, collaboration with security experts, and customer feedback, demonstrates that CGR delivers high-quality recommendations across all three tasks. We provide a comprehensive overview of the CGR architecture, setting a precedent as the first cybersecurity company to openly discuss these capabilities in such depth. Additionally, we GUIDE, the largest public collection of real-world security incidents, spanning 13M evidences across 1M annotated incidents. By enabling researchers and practitioners to conduct research on real-world data, GUIDE advances the state of cybersecurity and supports the development of next-generation machine learning systems."
595,2023,"Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems [arXiv] nan The dazzling promises of AI systems to augment humans in various tasks hinge on whether humans can appropriately rely on them. Recent research has shown that appropriate reliance is the key to achieving complementary team performance in AI-assisted decision making. This paper addresses an under-explored problem of whether the Dunning-Kruger Effect (DKE) among people can hinder their appropriate reliance on AI systems. DKE is a metacognitive bias due to which less-competent individuals overestimate their own skill and performance. Through an empirical study (N = 249), we explored the impact of DKE on human reliance on an AI system, and whether such effects can be mitigated using a tutorial intervention that reveals the fallibility of AI advice, and exploiting logic units-based explanations to improve user understanding of AI advice. We found that participants who overestimate their performance tend to exhibit under-reliance on AI systems, which hinders optimal team performance. Logic units-based explanations did not help users in either improving the calibration of their competence or facilitating appropriate reliance. While the tutorial intervention was highly effective in helping users calibrate their self-assessment and facilitating appropriate reliance among participants with overestimated self-assessment, we found that it can potentially hurt the appropriate reliance of participants with underestimated self-assessment. Our work has broad implications on the design of methods to tackle user cognitive biases while facilitating appropriate reliance on AI systems. Our findings advance the current understanding of the role of self-assessment in shaping trust and reliance in human-AI decision making. This lays out promising future directions for relevant HCI research in this community."
596,2023,"Consistency of Code: A prompt based approach to comprehend functionality nan Large language model (LLM)-based AI for code model (e.g., Copilot) demonstrates the potential of using AI in specialized domains such as software engineering. While previous research has focused on fine-tuning models with additional data and computational cost to construct models optimized for specific domains, our research focuses on prompt engineering methods that maximize the performance of existing models. We conducted a quantitative and qualitative user study using the AI for code model and identified two limitations that hinder the recommendation performance of the model. We propose two methods to address these limitations through effective prompt engineering. Finally, we identified the potential for the use of our proposed methods to be utilized and discussed the direction of future research for the effective use of the LLM."
597,2024,"Let's Fix this Together: Conversational Debugging with GitHub Copilot nan Despite advancements in IDE tooling, code understanding, generation, and automated repair, debugging continues to present significant challenges. Existing debugging strategies available to developers in literature are often too mechanical and rigid for day-to-day issues. Recent advances in Large Language Models (LLMs) promise practical solutions that allow for more free-form debugging strategies. While LLMs offer satisfactory assistance in some cases, they often leap to action without sufficient context, making implicit assumptions and providing inaccurate responses. Moreover, the dialogue between developers and LLMs predominantly takes the form of question-answer pairs, placing the burden of formulating the correct questions and sustaining multi-turn conversations on the developer. We introduce Robin, a novel multi-agent conversational AI-assistant within GitHub Copilot Chat, specifically designed for debugging. Robin moves beyond the question-answer pairs by introducing the investigate & respond pattern, that focuses on using information gathered automatically from the IDE or gathered interactively from the developer before responding. Robin incorporates a general debugging strategy to systematically analyze bugs to sustain collaborative interactions while ensuring that the conversation does not deviate from the debugging task at hand. Through a within-subjects user study with 16 industry professionals, we find that equipping Robin to-(1) leverage the insert expansion interaction pattern, (2) facilitate turn-taking, and (3) utilize debugging strategies-leads to lowered conversation barriers, a 2.5 x improvement in bug localization and a substantial 3.5x improvement in bug resolution compared to AI-assisted debugging in Visual Studio prior to Robin."
598,2024,"LLMs Integration in Software Engineering Team Projects: Roles, Impact, and a Pedagogical Design Space for AI Tools in Computing Education nan This work takes a pedagogical lens to explore the implications of generative AI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a semester-long 2nd-year undergraduate Software Engineering Team Project. Qualitative findings from survey (39 students) and interviews (eight students) provide insights into the students' views on the impact of GenAI use on their coding experience, learning, and self-efficacy. Our results address a particular gap in understanding the role and implications of GenAI on teamwork, team-efficacy, and team dynamics. The analysis of the learning aspects is distinguished by the application of learning and pedagogy informed lenses to discuss the data. We propose a preliminary design space for GenAI-based programming learning tools highlighting the importance of considering the roles that GenAI can play during the learning process, the varying support-ability patterns that can be applied to each role, and the importance of supporting transparency in GenAI for team members and students in addition to educators."
599,2021,"Shelley: A Crowd-sourced Collaborative Horror Writer nan Fear induction in the form of stories and visual images pervades the history of human culture. Creating a visceral emotion such as fear remains one of the cornerstones of human creativity. As artificial intelligence makes strides in solving challenging analytical problems like chess and Go, an important question still remains: can machines induce extreme human emotions, such as fear? In this work, we propose a deep-learning based collaborative horror writer that collaboratively writes scary stories with people on Twitter. We deploy our system as a bot on Twitter that regularly generates and posts new stories on Twitter, and invites users to participate. Users who interact with the stories produce multiple storylines originating from the same tweet, thereby creating a tree-based story structure. We further perform a validation study on n = 105 subjects to verify whether the generated stories psychologically move people on psychometrically validated measures of effect and anxiety such as I-PANAS-SF [43] and STAI-SF [26]. Our experiments show that 1) stories generated by our bot as well as the stories generated collaboratively between our bot and Twitter users produced statistically significant increases in negative affect and state anxiety compared to the control condition, and 2) collaborated stories are more successful in terms of increasing negative affect and state anxiety than the machine-generated ones. Furthermore, we make three novel datasets used in our framework publicly available at https://github.com/catlab-team/shelley for encouraging further research on this topic."
600,2024,"HAI-GEN 2024: 5thWorkshop on Human-AI Co-Creation with Generative Models nan Generative AI has rapidly entered the public consciousness with the development of applications such as ChatGPT, Midjourney, and GitHub Copilot. Nielsen recently argued that we have entered a new era of human-computer interaction in which users need only specify what they want and not how it should be produced [1]. This paradigm of intent-based outcome specification shifts control over from people to AI, enabling new forms of co-creativity and co-creation. Although these systems are capable of holding fluent conversations and producing high-fidelity images, difficulties remain regarding their ability to produce outputs that satisfy their users' needs. Our workshop will bring together researchers and practitioners from both the HCI and AI disciplines to explore the implications of this shift in control, deepen our understanding of the human-AI co-creative process, and examine how we can design, build, use, and evaluate human-AI co-creative systems that are both effective and safe."
601,2024,"How Do Analysts Understand and Verify AI-Assisted Data Analyses? nan Data analysis is challenging as it requires synthesizing domain knowledge, statistical expertise, and programming skills. Assistants powered by large language models (LLMs), such as ChatGPT, can assist analysts by translating natural language instructions into code. However, AI-assistant responses and analysis code can be misaligned with the analyst's intent or be seemingly correct but lead to incorrect conclusions. Therefore, validating AI assistance is crucial and challenging. Here, we explore how analysts understand and verify the correctness of AI-generated analyses. To observe analysts in diverse verification approaches, we develop a design probe equipped with natural language explanations, code, visualizations, and interactive data tables with common data operations. Through a qualitative user study (n=22) using this probe, we uncover common behaviors within verification workflows and how analysts' programming, analysis, and tool backgrounds reflect these behaviors. Additionally, we provide recommendations for analysts and highlight opportunities for designers to improve future AI-assistant experiences."
602,2024,"Generating java methods: an empirical assessment of four ai-based code assistants nan AI-based code assistants are promising tools that can facilitate and speed up code development. They exploit machine learning algorithms and natural language processing to interact with developers, suggesting code snippets (e.g., method implementations) that can be incorporated into projects. Recent studies empirically investigated the effectiveness of code assistants using simple exemplary problems (e.g., the re-implementation of well-known algorithms), which fail to capture the spectrum and nature of the tasks actually faced by developers. In this paper, we expand the knowledge in the area by comparatively assessing four popular AI-based code assistants, namely GitHub Copilot, Tabnine, ChatGPT, and Google Bard, with a dataset of 100 methods that we constructed from real-life open-source Java projects, considering a variety of cases for complexity and dependency from contextual elements. Results show that Copilot is often more accurate than other techniques, yet none of the assistants is completely subsumed by the rest of the approaches. Interestingly, the effectiveness of these solutions dramatically decreases when dealing with dependencies outside the boundaries of single classes. CCS CONCEPTS  Software and its engineering Integrated and visual development environments."
603,2023,"Personalised Programming Education with Knowledge Tracing nan In traditional programming education, addressing diverse student needs and providing effective and scalable learning experiences is challenging. Conventional methods struggle to adapt to varying learning styles and offer personalised feedback. AI-based Programming Tools (AIPTs) have shown promise in automating feedback, simplifying programming concepts, and guiding students. Their widespread adoption is hindered by limitations related to accuracy, explanation, and personalisation. Conversely, AIPTs tailored for expert programmers, such as ChatGPT and Copilot, have gained popularity for their productivity-enhancing capabilities, but they still fall short in terms of personalisation, neglecting individual students' unique knowledge and skills. Our research aims to leverage AI to create AIPTs that offer personalised feedback through adaptive learning, accommodating diverse student backgrounds and proficiency levels. In particular, we explore using Knowledge Tracing (KT) to anticipate specific syntax errors in programming assignments, addressing the challenges novices face in acquiring syntactical knowledge. The findings suggest the KT's potential to transform programming education by enabling timely interventions for students dealing with specific errors or misconceptions, automating personalised feedback, and informing tailored instructional strategies."
604,2024,The Human Side of Adaptive Autonomy: Design Considerations for Adaptive Autonomous Teammates nan nan
605,2022,"Explanations Can Reduce Overreliance on AI Systems During Decision-Making [arXiv] nan Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction."
606,2022,"Generating Diverse Code Explanations using the GPT-3 Large Language Model nan Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github's Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs' potential to support learning by explaining numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code."
607,2024,"FROM PENCIL TO PIXEL The Evolution of Design Ideation Tools nan This study explores the integration of Artificial Intelligence-Generated Content (AIGC) in design processes, focusing on the ideation phase. Utilizing in-depth interviews with experienced designers and an experimental approach with novices, it compares AIGC tools like ChatGPT, Midjourney and Copilot with traditional sketching methods. The findings reveal two distinct operational patterns in AIGC utilization: a subtractive method of refining AI outputs and an additive method of evolving design through AI suggestions. Experienced designers view AIGC as a powerful aid for creative ideation, while novices prefer familiar hand-drawing methods. The study proposes a Seeing-Instructing-Seeing model, adapting Schon's reflective practice model, to incorporate the collaborative dynamic between designers and AI, marking a shift from manual to intellectual labor in design ideation. This represents a paradigm shift in design methodologies, suggesting a future of co-creative partnerships between designers and AI tools."
608,2023,"Towards an AI-centric Requirements Engineering Framework for Trustworthy AI nan Ethical guidelines are an asset for artificial intelligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering(RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level."
609,2024,"A post-hurricane building debris estimation workflow enabled by uncertainty-aware AI and crowdsourcing nan Climate disasters often result in large amounts of debris that need to be cleaned up in the event's aftermath. Effective post-disaster debris management poses unique challenges due to limited resources, high expenses, and the nonhomogeneous and hazardous debris content. Current practices of debris cleanup rely on time-consuming and error-prone field-based estimation, which impedes immediate response and accurate analysis. While new approaches such as artificial intelligence (AI) and crowdsourcing have shown promise in various domains, their potential for debris estimation remains underexplored. This study leverages a human-AI teaming workflow that estimates post-disaster debris volume and composition. The workflow begins with the utilization of drones to capture aerial views for efficient disaster infrastructure reconnaissance. Subsequently, an AI model and a crowdsourcing module are calibrated and work together to rapidly and reliably detect damaged buildings and classify the levels of damage based on aerial imagery. By establishing a connection between building damage states and the resulting generated debris, these damage level predictions serve as a foundation for the efficient and accurate estimation of debris. Overall, the proposed approach uses drones for rapid data collection and AI to automate damage detection and classification, which reduces the time required for debris estimation from days to a few hours as well as minimizes predictive uncertainty with crowdsourcing by up to 40%. A case study on Hurricane Laura in 2020 was conducted to validate the proposed approach. Findings of this study lead to predicting debris volume and composition with minimal errors, particularly for partially damaged buildings."
610,2024,"The Penetration of Generative AI in Higher Education: A Survey nan Context: The global teacher shortage crisis is a severe challenge. The crisis also rises in Indonesia, where the problem extends to unequal teaching quality and learning facilities. This situation seriously threatens the Indonesian 2045 vision of being a developed country with knowledgeable human resources. The advancement of AI brings opportunities to address the challenges. In recent years, there has been a wave of generative AI (GenAI) technologies and their adoption in education. However, there is no research on the penetration of such technology in our learning environment. Objective: In this study, we investigate the penetration of GenAI by students in a higher education setting. Method: We surveyed 1,157 students of Institut Teknologi Del, a private university in western Indonesia, and developed local knowledge based on the responses. Results: Our results show that most students are well aware of GenAI technologies (70.96%) and have used them to support their learning (98.96%). The top five most used GenAI tools are GitHub Copilot, OpenAI ChatGPT, Codex, Grammarly, and ChatPDF. Conclusion: GenAI is already part of the daily learning process. We believe that, sooner or later, GenAI will be one of many deciding factors in our future education systems, and we must be ready to adapt to it."
611,2024,"Students' Perspectives on AI Code Completion: Benefits and Challenges nan AI Code Completion (e.g., GitHub's Copilot) has revolutionized how computer science students interact with programming languages. However, AI code completion has been studied from the developers' perspectives, not the students' perspectives who represent the future generation of our digital world. In this paper, we investigated the benefits, challenges, and expectations of AI code completion from students' perspectives. To facilitate the study, we first developed an open-source Visual Studio Code Extension tool AutoAurora, powered by a state-of-the-art large language model StarCoder, as an AI code completion research instrument. Next, we conduct an interview study with ten student participants and apply grounded theory to help analyze insightful findings regarding the benefits, challenges, and expectations of students on AI code completion. Our findings show that AI code completion enhanced students' productivity and efficiency by providing correct syntax suggestions, offering alternative solutions, and functioning as a coding tutor. However, the over-reliance on AI code completion may lead to a surface-level understanding of programming concepts, diminishing problem-solving skills and restricting creativity. In the future, AI code completion should be explainable and provide best coding practices to enhance the education process."
612,2024,"Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints nan Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series of cost-sensitive fraud detection scenarios with different teams of 9 synthetic fraud analysts, with individual work capacity constraints. The results demonstrate that our approach performs significantly better than the baselines in a wide array of scenarios, achieving an average 8.4% reduction in the misclassification cost."
613,2023,"EEG Context Fusion for AI-Based Object Detection and Drone Navigation in Situationally Aware Brain-Computer Interfaces nan We are interested in the utility that artificially intelligent mobile systems such as drones offer to personnel in fast-paced situations such as hostage rescue and disaster relief for real-time situational awareness. Ideally, these assistive systems place no additional cognitive or physical burden on their user; rather, they should respond to the user's intent with minimal physical and cognitive burden. To this end, brain-computer interface (BCI) and in particular electroencephalography (EEG) offers a novel way to capture intent. EEG-based drone control has been explored, but typically relies on the domain of motor imagery (MI), which still requires somewhat-manual drone piloting for micro directional movement, adding cognitive burden. We propose leveraging the robust computer-vision based AI that exists on modern drones to use objects as waypoints and fly mostly autonomously, with EEG in an object recognition paradigm for passively selecting the waypoint. While existing techniques for EEG object recognition - including steady-state visually evoked potentials (SSVEP), rapid serial visual presentation (RSVP), and eye tracking - rely on marked input or a controlled environment, we need one that will work on passive objects. In this work, we propose merging available scene imagery in a vision network with deep-learning based EEG processing to achieve passive intent recognition. The proposed context-scene fusion for a situational awareness paradigm presents accuracy in the same ranges as existing object recognition without requiring SSVEP, RSVP, eye tracking, or MI techniques and serves as proof-of-concept for this approach to real-world BCI applications."
614,2024,"From Persona to Personalization: A Survey on Role-Playing Language Agents nan Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony."
615,2024,"Conversational AI: A Treatise About Vying Chatbots nan With so many chatbot alternatives available today, users sometimes struggle to choose the best one for their sector. This research paper offers a comprehensive comparison of various chatbots including Microsoft Copilot, ChatGPT, GPT-4, BlenderBot-3, and Gemini to aid users in selecting the most suitable option for their respective sectors like for educational purposes. By helping consumers comprehend the capabilities of chatbots and enabling well-informed decision-making during chatbot installation, the research seeks to offer insightful information about the state of chatbot technology today. In order to give a thorough evaluation of chatbot efficacy, the study also explores important factors including emotional presentation, management of objectionable information, and flexibility to human input. Key features like discussion applicability, cost-effectiveness, accessibility, quality of training data, answer relevance, bias control and emotional expression were rigorously examined through a comprehensive review procedure that involved specific inquiries directed to different chatbots."
616,2024,"University Students' Perception and Expectations of Generative AI Tools for Software Engineering nan Adopting Generative Artificial Intelligence (AI) tools in software engineering represents a shift in how tasks like coding and idea generation are approached. This paper investigates university students' perceptions and expectations regarding the use of Generative AI tools such as ChatGPT and Copilot in software engineering. To achieve this, we conducted a questionnaire study with volunteer participants studying at Uppsala University in Sweden, resulting in a total of 127 responses. These responses were about the usage preferences, motivations for adoption, perceived benefits, encountered challenges, and suggested improvement of these tools. The findings reveal that 16% of participants have never used a Generative AI tool, while of those who have used such tools predominantly use ChatGPT3.5. Among users of Generative AI, respondents reported benefits such as code optimization and idea generation, alongside challenges such as inaccuracies in generated content and understanding user intent. Despite these challenges, participants perceive the integration of Generative AI tools as transformative for traditional software engineering practices. The results of this paper offer insights into the practical use of AI tools and suggestions for improving their functionality, thereby influencing the future direction of software engineering education."
617,2024,"When to Show a Suggestion? Integrating Human Feedback in AI-Assisted Programming nan AI powered code-recommendation systems, such as Copilot and CodeWhisperer, provide code suggestions inside a programmer's environment (e.g., an IDE) with the aim of improving productivity. We pursue mechanisms for leveraging signals about programmers' acceptance and rejection of code suggestions to guide recommendations. We harness data drawn from interactions with GitHub Copilot, a system used by millions of programmers, to develop interventions that can save time for programmers. We introduce a utility-theoretic framework to drive decisions about suggestions to display versus withhold. The approach, conditional suggestion display from human feedback (CDHF), relies on a cascade of models that provide the likelihood that recommended code will be accepted. These likelihoods are used to selectively hide suggestions, reducing both latency and programmer verification time. Using data from 535 programmers, we perform a retrospective evaluation of CDHF and show that we can avoid displaying a significant fraction of suggestions that would have been rejected. We further demonstrate the importance of incorporating the programmer's latent unobserved state in decisions about when to display suggestions through an ablation study. Finally, we showcase how using suggestion acceptance as a reward signal for guiding the display of suggestions can lead to suggestions of reduced quality, indicating an unexpected pitfall."
618,2024,"Exploring a Behavioral Model of Positive Friction in Human-AI Interaction [arXiv] nan Designing seamless, frictionless user experiences has long been a dominant trend in both applied behavioral science and artificial intelligence (AI), in which the goal of making desirable actions easy and efficient informs efforts to minimize friction in user experiences. However, in some settings, friction can be genuinely beneficial, such as the insertion of deliberate delays to increase reflection, preventing individuals from resorting to automatic or biased behaviors, and enhancing opportunities for unexpected discoveries. More recently, the popularization and availability of AI on a widespread scale has only increased the need to examine how friction can help or hinder users of AI; it also suggests a need to consider how positive friction can benefit AI practitioners, both during development processes (e.g., working with diverse teams) and to inform how AI is designed into offerings. This paper first proposes a positive friction model that can help characterize how friction is currently beneficial in user and developer experiences with AI, diagnose the potential need for friction where it may not yet exist in these contexts, and inform how positive friction can be used to generate solutions, especially as advances in AI continue to be progress and new opportunities emerge. It then explores this model in the context of AI users and developers by proposing the value of taking a hybrid AI+human lens, and concludes by suggesting questions for further exploration."
619,2023,"Five Futures with AI Coding Agents nan Many computer programmers are beginning to use computational agents to help them develop software. This article raises questions about the nature of programmer-to-agent relationships. The author's intent is to foster thought that will help human programmers best prepare for such relationships and perhaps design the relationships, ultimately keeping their jobs and improving their programming experience."
620,2024,"An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project [arXiv] nan Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs. In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration."
621,2024,"Enhancing Programming Error Messages in Real Time with Generative AI nan Generative AI is changing the way that many disciplines are taught, including computer science. Researchers have shown that generative AI tools are capable of solving programming problems, writing extensive blocks of code, and explaining complex code in simple terms. Particular promise has been shown in using generative AI to enhance programming error messages. Both students and instructors have complained for decades that these messages are often cryptic and difficult to understand. Yet recent work has shown that students make fewer repeated errors when enhanced via GPT-4. We extend this work by implementing feedback from ChatGPT for all programs submitted to our automated assessment tool, Athene, providing help for compiler, run-time, and logic errors. Our results indicate that adding generative AI to an automated assessment tool does not necessarily make it better and that design of the interface matters greatly to the usability of the feedback that GPT-4 provided."
623,2024,"AI in Software Development and Its Potential Influence on Accessibility Compliance. nan As artificial intelligence (AI) becomes increasingly integral to various sectors, its potential to enhance accessibility in digital services is of growing interest. This paper explores the influence of AI systems (AIS) on accessibility compliance, specifically within the context of Norway's national e-health service provider Norsk helsenett (NHN). Through a qualitative analysis, we assess how AISs are deployed in frontend programming, user interface design, and accessibility testing at NHN. Our study reveals that while AI has the potential to improve accessibility, it also presents challenges that require robust domain knowledge and effective human oversight. We explore the benefits and limitations using ChatGPT in accessibility testing, GitHub Copilot for generating accessible code, the implications of AI driven design tools, as well as the key differences between developing websites with existing component libraries and developing a simpler website from scratch. This paper aims to contribute to the ongoing discourse on AI and accessibility, offering insights and recommendations for future research and practical applications."
624,2023,"Generative AI [arXiv] nan The term generative AI refers to computational techniques that are capable of generating seemingly new, meaningful content such as text, images, or audio from training data. The widespread diffusion of this technology with examples such as Dall-E 2, GPT-4, and Copilot is currently revolutionizing the way we work and communicate with each other. In this article, we provide a conceptualization of generative AI as an entity in socio-technical systems and provide examples of models, systems, and applications. Based on that, we introduce limitations of current generative AI and provide an agenda for Business & Information Systems Engineering (BISE) research. Different from previous works, we focus on generative AI in the context of information systems, and, to this end, we discuss several opportunities and challenges that are unique to the BISE community and make suggestions for impactful directions for BISE research."
625,2023,"Teaching IT Software Fundamentals: Strategies and Techniques for Inclusion of Large Language Models: Strategies and Techniques for Inclusion of Large Language Models nan This paper argues for the inclusion of tools that utilize Artificial Intelligence (AI) Large Language Models (LLMs) in information technology (IT) undergraduate courses that teach the fundamentals of software. LLM tools have become widely available and disrupt traditional methods for teaching software concepts. Learning objectives are compromised when students submit AI-generated code for a classroom assignment without comprehending or validating the code. Since LLM tools including OpenAI Codex, Copilot by GitHub, and ChatGPT are being used in industry for software development, students need to be familiar with their use without compromising student learning. Incorporating LLM tools into the curriculum prepares students for real-world software development. However, students still need to understand software fundamentals including how to write and debug code. There are many challenges associated with the inclusion of AI tools into the IT curriculum that need to be addressed and mitigated. This paper presents strategies and techniques to integrate student use of LLM tools, assist students' interaction with the tools, and help prepare students for careers that increasingly use AI tools to design, develop, and maintain software."
626,2023,"Using new AI-driven techniques to ease serious games authoring nan Serious games are videogames whose purpose goes beyond mere entertainment. However, serious games use in mainstream education is still limited. The development of serious games is an expensive and complex process that requires the participation of different experts (e.g. domain, educators, graphic artists, and programmers). We consider that new generative AI techniques can help in the prototyping of serious games by reducing and automating some of the processes involved. There is increasing evidence that generative AI techniques such as ChatGPT or GitHub Copilot can increase the productivity of writing or coding tasks respectively. In our case, both prototyping and teaching serious games are complex because of the number and diversity of tasks involved and we are currently investigating whether AI techniques can be used to improve and simplify the process. For example, ChatGPT could support the process of creating the game narrative, and other systems such as Stable Diffusion could ease the creation of some of the graphics resources (e.g., for creating more cohesive and coherent backgrounds). Automating some of the costlier processes of game prototyping can contribute to creating better products by allowing the playtesting of different options for more effective games. As the field of generative AI is in continuous change, this paper presents a working methodology to simplify the development of serious games, that has been instantiated with concrete tools. This working methodology has been piloted effectively by one student from a Master of Design for the development of a serious game and will be tested in a serious games development course. In the article we also explore how these AI techniques can be combined with a game authoring environment such as uAdventure to systematize the development of serious games. The use of Generative AI offers great potential for improving the development of serious games and needs to be further researched alongside its applications for game-based learning education."
627,2023,"Large Language Models and Simple, Stupid Bugs [arXiv] nan With the advent of powerful neural language models, AI-based systems to assist developers in coding tasks are becoming widely available; Copilot is one such system. Copilot uses Codex, a large language model (LLM), to complete code conditioned on a preceding prompt. Codex, however, is trained on public GitHub repositories, viz., on code that may include bugs and vulnerabilities. Previous studies [1], [2] show Codex reproduces vulnerabilities seen in training. In this study, we examine how prone Codex is to generate an interesting bug category, single statement bugs, commonly referred to as simple, stupid bugs or SStuBs in the MSR community. We find that Codex and similar LLMs do help avoid some SStuBs, but do produce known, verbatim SStuBs as much as 2x as likely than known, verbatim correct code. We explore the consequences of the Codex generated SStuBs and propose avoidance strategies that suggest the possibility of reducing the production of known, verbatim SStubs, and increase the possibility of producing known, verbatim fixes."
628,2024,"How Useful are Current Chatbots Regarding Urology Patient Information? Comparison of the Ten Most Popular Chatbots' Responses About Female Urinary Incontinence nan This research evaluates the readability and quality of patient information material about female urinary incontinence (fUI) in ten popular artificial intelligence (AI) supported chatbots. We used the most recent versions of 10 widely-used chatbots, including OpenAI's GPT-4, Claude-3 Sonnet, Grok 1.5, Mistral Large 2, Google Palm 2, Meta's Llama 3, HuggingChat v0.8.4, Microsoft's Copilot, Gemini Advanced, and Perplexity. Prompts were created to generate texts about UI, stress type UI, urge type UI, and mix type UI. The modified Ensuring Quality Information for Patients (EQIP) technique and QUEST (Quality Evaluating Scoring Tool) were used to assess the quality, and the average of 8 well-known readability formulas, which is Average Reading Level Consensus (ARLC), were used to evaluate readability. When comparing the average scores, there were significant differences in the mean mQEIP and QUEST scores across ten chatbots (p = 0.049 and p = 0.018). Gemini received the greatest mean scores for mEQIP and QUEST, whereas Grok had the lowest values. The chatbots exhibited significant differences in mean ARLC, word count, and sentence count (p = 0.047, p = 0.001, and p = 0.001, respectively). For readability, Grok is the easiest to read, while Mistral is highly complex to understand. AI-supported chatbot technology needs to be improved in terms of readability and quality of patient information regarding female UI."
629,2024,"From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto [arXiv] nan Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed. We also propose possible research directions to overcome these gaps. We also see this work as a call to action to bring together the LLM and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making."
630,2023,"Towards more effective ai-assisted programming: a systematic design exploration to improve visual studio intellicode's user experience nan AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AI-driven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools."
631,2019,"How can geologic decision-making under uncertainty be improved? nan In the geosciences, recent attention has been paid to the influence of uncertainty on expert decision-making. When making decisions under conditions of uncertainty, people tend to employ heuristics (rules of thumb) based on experience, relying on their prior knowledge and beliefs to intuitively guide choice. Over 50 years of decision-making research in cognitive psychology demonstrates that heuristics can lead to less-than-optimal decisions, collectively referred to as biases. For example, the availability bias occurs when people make judgments based on what is most dominant or accessible in memory; geoscientists who have spent the past several months studying strike-slip faults will have this terrain most readily available in their mind when interpreting new seismic data. Given the important social and commercial implications of many geoscience decisions, there is a need to develop effective interventions for removing or mitigating decision bias.In this paper, we outline the key insights from decision-making research about how to reduce bias and review the literature on debiasing strategies. First, we define an optimal decision, since improving decision-making requires having a standard to work towards. Next, we discuss the cognitive mechanisms underlying decision biases and describe three biases that have been shown to influence geoscientists' decision-making (availability bias, framing bias, anchoring bias). Finally, we review existing debiasing strategies that have applicability in the geosciences, with special attention given to strategies that make use of information technology and artificial intelligence (AI). We present two case studies illustrating different applications of intelligent systems for the debiasing of geoscientific decision-making, wherein debiased decision-making is an emergent property of the coordinated and integrated processing of human-AI collaborative teams."
632,2024,"Generative AI for Cyber Security: Analyzing the Potential of ChatGPT, DALL-E, and Other Models for Enhancing the Security Space nan This research paper intends to provide real-life applications of Generative AI (GAI) in the cybersecurity domain. The frequency, sophistication and impact of cyber threats have continued to rise in today's world. This ever-evolving threat landscape poses challenges for organizations and security professionals who continue looking for better solutions to tackle these threats. GAI technology provides an effective way for them to address these issues in an automated manner with increasing efficiency. It enables them to work on more critical security aspects which require human intervention, while GAI systems deal with general threat situations. Further, GAI systems can better detect novel malware and threatening situations than humans. This feature of GAI, when leveraged, can lead to higher robustness of the security system. Many tech giants like Google, Microsoft etc., are motivated by this idea and are incorporating elements of GAI in their cybersecurity systems to make them more efficient in dealing with ever-evolving threats. Many cybersecurity tools like Google Cloud Security AI Workbench, Microsoft Security Copilot, SentinelOne Purple AI etc., have come into the picture, which leverage GAI to develop more straightforward and robust ways to deal with emerging cybersecurity perils. With the advent of GAI in the cybersecurity domain, one also needs to take into account the limitations and drawbacks that such systems have. This paper also provides some of the limitations of GAI, like periodically giving wrong results, costly training, the potential of GAI being used by malicious actors for illicit activities etc."
633,2020,Profiling Artificial Intelligence as a Material for User Experience Design nan nan
634,2022,"It would work for me too: How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools [arXiv] nan Software developers commonly engage in online communities to learn about new technologies. As revolutionary AI-powered code generation tools such as GitHub Copilot emerge, many developers are uncertain about how to trust them. While we see the promise of online communities in helping developers build appropriate trust in AI tools, we know little about how communities shape developers' trust in AI tools and how community features can facilitate trust in the design of AI tools. We investigate these questions through a two-phase study. Through an interview study with 17 developers, we unpack how developers in online communities collectively make sense of AI code generation tools by developing proper expectation, understanding, strategies, and awareness of broader implications, as well as how they leverage community signals to evaluate AI suggestions. We then surface design opportunities and conduct 11 design probe sessions to explore the design space of integrating a user community to AI code generation systems. We conclude with a series of design recommendations."
635,2019,"Designing Interactions with Intention-Aware Gaze-Enabled Artificial Agents nan As it becomes more common for humans to work alongside artificial agents on everyday tasks, it is increasingly important to design artificial agents that can understand and interact with their human counterparts naturally. We posit that an effective way to do this is to harness nonverbal cues used in human-human interaction. We, therefore, leverage knowledge from existing work on gaze-based intention recognition, where the awareness of gaze can provide insights into the future actions of an observed human subject. In this paper, we design and evaluate the use of a proactive intention-aware gaze-enabled artificial agent that assists a human player engaged in an online strategy game. The agent assists by recognising and communicating the intentions of a human opponent in real-time, potentially improving situation awareness. Our first study identifies the language requirements for the artificial agent to communicate the opponent's intentions to the assisted player, using an inverted Wizard of Oz method approach. Our second study compares the experience of playing an online strategy game with and without the assistance of the agent. Specifically, we conducted a within-subjects study with 30 participants to compare their experience of playing with (1) detailed AI predictions, (2) abstract AI predictions, and (3) no AI predictions but with a live visualisation of their opponent's gaze. Our results show that the agent can facilitate awareness of another user's intentions without adding visual distraction to the interface; however, the cognitive workload was similar across all three conditions, suggesting that the manner in which the agent communicates its predictions requires further exploration. Overall, our work contributes to the understanding of how to support human-agent teams in a dynamic collaboration scenario. We provide a positive account of humans interacting with an intention-aware artificial agent afforded by gaze input, which presents immediate opportunities for improving interactions between the counterparts."
636,2024,"Improving the Capabilities of Large Language Model Based Marketing Analytics Copilots With Semantic Search And Fine-Tuning nan Artificial intelligence (AI) is widely deployed to solve problems related to marketing attribution and budget optimization. However, AI models can be quite complex, and it can be difficult to understand model workings and insights without extensive implementation teams. In principle, recently developed large language models (LLMs), like GPT-4, can be deployed to provide marketing insights, reducing the time and effort required to make critical decisions. In practice, there are substantial challenges that need to be overcome to reliably use such models. We focus on domain-specific question-answering, SQL generation needed for data retrieval, and tabular analysis and show how a combination of semantic search, prompt engineering, and fine-tuning can be applied to dramatically improve the ability of LLMs to execute these tasks accurately. We compare both proprietary models, like GPT-4, and open-source models, like Llama-2-70b, as well as various embedding methods. These models are tested on sample use cases specific to marketing mix modeling and attribution."
637,2024,"Co-creation with AI in Car Design: A Diffusion Model Approach nan This study delves into the idea of AI co-creation in the car design domain by employing a unique methodology of training diffusion model to learn car design. Our primary objective is to investigate how the collaborative design process, where human designers and AI work together with the assist of pre-trained car design diffusion model, can revolutionize car design by enhancing creativity, efficiency, and overall user satisfaction. We built the car design dataset for the model training, which consists of real car photos and the design renderings together with the textual prompt of the images. Andwe invited the professional car designers to test the pre-trained car design diffusion model in the real design scenario. This research underscores the significance of AI co-creation in car design, showcasing the potential of AI technology to revolutionize the automotive industry. The findings suggest that collaborative AI co-creation approach had the ability to empower designers, enhance creativity, and redefine the future of car design. Project page https://automotive-design-copilot.super.site"
638,2024,"Cautious Optimism: The Influence of Generative AI Tools in Software Development Projects nan Generative artificial intelligence has emerged as a disruptive technology with the potential to transform traditional software development practices and methodologies. This study examines the implications of integrating AI tools in software development projects, focusing on potential benefits, challenges, and perceptions of the broader software development community. The study employs a qualitative methodology that captures the sentiments and personal adaptive measures from a diverse group of industry professionals who integrate generative AI tools such as ChatGPT and GitHub's Copilot in their software development projects. Findings suggest that generative AI tools aid developers in automating repetitive tasks, improve their workflow efficiency, reduce the coding learning curve, and complement traditional coding practices and project management techniques. However, generative AI tools also present ethical limitations, including privacy and security issues. The study also raises concerns regarding the long-term potential for job elimination (insecurity), over-reliance on generative AI assistance by developers, generativeAI lack of contextual understanding, and technical skills erosion. While developers are optimistic about the positive benefits of generative AI use within project environments in the short term, they also hold a pessimistic view in the longer term. There is a need for the software development projects community to critically assess the use of generative AI in software development projects while exploring how to retain the critical aspect of human oversight and judgment in the software development process in the long term."
640,2024,"Large language models can outperform humans in social situational judgments nan Large language models (LLM) have been a catalyst for the public interest in artificial intelligence (AI). These technologies perform some knowledge-based tasks better and faster than human beings. However, whether AIs can correctly assess social situations and devise socially appropriate behavior, is still unclear. We conducted an established Situational Judgment Test (SJT) with five different chatbots and compared their results with responses of human participants (N = 276). Claude, Copilot and you.com's smart assistant performed significantly better than humans in proposing suitable behaviors in social situations. Moreover, their effectiveness rating of different behavior options aligned well with expert ratings. These results indicate that LLMs are capable of producing adept social judgments. While this constitutes an important requirement for the use as virtual social assistants, challenges and risks are still associated with their wide-spread use in social contexts."
641,2023,"3DALL-E: Integrating Text-to-Image AI in 3D DesignWorkflows nan Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workfows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workfows and could use text-to-image AI to produce reference images, prevent design fxation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our fndings, we discuss how 3DALL-E can merge with existing generative design workfows and propose prompt bibliographies as a form of human-AI design history."
642,2023,"Towards an AI-centric Requirements Engineering Framework for Trustworthy AI nan Ethical guidelines are an asset for artificial intelligence(AI) development and conforming to them will soon be a procedural requirement once the EU AI Act gets ratified in the European parliament. However, developers often lack explicit knowledge on how to apply these guidelines during the system development process. A literature review of different ethical guidelines from various countries and organizations has revealed inconsistencies in the principles presented and the terminology used to describe such principles. This research begins by identifying the limitations of existing ethical AI development frameworks in performing requirements engineering (RE) processes during the development of trustworthy AI. Recommendations to address those limitations will be proposed to make the frameworks more applicable in the RE process to foster the development of trustworthy AI. This could lead to wider adoption, greater productivity of the AI systems, and reduced workload on humans for non-cognitive tasks. Considering the impact of some of the newer foundation models like GitHub Copilot and ChatGPT, the vision for this research project is to work towards the development of holistic operationalisable RE guidelines for the development and implementation of trustworthy AI not only on a product level but also on process level."
643,2024,"Students' Perspective on AI Code Completion: Benefits and Challenges [arXiv] nan AI Code Completion (e.g., GitHub's Copilot, Amazon CodeWhisperer) has revolutionized the way in which computer science students interact with programming languages. However, these tools are not available for free public use, preventing us from conducting our research. In addition, AI code completion has been studied from developers' perspective, not students' perspective who represent the future generation of our digital world. In this article, we investigated the benefits, challenges, and expectations of AI code completion from students' perspectives and introduced AutoAurora, an AI code completion tool integrated into the Visual Studio Code Extension as a research instrument. Through an interview study with ten participants, we found that AI code completion enhanced students' productivity and efficiency by providing correct syntax suggestions, offering alternative solutions, and functioning as a coding tutor. However, the over-reliance on AI code completion may lead to a surface-level understanding of programming concepts, diminishing problem-solving skills and restricting creativity. In the future, AI code completion must be explainable to facilitate the learning of coding concepts."
644,2024,"Learn to Code Sustainably: An Empirical Study on Green Code Generation nan The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generated code considered in this study is produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's green capacity, based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development."
645,2024,"Comparison and Quantification of GAI Tools Use Among Different Academic Population Segments nan This study explores the Generative Artificial Intelligence (GAI) field, focusing on its integration within the academic community of University North in Croatia. Specifically, it compares and quantifies the usage of advanced GAI tools such as ChatGPT, Microsoft Copilot (Bing AI), Gemini (Google Bard), and others within the different user groups - including students (undergraduate, graduate, doctoral students), and teachers/researchers. The objective was to discern the varying usage patterns and acceptance levels among these groups, particularly in the context of research and work. Our findings indicate significant variations among those groups when considering both GAI users and non-users. However, once we narrow the analysis to GAI users only, the differences between different groups diminish. The study concluded that the most widely accepted and used GAI tool was ChatGPT, with undergraduate and doctoral students adopting it at the highest rates. Also, self-assessed GAI proficiency was higher for individuals who use those tools longer and for teachers/researchers and doctoral students who use them for more complex tasks. Doctoral students were more likely to pay for premium features."
646,2024,"A Tutorial on Software Engineering for FMware nan Foundation Models (FMs) like GPT-4 have given rise to FMware, FMpowered applications representing a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. In our tutorial, we will present the latest research and industrial practices in engineering FMware, along with a hands-on session to acquaint attendees with core tools and techniques to build FMware. Our tutorial's perspective is firmly rooted in SE rather than artificial intelligence (AI), ensuring that participants are spared from delving into mathematical and AI-related intricacies unless they are crucial for introducing SE challenges and opportunities."
647,2023,"Smart Decentralized Autonomous Organizations and Operations for Smart Societies: Human-Autonomous Organizations for Industry 5.0 and Society 5.0 nan This article explores the concept of human-autonomous organizations (HAOs) based on decentralized autonomous organizations (DAOs) and operations as well as human, artificial, natural, and organizational intelligence and their roles in shaping smart societies in the context of Industry 5.0 and Society 5.0. It discusses the potential of AI-generated content and prompt engineering in specific goal-guided manufacture and governance. Additionally, the article introduces the concept of the HAO as a framework for integrating human intelligence to achieve fair, transparent, and accountable decision making within DAOs. The proposed HAO reduces the risk of instability and unreliability in human-in-the-loop copilot systems and human-machine hybrid systems, leading to more reliable, secure, and flexible systems. It provides insights into the future management of smart societies and the symbiotic relationship between human ingenuity and the suite of emerging new AI technologies."
648,2023,"Consistency of code: a prompt based approach to comprehend functionality nan Large language model (LLM)-based AI for code model (e.g., Copilot) demonstrates the potential of using AI in specialized domains such as software engineering. While previous research has focused on fine-tuning models with additional data and computational cost to construct models optimized for specific domains, our research focuses on prompt engineering methods that maximize the performance of existing models. We conducted a quantitative and qualitative user study using the AI for code model and identified two limitations that hinder the recommendation performance of the model. We propose two methods to address these limitations through effective prompt engineering. Finally, we identified the potential for the use of our proposed methods to be utilized and discussed the direction of future research for the effective use of the LLM."
651,2024,"Innovative approaches to high school physics competitions: Harnessing the power of AI and open science nan High school physics competitions serve as a platform for talented students to showcase their skills, engage in challenging problems, and foster a passion for science. This paper explores innovative approaches to enhance these competitions by harnessing the power of open science and artificial intelligence (AI) tools. Particularly we delve into the capabilities of state-of-the-art AI chatbots, i.e. ChatGPT, Bard, Claude, related to problem solving in physics. Together with open science tools like SageMath and Jupyter AI, they have the potential to serve as intelligent, powerful co-pilots, tutors, and assistants in understanding and applying physics, as well as knowledge from connected STEM fields. Furthermore, these innovative approaches can revolutionize high school physics competitions, providing students and their tutors with powerful resources to excel in their scientific pursuits."
652,2023,"Evaluating the Performance of AI Models for Code Generation: Capabilities, Limitations, and Potential Paths to Improvement nan nan"
655,2023,"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation nan The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community."
656,2024,"Ethical Incorporation of Artificial Intelligence into Neurosurgery: A Generative Pretrained Transformer Chatbot-Based, Human-Modified Approach nan - INTRODUCTION: Artificial intelligence (AI) has become increasingly used in neurosurgery. Generative pretrained transformers (GPTs) have been of particular interest. However, ethical concerns regarding the incorporation of AI into the field remain underexplored. We delineate key ethical considerations using a novel GPT-based, humanmodified approach, synthesize the most common considerations, and present an ethical framework for the involvement of AI in neurosurgery. - METHODS: GPT-4, ChatGPT, Bing Chat/Copilot, You, Perplexity.ai, and Google Bard were queried with the prompt How can artificial intelligence be ethically incorporated into neurosurgery? . Then, a layered GPTbased thematic analysis was performed. The authors synthesized the results into considerations for the ethical incorporation of AI into neurosurgery. Separate Pareto analyses with 20% threshold and 10% threshold were conducted to determine salient themes. The authors refined these salient themes. - RESULTS: Twelve key ethical considerations focusing on stakeholders, clinical implementation, and governance were identified. Refinement of the Pareto analysis of the top 20% most salient themes in the aggregated GPT outputs yielded 10 key considerations. Additionally, from the top 10% most salient themes, 5 considerations were retrieved. An ethical framework for the use of AI in neurosurgery was developed. - CONCLUSIONS: It is critical to address the ethical considerations associated with the use of AI in neurosurgery. The framework described in this manuscript may facilitate the integration of AI into neurosurgery, benefitting both patients and neurosurgeons alike. We urge neurosurgeons to use AI only for validated purposes and caution against automatic adoption of its outputs without neurosurgeon interpretation."
657,2023,"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming nan AI code generators like OpenAI Codex have the potential to assist novice programmers by generating code from natural language descriptions, however, over-reliance might negatively impact learning and retention. To explore the implications that AI code generators have on introductory programming, we conducted a controlled experiment with 69 novices (ages 10-17). Learners worked on 45 Python code-authoring tasks, for which half of the learners had access to Codex, each followed by a code-modification task. Our results show that using Codex significantly increased code-authoring performance (1.15x increased completion rate and 1.8x higher scores) while not decreasing performance on manual code-modifcation tasks. Additionally, learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance. Of interest, learners with higher Scratch pre-test scores performed significantly better on retention post-tests, if they had prior access to Codex."
658,2021,"Artificial intelligence as ally in hazard analysis nan Hazard analysis techniques have been around for many years, and have proven effective in the prevention of incidents and no doubt the saving of lives. Process hazard analysis (PHA) is now fairly robust and regulated, focused on overarching risks associated with the safe handling of hazardous materials and approaches to engineer-out such risks. Occupational hazard analysis (OHA) is keenly focused on human activity, and personal protection in hazardous working conditions. Both approaches are critical - but are often carried out separately, by different parts of an organization, which could result in an incomplete picture of the full set of operational risks in the field. Developing a holistic picture of both past and present dangers calls for a deep exploration of evidence. HAZOPs, PHA's, incident records and investigations provide expert analysis of hazards and mitigating strategies. Near-miss reports and safety observations add a large amount of information as well; the reporting frequency of these leading indicators can be both a blessing and a curse, as time and available resources constrain the ability to analyze and detect hazard signals within. As important as analyzing the historical record is for lessons learned, the more recent observations could indicate new hazards or highlight concerning trends. These could feed valuable real time information back to operations and maintenance teams to improve risk assessments and task planning. Enter artificial intelligence (AI) as a means to analyze the large amount of written hazard analyses, reports and observations to quickly extract insights around hazardous conditions, activities, incident causes and risk mitigation measures. Trained to understand concepts and contexts in both process and personal safety, AI can provide a natural-language information exploration environment for scanning thousands of documents in seconds and present common themes and related records. Not unlike us humans, AI learns from the past, informs the present and can help reduce risks in the future."
659,2024,"Lessons from Building StackSpot AI: A Contextualized AI Coding Assistant nan With their exceptional natural language processing capabilities, tools based on Large Language Models (LLMs) like ChatGPT and CoPilot have swiftly become indispensable resources in the software developer's toolkit. While recent studies suggest the potential productivity gains these tools can unlock, users still encounter drawbacks, such as generic or incorrect answers. Additionally, the pursuit of improved responses often leads to extensive prompt engineering efforts, diverting valuable time from writing code that delivers actual value. To address these challenges, a new breed of tools, built atop LLMs, is emerging. These tools aim to mitigate drawbacks by employing techniques like fine-tuning or enriching user prompts with contextualized information.In this paper, we delve into the lessons learned by a software development team venturing into the creation of such a contextualized LLM-based application, using retrieval-based techniques, called StackSpot AI. Over a four-month period, the team, despite lacking prior professional experience in LLM-based applications, built the product from scratch. Following the initial product release, we engaged with the development team responsible for the code generative components. Through interviews and analysis of the application's issue tracker, we uncover various intriguing challenges that teams working on LLM-based applications might encounter. For instance, we found three main group of lessons: LLM-based lessons, User-based lessons, and Technical lessons. By understanding these lessons, software development teams could become better prepared to build LLM-based applications."
660,2023,"Beyond Text-to-Image: Multimodal Prompts to Explore Generative AI nan Text-to-image AI systems have proven to have extraordinary generative capacities that have facilitated widespread adoption. However, these systems are primarily text-based, which is a fundamental inversion of what many artists are traditionally used to: having full control over the composition of their work. Prior work has shown that there is great utility in using text prompts and that AI augmented workflows can increase momentum on creative tasks for end users. However, multimodal interactions beyond text need to be further defined, so end users can have rich points of interaction that allow them to truly co-pilot AI-generated content creation. To this end, the goal of my research is to equip creators with workflows that 1) translate abstract design goals into prompts of visual language, 2) structure exploration of design outcomes, and 3) integrate creator contributions into generations."
661,2023,"Exploring the Pedagogical Use of AI-Powered Chatbots Educational Perceptions and Practices nan By providing personalized and adaptable learning experiences, artificial intelligence (AI) has the potential to revolutionize education. One promising development in this field is the use of generative artificial intelligence technology, such as the ChatGPT conversational agent. Chatbot technology has the potential to revolutionize the way educational content is accessed, created, understood, and implemented. In this research paper, we examine the current state of AI-powered chatbots in education and discuss the advantages and disadvantages of using chatbots in this context. A case study of an AI unit planner (Copilot) built on top of the GPT-3 application in K-12 settings will also be presented, examining how chatbots are used to assist teachers in lesson design and the extent to which teachers are familiar with them. Finally, the implications of chatbot technology for the future of education are discussed, along with recommendations for future research directions."
662,2024,"AI Chatbots for Mental Health: A Scoping Review of Effectiveness, Feasibility, and Applications nan Mental health disorders are a leading cause of disability worldwide, and there is a global shortage of mental health professionals. AI chatbots have emerged as a potential solution, offering accessible and scalable mental health interventions. This study aimed to conduct a scoping review to evaluate the effectiveness and feasibility of AI chatbots in treating mental health conditions. A literature search was conducted across multiple databases, including MEDLINE, Scopus, and PsycNet, as well as using AI-powered tools like Microsoft Copilot and Consensus. Relevant studies on AI chatbot interventions for mental health were selected based on predefined inclusion and exclusion criteria. Data extraction and quality assessment were performed independently by multiple reviewers. The search yielded 15 eligible studies covering various application areas, such as mental health support during COVID-19, interventions for specific conditions (e.g., depression, anxiety, substance use disorders), preventive care, health promotion, and usability assessments. AI chatbots demonstrated potential benefits in improving mental and emotional well-being, addressing specific mental health conditions, and facilitating behavior change. However, challenges related to usability, engagement, and integration with existing healthcare systems were identified. AI chatbots hold promise for mental health interventions, but widespread adoption hinges on improving usability, engagement, and integration with healthcare systems. Enhancing personalization and context-specific adaptation is key. Future research should focus on large-scale trials, optimal human-AI integration, and addressing ethical and social implications."
663,2024,"Comparison of Large Language Models in Diagnosis and Management of Challenging Clinical Cases. nan Purpose: Compare large language models (LLMs) in analyzing and responding to a difficult series of ophthalmic cases.Design: A comparative case series involving LLMs that met inclusion criteria tested on twenty difficult case studies posed in open-text format.Methods: Fifteen LLMs accessible to ophthalmologists were tested against twenty case studies published in JAMA Ophthalmology. Each case was presented in identical, open-ended text fashion to each LLM and open-ended responses regarding differential diagnosis, next diagnostic tests and recommended treatments were requested. Responses were recorded and assessed for accuracy against published correct answers. The main outcome was accuracy of LLMs against the correct answers. Secondary outcomes included comparative performance on the differential diagnosis, ancillary testing, and treatment subtests; and readability of responses.Results: Scores were normally distributed and ranged from 0-35 (with a maximum score of 60) with a mean  standard deviation of 19  9. Scores for three of the LLMs (ChatGPT 3.5, Claude Pro, and Copilot Pro) were statistically significantly higher than the mean. Two of the high-performing LLMs were paid subscription (Claude Pro and Copilot Pro) and one was free (ChatGPT 3.5). While there were no clinical or statistical differences between ChatGPT 3.5 and Claude Pro, a separation of +5 points, or 0.56 standard deviations, between Copilot Pro and the other highly ranked LLMs was present. Readability of all tested programs were above the AMA (American Medical Association) reading level recommendations to public consumers of eight grade.Conclusion: Subscription LLMs were more prevalent among highly ranked LLMs suggesting that these perform better as ophthalmic assistants. While readability was poor for the average person, the content was understood by a board-certified ophthalmologist. The accuracy of LLMs is not high enough to recommend patient care in standalone mode, but aiding clinicians in patient care and prevent oversights is promising."
664,2024,"AI Tool Use and Adoption in Software Development by Individuals and Organizations: A Grounded Theory Study nan AI assistance tools such as ChatGPT, Copilot, and Gemini have dramatically impacted the nature of software development in recent years. Numerous studies have studied the positive benefits that practitioners have achieved from using these tools in their work. While there is a growing body of knowledge regarding the usability aspects of leveraging AI tools, we still lack concrete details on the issues that organizations and practitioners need to consider should they want to explore increasing adoption or use of AI tools. In this study, we conducted a mixed methods study involving interviews with 26 industry practitioners and 395 survey respondents. We found that there are several motives and challenges that impact individuals and organizations and developed a theory of AI Tool Adoption. For example, we found creating a culture of sharing of AI best practices and tips as a key motive for practitioners' adopting and using AI tools. In total, we identified 2 individual motives, 4 individual challenges, 3 organizational motives, and 3 organizational challenges, and 3 interleaved relationships. The 3 interleaved relationships act in a push-pull manner where motives pull practitioners to increase the use of AI tools and challenges push practitioners away from using AI tools."
665,2024,"Opportunities and Considerations for the Incorporation of Artificial Intelligence into Global Neurosurgery: A Generative Pretrained Transformer Chatbot-Based Approach nan OBJECTIVE: Global neurosurgery is a public health focus in neurosurgery that seeks to ensure safe, timely, and affordable neurosurgical care to all individuals worldwide. Although investigators have begun to explore the promise of artificial intelligence (AI) for neurosurgery, its applicability to global neurosurgery has been largely hypothetical. We characterize opportunities and considerations for the incorporation of AI into global neurosurgery by synthesizing key themes yielded from a series of generative pretrained transformers (GPTs), discuss important limitations of GPTs and cautions when using AI in neurosurgery, and develop a framework for the equitable incorporation of AI into global neurosurgery. METHODS: ChatGPT, Bing Chat/Copilot, You, Perplexity.ai, and Google Bard were queried with the prompt How can AI be incorporated into global neurosurgery?  A layered ChatGPT-based thematic analysis was performed. The authors synthesized the results into opportunities and considerations for the incorporation of AI in global neurosurgery. A Pareto analysis was conducted to determine common themes. RESULTS: Eight opportunities and 14 important considerations were synthesized. Six opportunities related to patient care, 1 to education, and another to public health planning. Four of the important considerations were deemed specific to global neurosurgery. The Pareto analysis included all 8 opportunities and 5 considerations. CONCLUSIONS: AI may be incorporated into global neurosurgery in a variety of capacities requiring numerous considerations. The framework presented in this manuscript may facilitate the incorporation of AI into global neurosurgery initiatives while balancing contextual factors and the reality of limited resources."
666,2024,"Using LLM Artificial Intelligence Systems as Complex SQL Programming Assistants nan Learning database programming such as SQL programming is a challenging task when the queries become more complex. SQL is a declarative language based on relational calculus which describes the definition of the query results instead of describing the procedure or steps used to obtain the query result. Tutorial sessions using tutorial assistances are generally required to support the learning of advanced part of the language. Recently generative AI systems demonstrated question answering capabilities including programming codes generation. This paper verifies the SQL code generating capabilities of four generative AI systems: Bing, Bard, ChatGPT, and Copilot and their suitability as SQL programming assistants."
667,1990,"COPILOTa PC-based expert system for reactor operational assistance using a Bayesian diagnostic module nan The authors describe an expert system for nuclear plant operational assistance built around the idea of Bayesian diagnosis. A key feature of the system is its ability to handle uncertainties, human operator actions, and time-dependent or trajectory-type input information. The hierarchical structure of a sample knowledge base is shown for a typical pressurized water reactor, and numerical examples are presented."
668,2023,Automated Cyberbullying Detection Using Transformer Based Architecture nan nan
669,2024,"Chart What I Say: Exploring Cross-Modality Prompt Alignment in AI-Assisted Chart Authoring nan Recent chart-authoring systems, such as Amazon Q in QuickSight and Copilot for Power BI, demonstrate an emergent focus on supporting natural language input to share meaningful insights from data through chart creation. Currently, chart-authoring systems tend to integrate voice input capabilities by relying on speech-to-text transcription, processing spoken and typed input similarly. However, cross-modality input comparisons in other interaction domains suggest that the structure of spoken and typed-in interactions could notably differ, reflecting variations in user expectations based on interface affordances. Thus, in this work, we compare spoken and typed instructions for chart creation. Findings suggest that while both text and voice instructions cover chart elements and element organization, voice descriptions have a variety of command formats, element characteristics, and complex linguistic features. Based on these findings, we developed guidelines for designing voice-based authoring-oriented systems and additional features that can be incorporated into existing text-based systems to support speech modality."
670,2020,"Artificial Intelligence Chatbot Behavior Change Model for Designing Artificial Intelligence Chatbots to Promote Physical Activity and a Healthy Diet: Viewpoint nan Background: Chatbots empowered by artificial intelligence (AI) can increasingly engage in natural conversations and build relationships with users. Applying AI chatbots to lifestyle modification programs is one of the promising areas to develop cost-effective and feasible behavior interventions to promote physical activity and a healthy diet.Objective: The purposes of this perspective paper are to present a brief literature review of chatbot use in promoting physical activity and a healthy diet, describe the AI chatbot behavior change model our research team developed based on extensive interdisciplinary research, and discuss ethical principles and considerations.Methods: We conducted a preliminary search of studies reporting chatbots for improving physical activity and/or diet in four databases in July 2020. We summarized the characteristics of the chatbot studies and reviewed recent developments in human-AI communication research and innovations in natural language processing. Based on the identified gaps and opportunities, as well as our own clinical and research experience and findings, we propose an AI chatbot behavior change model.Results: Our review found a lack of understanding around theoretical guidance and practical recommendations on designing AI chatbots for lifestyle modification programs. The proposed AI chatbot behavior change model consists of the following four components to provide such guidance: (1) designing chatbot characteristics and understanding user background; (2) building relational capacity; (3) building persuasive conversational capacity; and (4) evaluating mechanisms and outcomes. The rationale and evidence supporting the design and evaluation choices for this model are presented in this paper.Conclusions: As AI chatbots become increasingly integrated into various digital communications, our proposed theoretical framework is the first step to conceptualize the scope of utilization in health behavior change domains and to synthesize all possible dimensions of chatbot features to inform intervention design and evaluation. There is a need for more interdisciplinary work to continue developing AI techniques to improve a chatbot's relational and persuasive capacities to change physical activity and diet behaviors with strong ethical principles."
671,2023,"Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation nan We evaluate AI-assisted generative capabilities on fundamental numerical kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG. We test the generated kernel codes for a variety of language-supported programming models, including (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numpy, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by the GPT-based OpenAI Codex available in Visual Studio Code as of April 2023 to generate a vast amount of implementations given simple + + prompt variants. To quantify and compare the results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding code keywords, while Julia prompts perform acceptably well for its mature programming models (e.g., Threads and CUDA.jl). We expect for these benchmarks to provide a point of reference for each programming model's community. Overall, understanding the convergence of large language models, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions."
672,2024,Context-Aware Machine Learning for Low-Burden Brain-Computer Interfaces nan nan
673,2024,"The Possibility of Writing Education through Content Creation nan This paper proposes an example of a story writing class, to show that creative writing can be another good choice for university writing and liberal arts education. is a liberal arts course of A university aimed at strengthening writing skills and providing an experience to write stories for media content creation. has several advantages; first, it deals with media content, which is a major cultural activity of college students. This means that students can easily open their minds in class and freely imagine and be creative through this course. In addition, creating stories arouses students interest in writing and improves their writing skills while accumulating writing experience.Second, students develop critical thinking and self-expression skills through group discussions and media reviews. All activities to foster communication and writing skills in college writingwriting, correction, discussion, presentation, group activities, colleague feedback, etc.could be applied to creative writing equally. It means, story creation classes can be another solution for writing education in universities.Third, students collaborate and fuse their various majors/expertise during team projects. They used web platforms to communicate at any time, create stories together, and design digital media content. Furthermore, students used AI technology and this means cooperation of human and post-human(AI). Therefore, in , we can see the possibility of realizing ideal liberal arts education which is creative and integrated.In conclusion, if creative activities can be carried out in liberal arts writing classes in connection with cultural content, much more students will be able to participate in creativity education. Above all, reflects the realistic needs and interests of students to create content, so it is a liberal arts course suitable for expanding the demand for writing education and developing digital literacy and humanities competencies."
674,2001,"Evaluation of a fuzzy system-based automotive copilot dedicated to lateral guidance nan A fuzzy copilot system called RPV has been developed to lighten the driver's task in the lateral control of his vehicle. Fuzzy systems were designed and embedded with various sensors on a passenger car to perform the lateral control of the vehicle. A fuzzy controller outputs a torque command, which is applied to the steering column to steer the vehicle along a desired immaterial trajectory, while a fuzzy diagnosis monitors the driver's steering actions and intentions. This enables a supervisor to trigger the adequate driving strategy in accordance with driver expectations. This paper presents encouraging results from field-test experiments based on drivers' point of view, taking benefit from a positive cooperation of fuzzy logic and ergonomics."
675,2023,"How Secure is AI-based Coding?: A Security Analysis Using STRIDE and Data Flow Diagrams nan The widespread adoption of Artificial Intelligence (AI)-based coding tools such as ChatGPT, Copilot, Open AI Codex, and Tabnine necessitates a comprehensive evaluation of their security measures, aiming to identify potential threats and vulnerabilities. In this paper, we present a systematic and structured approach to conducting a threat model-based security analysis using the widely accepted STRIDE threat model and data flow diagrams (DFD) for AI-based coding tools. By establishing clear system boundaries and constructing detailed data flow diagrams, the data flow within the system is visually represented. Then we apply the STRIDE threat modeling, encompassing spoofing, tampering, repudiation, information disclosure, denial of service, and elevation of privilege, to thoroughly examine potential threats and prioritize threats based on their impact, which allows us to develop targeted mitigation strategies. Our threat model can provide organizations with a robust methodology to ensure the security of their AI-based coding tools and proactively address emerging risks to build a trustworthy coding environment."
676,2023,"With ChatGPT, do we have to rewrite our learning objectives -- CASE study in Cybersecurity [arXiv] nan With the emergence of Artificial Intelligent chatbot tools such as ChatGPT and code writing AI tools such as GitHub Copilot, educators need to question what and how we should teach our courses and curricula in the future. In reality, automated tools may result in certain academic fields being deeply reduced in the number of employable people. In this work, we make a case study of cybersecurity undergrad education by using the lens of ``Understanding by Design'' (UbD). First, we provide a broad understanding of learning objectives (LOs) in cybersecurity from a computer science perspective. Next, we dig a little deeper into a curriculum with an undergraduate emphasis on cybersecurity and examine the major courses and their LOs for our cybersecurity program at Miami University. With these details, we perform a thought experiment on how attainable the LOs are with the above-described tools, asking the key question ``what needs to be enduring concepts?'' learned in this process. If an LO becomes something that the existence of automation tools might be able to do, we then ask ``what level is attainable for the LO that is not a simple query to the tools?''. With this exercise, we hope to establish an example of how to prompt ChatGPT to accelerate students in their achievements of LOs given the existence of these new AI tools, and our goal is to push all of us to leverage and teach these tools as powerful allies in our quest to improve human existence and knowledge."
677,2024,"Reliability of a generative artificial intelligence tool for pediatric familial Mediterranean fever: insights from a multicentre expert survey nan BackgroundArtificial intelligence (AI) has become a popular tool for clinical and research use in the medical field. The aim of this study was to evaluate the accuracy and reliability of a generative AI tool on pediatric familial Mediterranean fever (FMF).MethodsFifteen questions repeated thrice on pediatric FMF were prompted to the popular generative AI tool Microsoft Copilot with Chat-GPT 4.0. Nine pediatric rheumatology experts rated response accuracy with a blinded mechanism using a Likert-like scale with values from 1 to 5.ResultsMedian values for overall responses at the initial assessment ranged from 2.00 to 5.00. During the second assessment, median values spanned from 2.00 to 4.00, while for the third assessment, they ranged from 3.00 to 4.00. Intra-rater variability showed poor to moderate agreement (intraclass correlation coefficient range: -0.151 to 0.534). A diminishing level of agreement among experts over time was documented, as highlighted by Krippendorff's alpha coefficient values, ranging from 0.136 (at the first response) to 0.132 (at the second response) to 0.089 (at the third response). Lastly, experts displayed varying levels of trust in AI pre- and post-survey.ConclusionsAI has promising implications in pediatric rheumatology, including early diagnosis and management optimization, but challenges persist due to uncertain information reliability and the lack of expert validation. Our survey revealed considerable inaccuracies and incompleteness in AI-generated responses regarding FMF, with poor intra- and extra-rater reliability. Human validation remains crucial in managing AI-generated medical information."
678,2024,"BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science nan Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle Understanding Literature into two atomic abilities, i) Understanding the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of Literature grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench."
679,2024,"How Do Data Analysts Respond to AI Assistance? A Wizard-of-Oz Study nan Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts' workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts' preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants."
680,2024,"Developers' Perspective on Today's and Tomorrow's Programming Tool Assistance: A Survey nan Software development is a complex activity that needs a lot of tool assistance. Over the years there has been a lot of effort put into development of automated assistance to help with activities such as detection of issues via program analysis, or refactoring of code. Recently, the landscape of developer tool assistance is being disrupted with the entry of AI tools, such as Copilot and ChatGPT, powered via Large Language Models. Other kinds of tool assistance, for instance, gaze-driven assistance, is around the corner. What are programmers' perceptions on tool assistance today? What do they see as good directions for the future?In this paper, we present the results of a survey where we asked developers about their programming practices, experience with program analysis, and attitudes and views on enabling technologies, like AI and eye-tracking.We received 68 replies from a diverse group of developers from 12 countries. We found that 50% of the participants use program analysis and that many participants (N=28) already use AI-enabled tools for programming. We found that our participants were positive toward AI-powered tools, neutral toward eye-tracking, and negative toward gamification. We discuss these and other findings and point out directions for future work."
681,2023,"An explanation space to align user studies with the technical development of Explainable AI nan Providing meaningful and actionable explanations for end-users is a situated problem requiring the intersection of multiple disciplines to address social, operational, and technical challenges. However, the explainable artificial intelligence community has not commonly adopted or created tangible design tools that allow interdisciplinary work to develop reliable AI-powered solutions. This paper proposes a formative architecture that defines the explanation space from a user-inspired perspective. The architecture comprises five intertwined components to outline explanation requirements for a task: (1) the end-users' mental models, (2) the end-users' cognitive process, (3) the user interface, (4) the Human-Explainer Agent, and (5) the agent process. We first define each component of the architecture. Then, we present the Abstracted Explanation Space, a modeling tool that aggregates the architecture's components to support designers in systematically aligning explanations with end-users' work practices, needs, and goals. It guides the specifications of what needs to be explained (content: end-users' mental model), why this explanation is necessary (context: end-users' cognitive process), to delimit how to explain it (format: Human-Explainer Agent and user interface), and when the explanations should be given. We then exemplify the tool's use in an ongoing case study in the aircraft maintenance domain. Finally, we discuss possible contributions of the tool, known limitations or areas for improvement, and future work to be done."
682,2024,"From Today's Code to Tomorrow's Symphony: The AI Transformation of Developer's Routine by 2030 nan In the rapidly evolving landscape of software engineering, the integration of Artificial Intelligence (AI) into the Software Development Life-Cycle (SDLC) heralds a transformative era for developers. Recently, we have assisted to a pivotal shift towards AI-assisted programming, exemplified by tools like GitHub Copilot and OpenAI's ChatGPT, which have become a crucial element for coding, debugging, and software design. In this paper we provide a comparative analysis between the current state of AI-assisted programming in 2024 and our projections for 2030, by exploring how AI advancements are set to enhance the implementation phase, fundamentally altering developers' roles from manual coders to orchestrators of AI-driven development ecosystems. We envision HyperAssistant, an augmented AI tool that offers comprehensive support to 2030 developers, addressing current limitations in mental health support, fault detection, code optimization, team interaction, and skill development. We emphasize AI as a complementary force, augmenting developers' capabilities rather than replacing them, leading to the creation of sophisticated, reliable, and secure software solutions. Our vision seeks to anticipate the evolution of programming practices, challenges, and future directions, shaping a new paradigm where developers and AI collaborate more closely, promising a significant leap in SE efficiency, security and creativity."
683,2024,"ChatGPT Combining Machine Learning for the Prediction of Nanozyme Catalytic Types and Activities nan The design of nanozymes with superior catalytic activities is a prerequisite for broadening their biomedical applications. Previous studies have exerted significant effort in theoretical calculation and experimental trials for enhancing the catalytic activity of nanozyme. Machine learning (ML) provides a forward-looking aid in predicting nanozyme catalytic activity. However, this requires a significant amount of human effort for data collection. In addition, the prediction accuracy urgently needs to be improved. Herein, we demonstrate that ChatGPT can collaborate with humans to efficiently collect data. We establish four qualitative models (random forest (RF), decision tree (DT), adaboost random forest (adaboost-RF), and adaboost decision tree (adaboost-DT)) for predicting nanozyme catalytic types, such as peroxidase, oxidase, catalase, superoxide dismutase, and glutathione peroxidase. Furthermore, we use five quantitative models (random forest (RF), decision tree (DT), Support Vector Regression (SVR), gradient boosting regression (GBR), and fully connected deep neuron network (DNN)) to predict nanozyme catalytic activities. We find that GBR model demonstrates superior prediction performance for nanozyme catalytic activities (R-2 = 0.6476 for Km and R-2 = 0.95 for Kcat). Moreover, an open-access web resource, AI-ZYMES, with a ChatGPT-based nanozyme copilot is developed for predicting nanozyme catalytic types and activities and guiding the synthesis of nanozyme. The accuracy of the nanozyme copilot's responses reaches more than 90% through the retrieval augmented generation. This study provides a new potential application for ChatGPT in the field of nanozymes."
684,2024,"Using AI Assistants in Software Development: A Qualitative Study on Security Practices and Concerns nan Following the recent release of AI assistants, such as OpenAI's ChatGPT and GitHub Copilot, the software industry quickly utilized these tools for software development tasks, e.g., generating code or consulting AI for advice. While recent research has demonstrated that AI-generated code can contain security issues, how software professionals balance AI assistant usage and security remains unclear. This paper investigates how software professionals use AI assistants in secure software development, what security implications and considerations arise, and what impact they foresee on secure software development. We conducted 27 semi-structured interviews with software professionals, including software engineers, team leads, and security testers. We also reviewed 190 relevant Reddit posts and comments to gain insights into the current discourse surrounding AI assistants for software development. Our analysis of the interviews and Reddit posts finds that despite many security and quality concerns, participants widely use AI assistants for security-critical tasks, e.g., code generation, threat modeling, and vulnerability detection. Their overall mistrust leads to checking AI suggestions in similar ways to human code, although they expect improvements and, therefore, a heavier use for security tasks in the future. We conclude with recommendations for software professionals to critically check AI suggestions, AI creators to improve suggestion security and capabilities for ethical security tasks, and academic researchers to consider general-purpose AI in software development."
685,2023,"Comparing Llama-2 and GPT-3 LLMs for HPC kernels generation [arXiv] nan We evaluate the use of the open-source Llama-2 model for generating well-known, high-performance computing kernels (e.g., AXPY, GEMV, GEMM) on different parallel programming models and languages (e.g., C++: OpenMP, OpenMP Offload, OpenACC, CUDA, HIP; Fortran: OpenMP, OpenMP Offload, OpenACC; Python: numpy, Numba, pyCUDA, cuPy; and Julia: Threads, CUDA.jl, AMDGPU.jl). We built upon our previous work that is based on the OpenAI Codex, which is a descendant of GPT-3, to generate similar kernels with simple prompts via GitHub Copilot. Our goal is to compare the accuracy of Llama-2 and our original GPT-3 baseline by using a similar metric. Llama-2 has a simplified model that shows competitive or even superior accuracy. We also report on the differences between these foundational large language models as generative AI continues to redefine human-computer interactions. Overall, Copilot generates codes that are more reliable but less optimized, whereas codes generated by Llama-2 are less reliable but more optimized when correct."
686,2024,"A Large-Scale Survey on the Usability of AI Programming Assistants: Successes and Challenges nan The software engineering community recently has witnessed widespread deployment of AI programming assistants, such as GitHub Copilot. However, in practice, developers do not accept AI programming assistants' initial suggestions at a high frequency. This leaves a number of open questions related to the usability of these tools. To understand developers' practices while using these tools and the important usability challenges they face, we administered a survey to a large population of developers and received responses from a diverse set of 410 developers. Through a mix of qualitative and quantitative analyses, we found that developers are most motivated to use AI programming assistants because they help developers reduce key-strokes, finish programming tasks quickly, and recall syntax, but resonate less with using them to help brainstorm potential solutions. We also found the most important reasons why developers do not use these tools are because these tools do not output code that addresses certain functional or non-functional requirements and because developers have trouble controlling the tool to generate the desired output. Our findings have implications for both creators and users of AI programming assistants, such as designing minimal cognitive effort interactions with these tools to reduce distractions for users while they are programming."
688,2019,"CAI4CAI: the rise of contextual artificial intelligence in computer assisted interventions [arXiv] nan Data-driven computational approaches have evolved to enable extraction of information from medical images with a reliability, accuracy and speed which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theatres are extremely complex and typically rely on poorly integrated intra-operative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer assisted interventions, we highlight the crucial need to take context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer assisted intervention, or CAI4CAI, arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human-AI actor team; how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision making ultimately producing more precise and reliable interventions. [doi:10.1109/JPROC.2019.2946993]."
689,2023,"PCR-Chain: Partial Code Reuse Assisted by Hierarchical Chaining of Prompts on Frozen Copilot nan API documentation, technical blogs and programming Q&A sites contain a large amount of partial code that can be reused in programming tasks. However, due to unresolved simple names and last-mile syntax errors, such partial code is frequently not compilable. To facilitate partial code reuse, we develop PCR-Chain for resolving FQNs and fixing last-mile syntax errors in partial code based on a giant pre-trained code model (e.g., Copilot). Methodologically, PCR-Chain is backed up by the underlying global-level prompt architecture (which combines three design ideas: hierarchical task breakdown, prompt composition including sequential and conditional structures, and a mix of prompt-based AI and non-AI units) and the local-level prompt design. Technically, we propose PCR-Chain, which employs in-context learning rather than supervised fine-tuning with gradient updates on downstream task data. This approach enables the frozen, giant pre-trained code model to learn the desired behavior for a specific task through behavior-describing prompts and imitate it to complete the task. Experimental results show that PCR-Chain automatically resolves the FQNs and fixes last-mile syntax errors in 50 partial code samples collected from Stack Overflow with high success rates, without requiring any program analysis. The correct execution of the unit, module, and PCR-Chain demonstrates the effectiveness of the prompt design, prompt composition, and prompt architecture. Website:https://github.com/SE-qinghuang/PCR-ChainDemoVideo: https://youtu.be/6HGRNc2JE"
690,2023,"Hybrid Intelligence: Collaboration with AI Systems for Knowledge Work nan With AI agents or generative AI systems such as ChatGPT/GPT-4, very powerful assistance systems will be widely available in the foreseeable future. These assistance systems can be used in a wide variety of occupational fields and for a wide variety of tasks. This raises questions about (1) the options for shaping the cooperation between humans and AI agents, (2) the interaction between human and artificial intelligence and (3) the competences required for successful cooperation with intelligent assistance systems. Previous modelling of AI competences remains rather general and is clearly not specified in terms of what knowledge, skills and attitudes are required for successful interaction with generative AI assistance systems such as ChatGPT, Midjourney or GitHub Copilot. This is the starting point for this paper, which is based on a deductive-conceptual approach as well as on the review and analysis of selected literature in the research fields of Human-Machine Collaboration and Hybrid Intelligence. With regard to the cooperation of humans and AI agents, different types of co-operation and levels of intensity of cooperation can be distinguished. In addition, different roles for AI agents as team members can also be distinguished (e.g., assis-tant, coordinator, doer, expert). Humans and AI agents bring different strengths to the collaboration, resulting in hybrid intelligence. However, successful collaboration also requires suitable framework conditions as well as attitudes and mindsets of the people involved (e.g.,a growth mindset). This is linked to important management tasks, such as establishing ethical guidelines or growth mindset cultures in companies and organisations."
691,2024,"AI for Low-Code for AI nan Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) textual programs, whereas visual tools abstract away most or all program text but struggle to provide easy access to large APIs. At their intersection, we propose LowCODER, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCODER(VP)) and an AI-powered natural language interface (LowCODER(NL)). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCODER, replacing the LowCODER(NL) component with a simple keyword search in half the tasks. Overall, we find that LowCODER is especially useful for (i) Discoverability: using LowCODER(NL), participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of AI with low-code programming."
692,2024,"Comparison of Large Language Models in Generating Machine Learning Curricula in High Schools nan With the rapid advancement of artificial intelligence technologies, the integration of AI concepts into educational curricula represents an increasingly important issue. This paper presents a comparative analysis of four AI large language models, ChatGPT (now GPT-4o), Bard (now Gemini), Copilot, and Auto-GPT, in the last year, progressing from the previous into the newer versions, thus also revealing the progress over time. Tasks were selected from the Valence project, which aims to advance machine learning in high school education with material designed by human experts. The four LLMs were assessed across 13 topics, 35 units, and 12 code segments, focusing on their code generation, definition formulation, and textual task capabilities. The results were analyzed using various metrics to conduct a comprehensive evaluation. Each LLM was allowed up to five attempts to produce outputs closely aligned with human-written materials, with experts providing iterative feedback. This study evaluated the effectiveness and accuracy of these LLMs in educational content creation, offering insights into their potential roles in shaping current and future AI-centric education systems."
693,2024,"Codexity: Secure AI-assisted Code Generation nan Despite the impressive performance of Large Language Models (LLMs) in software development activities, recent studies show the concern of introducing vulnerabilities into software codebase by AI programming assistants (e.g., Copilot, CodeWhisperer). In this work, we present Codexity, a security-focused code generation framework integrated with five LLMs. Codexity leverages the feedback of static analysis tools such as Infer and CppCheck to mitigate security vulnerabilities in LLM-generated programs. Our evaluation in a real-world benchmark with 751 automatically generated vulnerable subjects demonstrates Codexity can prevent 60% of the vulnerabilities being exposed to the software developer."
694,2023,"With ChatGPT, do we have to rewrite our learning objectives - CASE Study in Cybersecurity nan With the emergence of Artificial Intelligent chatbot tools such as ChatGPT and code writing AI tools such as GitHub Copilot, educators need to question what and how we should teach our courses and curricula in the future. In reality, automated tools may result in certain academic fields being deeply reduced in the number of employable people. In this work, we make a case study of cybersecurity undergrad education by using the lens of Understanding by Design (UbD). First, we provide a broad understanding of learning objectives (LOs) in cybersecurity from a computer science perspective. Next, we dig a little deeper into a curriculum with an undergraduate emphasis on cybersecurity and examine the major courses and their LOs for our cybersecurity program at Miami University. With these details, we perform a thought experiment on how attainable the LOs are with the above-described tools, asking the key question what needs to be enduring concepts? learned in this process. If an LO becomes something that the existence of automation tools might be able to do, we then ask what level is attainable for the LO that is not a simple query to the tools?. With this exercise, we hope to establish an example of how to prompt ChatGPT to accelerate students in their achievements of LOs given the existence of these new AI tools, and our goal is to push all of us to leverage and teach these tools as powerful allies in our quest to improve human existence and knowledge."
695,2023,"Can Large Language Models Support Medical Facilitation Work? A Speculative Analysis nan Mobile messaging apps and SMS-based tools have been deployed to extend healthcare services beyond the clinic; peer support chat groups, consisting of patients and healthcare providers, can improve medication adherence. However, moderation can be burdensome for busy healthcare professionals who must respond to patients, provide accurate and timely information, and engage and build community among patients. In this paper, taking an ethnographic approach, we examine the moderation of chat groups for young people living with HIV in Kenya. We describe the roles and responsibilities of the moderator while striving to engage and build community among the participants and manage the group chat, highlighting the challenges they face. Grounded in the moderators' work, we explore how an LLM-enabled copilot could help or hinder group facilitation. In doing so, we contribute to discussions about the potential of Artificial Intelligence in supporting healthcare professionals."
696,2021,"Enhancing Human-Machine Teaming for Medical Prognosis Through Neural Ordinary Differential Equations (NODEs) [arXiv] nan Machine Learning (ML) has recently been demonstrated to rival expert-level human accuracy in prediction and detection tasks in a variety of domains, including medicine. Despite these impressive findings, however, a key barrier to the full realization of ML's potential in medical prognoses is technology acceptance. Recent efforts to produce explainable AI (XAI) have made progress in improving the interpretability of some ML models, but these efforts suffer from limitations intrinsic to their design: they work best at identifying why a system fails, but do poorly at explaining when and why a model's prediction is correct. We posit that the acceptability of ML predictions in expert domains is limited by two key factors: the machine's horizon of prediction that extends beyond human capability, and the inability for machine predictions to incorporate human intuition into their models. We propose the use of a novel ML architecture, Neural Ordinary Differential Equations (NODEs) to enhance human understanding and encourage acceptability. Our approach prioritizes human cognitive intuition at the center of the algorithm design, and offers a distribution of predictions rather than single outputs. We explain how this approach may significantly improve human-machine collaboration in prediction tasks in expert domains such as medical prognoses. We propose a model and demonstrate, by expanding a concrete example from the literature, how our model advances the vision of future hybrid Human-AI systems. [doi:10.13140/RG.2.2.18067.60963]."
697,2023,"Transformed by Transformers: Navigating the AI Coding Revolution for Computing Education An ITiCSE Working Group Conducted by Humans nan The recent advent of highly accurate and scalable large language models (LLMs) has taken the world by storm. From art to essays to computer code, LLMs are producing novel content that until recently was thought only humans could produce. Recent work in computing education has sought to understand the capabilities of LLMs for solving tasks such as writing code, explaining code, creating novel coding assignments, interpreting programming error messages, and more. However, these technologies continue to evolve at an astonishing rate leaving educators little time to adapt. This working group seeks to document the state-of-the-art for code generation LLMs, detail current opportunities and challenges related to their use, and present actionable approaches to integrating them into computing curricula."
698,2024,"Program Code Generation with Generative AIs nan Our paper compares the correctness, efficiency, and maintainability of human-generated and AI-generated program code. For that, we analyzed the computational resources of AI- and human-generated program code using metrics such as time and space complexity as well as runtime and memory usage. Additionally, we evaluated the maintainability using metrics such as lines of code, cyclomatic complexity, Halstead complexity and maintainability index. For our experiments, we had generative AIs produce program code in Java, Python, and C++ that solves problems defined on the competition coding website leetcode.com. We selected six LeetCode problems of varying difficulty, resulting in 18 program codes generated by each generative AI. GitHub Copilot, powered by Codex (GPT-3.0), performed best, solving 9 of the 18 problems (50.0%), whereas CodeWhisperer did not solve a single problem. BingAI Chat (GPT-4.0) generated correct program code for seven problems (38.9%), ChatGPT (GPT-3.5) and Code Llama (Llama 2) for four problems (22.2%) and StarCoder and InstructCodeT5+ for only one problem (5.6%). Surprisingly, although ChatGPT generated only four correct program codes, it was the only generative AI capable of providing a correct solution to a coding problem of difficulty level hard. In summary, 26 AI-generated codes (20.6%) solve the respective problem. For 11 AI-generated incorrect codes (8.7%), only minimal modifications to the program code are necessary to solve the problem, which results in time savings between 8.9% and even 71.3% in comparison to programming the program code from scratch."
699,2024,"Self-Regulation, Self-Efficacy, and Fear of Failure Interactions with How Novices Use LLMs to Solve Programming Problems nan We explored how undergraduate introductory programming students naturalistically used generative AI to solve programming problems. We focused on the relationship between their use of AI to their self-regulation strategies, self-efficacy, and fear of failure in programming. In this repeated-measures, mixed-methods research, we examined students' patterns of using generative AI with qualitative student reflections and their self-regulation, self-efficacy, and fear of failure with quantitative instruments at multiple times throughout the semester. We also explored the relationships among these variables to learner characteristics, perceived usefulness of AI, and performance. Overall, our results suggest that student factors affect their baseline use of AI. In particular, students with higher self-efficacy, lower fear of failure, or higher prior grades tended to use AI less or later in the problem-solving process and rated it as less useful than others. Interestingly, we found no relationship between students' self-regulation strategies and their use of AI. Students who used AI less or later in problem-solving also had higher grades in the course, but this is most likely due to prior characteristics as our data do not suggest that this is a causal relationship."
700,2023,"Optimization of a Human-Machine Team for Geographic Region Digitization nan The state of the art in collaborative human-machine geographic region digitization yields 80% automation in vertex placement when digitizing land-water boundaries in small remotely sensed persistent image sets. However, these techniques have difficulty scaling to large datasets due to O(logN) complexity of instance-based learning and concept drift, or sudden shifts in environmental context. We report the performance of the human-machine team on a 50-image dataset of a littoral region targeting the highly dynamic nearshore region, which is the region of breaking waves and swash. As we have observed that both computational performance and precision are lacking, we define and apply several novel optimization techniques built specifically for the human-machine team. The best performing optimization technique, which is named compositional interface schemata, utilizes the novel concept of providing an objective function based on the machine teammate's performance in automation rather than an objective function based on the underlying classifier's pixel classification accuracy. Results show that all proposed optimization methods, including simply clearing the learner of instances each period, perform at least 3% points better than an unoptimized approach. The best performing optimizer yields a vertex placement accuracy of 74% compared to the non-optimized accuracy of 65%. Additionally, we present an online heuristic that attempts to dynamically choose the best optimization method, which yields 77% vertex placement accuracy."
701,2024,"What Was Your Prompt? A Remote Keylogging Attack on AI Assistants nan AI assistants are becoming an integral part of society, used for asking advice or help in personal and confidential issues. In this paper, we unveil a novel side-channel that can be used to read encrypted responses from AI Assistants over the web: the token-length side-channel. We found that many vendors, including OpenAI and Microsoft, have this side-channel. However, inferring the content of a response from a token-length sequence alone proves challenging. This is because tokens are akin to words, and responses can be several sentences long leading to millions of grammatically correct sentences. In this paper, we show how this can be overcome by (1) utilizing the power of a large language model (LLM) to translate these sequences, (2) providing the LLM with inter-sentence context to narrow the search space and (3) performing a known-plaintext attack by fine-tuning the model on the target model's writing style. Using these methods, we were able to accurately reconstruct 29\% of an AI assistant's responses and successfully infer the topic from 55\% of them. To demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and Microsoft's Copilot on both browser and API traffic."
702,2024,"Probeable Problems for Beginner-level Programming-with-AI Contests nan To broaden participation, competitive programming contests may include beginner-level problems that do not require knowledge of advanced Computer Science concepts (e.g., algorithms and data structures). However, since most participants have easy access to AI code-generation tools, these problems often become trivial to solve. For beginner-friendly programming contests that do not prohibit the use of AI tools, we propose Probeable Problems: code writing tasks that provide (1) a problem specification that deliberately omits certain details, and (2) a mechanism to probe for these details by asking clarifying questions and receiving immediate feedback. To evaluate our proposal, we conducted a 2-hour programming contest for undergraduate Computer Science students from multiple institutions, where each student was an active member of their institution's ACM student chapter. The contest comprised of six Probeable Problems for which a popular code-generation tools (e.g., GitHub Copilot) were unable to generate accurate solutions due to the absence of details. Students were permitted to work individually or in groups, and were free to use AI tools. We obtained consent from 26 groups (67 students) to use their submissions for research. To determine whether Probeable Problems are suitable for such contests, we analyze the extent to which the code submitted by these groups identifies missing details."
703,2023,Calibrating Social Experience in Human-AI Collaboration: Toward More Innovative and Inclusive Work Futures nan nan
704,2024,"LASIK Versus PRK Based on Increased Risk of Corneal Haze: Assessing Current Decision-Making Capabilities of Six Artificial Intelligence Models in Refractive Surgery nan PURPOSE: To investigate the current decision-making capabilities of 6 different artificial intelligence (AI) models by assessing their refractive surgery recommendations (laser in-situ keratomileusis [LASIK] or photorefractive keratectomy [PRK]) for a theoretical patient with a history of keloid formation. METHODS: Claude-2 (Anthropic, 2023), GPT-4 (OpenAI, 2023), GPT-3.5 (OpenAI, 2022), Gemini 1.0 (Google DeepMind, 2023), Microsoft Copilot (Microsoft AI, 2023), and Google-PaLM (Google AI, 2022) underwent three systematic queries to determine the most appropriate surgical plan (LASIK or PRK) for a theoretical patient with an increasing manifest refraction of-3.50,-5.00, and-7.00 diopters (D) in both eyes, an uncomplicated ocular examination, and history of keloid formation. They were then tasked with providing published scientific references to support their responses. The AI models' recommendations were compared to those of a group of 6 experienced ophthalmologists, serving as a benchmark. RESULTS: The group of ophthalmologists unanimously recommended LASIK (6/6 ophthalmologists), in contrast to the unanimous initial recommendation for PRK from the AI models (6/6 models). Of the 42 references provided by the AI models, 55% were fictitious and 45% were authentic. Only 1 of the 6 models altered its initial recommendation to LASIK when presented with the same patient with a history of keloid formation but with increasing severity of myopia (-3.50 to 5.00 to 7.00 D). DISCUSSION: It is evident that current AI models lack the critical-thinking abilities required to accurately analyze and assess apparent risk factors in clinical scenarios, such as the risk of corneal haze after PRK at higher levels of myopia, particularly in cases with a history of keloid formation."
705,2023,"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI's Codex on CS2 Programming Exercises nan The introduction of OpenAI Codex sparked a surge of interest in the impact of generative AI models on computing education practices. Codex is also the underlying model for GitHub Copilot, a plugin which makes AI-generated code accessible to students through auto-completion in popular code editors. Research in this area, particularly on the educational implications, is nascent and has focused almost exclusively on introductory programming (or CS1) questions. Very recent work has shown that Codex performs considerably better on typical CS1 exam questions than most students. It is not clear, however, what Codex's limits are with regard to more complex programming assignments and exams. In this paper, we present results detailing how Codex performs on more advanced CS2 (data structures and algorithms) exam questions taken from past exams. We compare these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students. We consider the implications of such tools for the future of undergraduate computing education."
706,2023,"AI for Low-Code for AI [arXiv] nan Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) programs, whereas visual tools abstract away most or all programming but struggle to provide easy access to large APIs. At their intersection, we propose LowCoder, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCoder_VP) and an AI-powered natural language interface (LowCoder_NL). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCoder, replacing the LowCoder_NL component with a simple keyword search in half the tasks. Overall, we find that LowCoder is especially useful for (i) Discoverability: using LowCoder_NL, participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of AI with low-code programming."
707,2023,"Mitigating knowledge imbalance in AI-advised decision-making through collaborative user involvement nan Integrating artificial intelligence (AI) systems into decision-making tasks attempts to assist people by augment -ing or complementing their abilities and ultimately improve task performance. However, when considering recommendations from modern black boxintelligent systems, users are confronted with the decision of accepting or overriding AI's recommendations. These decisions are even more challenging to make when there exists a significant knowledge imbalance between the users and the AI system-namely, when people lack necessary task knowledge and are therefore unable to accurately complete the task on their own. In this work, we aim to understand people's behavior in AI-assisted decision-making tasks when faced with the challenge of knowledge imbalance and explore whether involving users in an AI's prediction generation process makes them more willing to follow the AI's recommendations and enhances their perception of collaboration. Our empirical study reveals that the involvement of users in generating AI recommendations during a task with notable knowledge imbalance causes them to be more willing to agree with the AI's suggestions and to perceive the AI agent and their collaboration as a team more positively."
708,2021,"The Impact of Human-Artificial Intelligence Collaboration on Innovation Activities in Knowledge-Intensive Companies nan Researching the impact of artificial intelligence (AI) on innovation activities in knowledge-intensive companies (KICs), the authors use a systematic approach in observing innovative entities. Scientific observation begins with an innovation ecosystem that is treated as a global innovative entity. In addition to the innovation ecosystem, the first part of the paper focuses on KICs. KICs are analysed as a subsystem of the global innovation ecosystem, on the one hand, and as a unique innovation system made up of numerous subsystems, on the other hand. Innovation activities in KICs are the next innovative entity of scientific observation. At the same time, they represent a subsystem in KICs and a special system made up of interconnected subsystems. Subsystems of innovation activities are also treated as unique systems made up of other subsystems. In order to respond to the requirements defined in the title of the paper, the authors specifically analyse research and development (R&D), and newly created knowledge (innovative output) as subsystems of innovation activities. At the end of the first part of the paper, the authors sublimate the conclusions related to AI as an innovative entity. AI is also viewed dual: as a subsystem of innovation activities (part of newly created knowledge), and as a resource for generating new inventions. Using this finding, in the second part of the paper, the authors state that AI will certainly cause changes in all subsystems in KICs. The most dominant will be changes in: human capital, knowledge and skills of knowledge workers, and the structure of team. The aforementioned changes will redefine innovation activities and their subsystems. Influenced by the trend of these changes, AI will influence changes in management practices in KICs. The impact of AI as a new resource in generating new inventions will have effects not only on KICs, but also on the global innovation ecosystem, as a starting point for researching the impact of human-AI collaboration on innovation activities in KICs."
709,2024,"Learn to Code Sustainably: An Empirical Study on LLM-based Green Code Generation [arXiv] nan The increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. These contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (AI) models. The need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of AI models can lead to energy efficiency gains. Here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of AI models. In this framework, we evaluate the sustainability of auto-generated code. The auto-generate codes considered in this study are produced by generative commercial AI language models, GitHub Copilot, OpenAI ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to quantify the sustainability awareness of these AI models, we propose a definition of the code's green capacity, based on certain sustainability metrics. We compare the performance and green capacity of human-generated code and code generated by the three AI language models in response to easy-to-hard problem statements. Our findings shed light on the current capacity of AI models to contribute to sustainable software development."
710,2024,"Analysis of How ChatGPT and Gemini Help in the Generation of Cyber Attacks nan The introduction of AI-powered language models such as ChatGPT and Copilot has led to notable progress in automation and productivity in recent years. However, these technological advancements also entail substantial vulnerabilities, especially in the field of cybersecurity. This study examines the feasibility of generating cyberattacks using ChatGPT and Copilot. This study analyzes the functionalities, constraints, and ethical implications linked to the improper utilization of these technologies. This research shows that although these models may unintentionally contribute to the occurrence of cyberattacks, their misuse may be greatly prevented by implementing strong mitigation mechanisms. This paper examines the various privacy, cybersecurity, and business-related issues associated with ChatGPT and Gemini. The content encompasses malevolent attacker strategies such as phishing, denial-of-service, and eavesdropping attacks. The discussion also highlights the significance of proactive actions taken by defenders, underscoring the valuable role of ChatGPT in threat intelligence and security operations. An in-depth discussion examines the connection between offensive and defensive applications that tackle moral and practical concerns. The text discusses potential strategies to protect against future attacks, such as content screening and collaboration."
711,2023,"Talkin' 'Bout AI generation: copyright and the generative-AI supply chain [arXiv] nan Does generative AI infringe copyright? is an urgent question. It is also a difficult question, for two reasons. First, generative AI is not just one product from one company. It is a catch-all name for a massive ecosystem of loosely related technologies, including conversational text chatbots like ChatGPT, image generators like Midjourney and DALL-E, coding assistants like GitHub Copilot, and systems that compose music and create videos. These systems behave differently and raise different legal issues. The second problem is that copyright law is notoriously complicated, and generative-AI systems manage to touch on a great many corners of it: authorship, similarity, direct and indirect liability, fair use, and licensing, among much else. These issues cannot be analyzed in isolation, because there are connections everywhere. In this Article, we aim to bring order to the chaos. To do so, we introduce the generative-AI supply chain: an interconnected set of stages that transform training data (millions of pictures of cats) into generations (a new, potentially never-seen-before picture of a cat that has never existed). Breaking down generative AI into these constituent stages reveals all of the places at which companies and users make choices that have copyright consequences. It enables us to trace the effects of upstream technical designs on downstream uses, and to assess who in these complicated sociotechnical systems bears responsibility for infringement when it happens. Because we engage so closely with the technology of generative AI, we are able to shed more light on the copyright questions. We do not give definitive answers as to who should and should not be held liable. Instead, we identify the key decisions that courts will need to make as they grapple with these issues, and point out the consequences that would likely flow from different liability regimes."
712,2024,"AI for Low-Code for AI nan Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) textual programs, whereas visual tools abstract away most or all program text but struggle to provide easy access to large APIs. At their intersection, we propose LowCoder, the first low-code tool for developing AI pipelines that supports both a visual programming interface (LowCoderVP) and an AI-powered natural language interface (LowCoderNL). We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of AI expertise with implementing four ML pipelines using LowCoder, replacing the LowCoderNL component with a simple keyword search in half the tasks. Overall, we find that LowCoder is especially useful for (i) Discoverability: using LowCoderNL, participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively in the keyword-search condition, and (ii) Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that AI helps users discover how to implement constructs when they know what to do, but still fails to support novices when they lack clarity on what they want to accomplish. Overall, our work highlights the benefits of combining the power of AI with low-code programming."
713,2024,"Exploring the Feasibility of Multimodal Chatbot AI as Copilot in Pathology Diagnostics: Generalist Model's Pitfall nan Pathology images are crucial for diagnosing and managing various diseases by visualizing cellular and tissue-level abnormalities. Recent advancements in artificial intelligence (AI), particularly multimodal models like ChatGPT, have shown promise in transforming medical image analysis through capabilities such as medical vision-language question answering. However, there remains a significant gap in integrating pathology image data with these AI models for clinical applications. This study benchmarks the performance of GPT on pathology images, assessing their diagnostic accuracy and efficiency in real-word clinical records. We observe significant deficits of GPT in bone diseases and a fair-level performance in diseases from other three systems. Despite offering satisfactory abnormality annotations, GPT exhibits consistent disadvantage in terminology accuracy and multimodal integration. Specifically, we demonstrate GPT's failures in interpreting immunohistochemistry results and diagnosing metastatic cancers. This study highlight the weakness of current generalist GPT model and contribute to the integration of pathology and advanced AI."
714,2024,"What Skills Do You Need When Developing Software Using ChatGPT? (Discussion Paper) [arXiv] nan Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the media and popular scientific literature, but also journals such as the Communications of the ACM, have been flooded with opinions how these tools will change programming. The opinions range from ``machines will program themselves'', to ``AI does not help programmers''. Of course, these statements are meant to to stir up a discussion, and should be taken with a grain of salt, but we argue that such unfounded statements are potentially harmful. Instead, we propose to investigate which skills are required to develop software using LLM-based tools. In this paper we report on an experiment in which we explore if Computational Thinking (CT) skills predict the ability to develop software using LLM-based tools. Our results show that the ability to develop software using LLM-based tools can indeed be predicted by the score on a CT assessment. There are many limitations to our experiment, and this paper is also a call to discuss how to approach, preferably experimentally, the question of which skills are required to develop software using LLM-based tools. We propose to rephrase this question to include by what kind of people/programmers, to develop what kind of software using what kind of LLM-based tools."
715,2024,"Teaching CS50 with AI Leveraging Generative Artificial Intelligence in Computer Science Education nan In Summer 2023, we developed and integrated a suite of AI-based software tools into CS50 at Harvard University. These tools were initially available to approximately 70 summer students, then to thousands of students online, and finally to several hundred on campus during Fall 2023. Per the course's own policy, we encouraged students to use these course-specific tools and limited the use of commercial AI software such as ChatGPT, GitHub Copilot, and the new Bing. Our goal was to approximate a 1:1 teacher-to-student ratio through software, thereby equipping students with a pedagogically-minded subject-matter expert by their side at all times, designed to guide students toward solutions rather than offer them outright. The tools were received positively by students, who noted that they felt like they had a personal tutor. Our findings suggest that integrating AI thoughtfully into educational settings enhances the learning experience by providing continuous, customized support and enabling human educators to address more complex pedagogical issues. In this paper, we detail how AI tools have augmented teaching and learning in CS50, specifically in explaining code snippets, improving code style, and accurately responding to curricular and administrative queries on the course's discussion forum. Additionally, we present our methodological approach, implementation details, and guidance for those considering using these tools or AI generally in education."
716,2023,"Towards More Effective AI-Assisted Programming: A Systematic Design Exploration to Improve Visual Studio IntelliCode's User Experience nan AI-driven code editor extensions such as Visual Studio IntelliCode and Github CoPilot have become extremely popular. These tools recommend inserting chunks of code, with the lines to be inserted presented inline at the current cursor location as gray text. In contrast to their popularity, other AIdriven code recommendation tools that suggest code changes (as opposed to code completions) have remained woefully underused. We conducted lab studies at Microsoft to understand this disparity and found one major cause: discoverability. Code change suggestions are hard to surface through bold, inline interfaces and hence, developers often do not even notice them.Towards a systematic understanding of code change interfaces, we performed a thorough design exploration for various categories of code changes: additive single-line changes, single-line changes, and multi-line changes. Overall, we explored 19 designs through a series of 7 laboratory studies involving 61 programmers and distilled our findings into a set of 5 design principles. To validate our results, we built and deployed a new version of IntelliCode with two of our new inline interfaces in Microsoft Visual Studio 2022 and found that they lead to a significant increase in usage of the corresponding tools."
717,2024,"Probeable Problems for Beginner-level Programming-with-AI Contests nan To broaden participation, competitive programming contests may include beginner-level problems that do not require knowledge of advanced Computer Science concepts (e.g., algorithms and data structures). However, since most participants have easy access to AI code-generation tools, these problems often become trivial to solve. For beginner-friendly programming contests that do not prohibit the use of AI tools, we propose Probeable Problems: code writing tasks that provide (1) a problem specification that deliberately omits certain details, and (2) a mechanism to probe for these details by asking clarifying questions and receiving immediate feedback. To evaluate our proposal, we conducted a 2-hour programming contest for undergraduate Computer Science students from multiple institutions, where each student was an active member of their institution's computing club. The contest comprised of six Probeable Problems for which a popular code-generation tool (GitHub Copilot) was unable to generate accurate solutions due to the absence of details. Students were permitted to work individually or in groups, and were free to use AI tools. We obtained consent from 26 groups (67 students) to use their submissions for research. We analyze the extent to which the code submitted by these groups identifies missing details and identify ways in which Probeable Problems can support learning in formal and informal CS educational contexts."
718,2024,"Quantitative Assessment of Drone Pilot Performance nan This paper introduces a quantitative methodology for assessing drone pilot performance, aiming to reduce drone-related incidents by understanding the human factors influencing performance. The challenge lies in balancing evaluations in operationally relevant environments with those in a standardized test environment for statistical relevance. The proposed methodology employs a novel virtual test environment that records not only basic flight metrics but also complex mission performance metrics, such as the video quality from a target. A group of Belgian Defence drone pilots were trained using this simulator system, yielding several practical results. These include a human-performance model linking human factors to pilot performance, an AI co-pilot providing real-time flight performance guidance, a tool for generating optimal flight trajectories, a mission planning tool for ideal pilot assignment, and a method for iterative training improvement based on quantitative input. The training results with real pilots demonstrate the methodology's effectiveness in evaluating pilot performance for complex military missions, suggesting its potential as a valuable addition to new pilot training programs."
719,2024,"PathOCL: Path-Based Prompt Augmentation for OCL Generation with GPT-4 nan The rapid progress of AI-powered programming assistants, such as GitHub Copilot, has facilitated the development of software applications. These assistants rely on large language models (LLMs), which are foundation models (FMs) that support a wide range of tasks related to understanding and generating language. LLMs have demonstrated their ability to express UML model specifications using formal languages like the Object Constraint Language (OCL). However, the context size of the prompt is limited by the number of tokens an LLM can process. This limitation becomes significant as the size of UML class models increases. In this study, we introduce PathOCL, a novel path-based prompt augmentation technique designed to facilitate OCL generation. PathOCL addresses the limitations of LLMs, specifically their token processing limit and the challenges posed by large UML class models. PathOCL is based on the concept of chunking, which selectively augments the prompts with a subset of UML classes relevant to the English specification. Our findings demonstrate that PathOCL, compared to augmenting the complete UML class model (UML-Augmentation), generates a higher number of valid and correct OCL constraints using the GPT-4 model. Moreover, the average prompt size crafted using PathOCL significantly decreases when scaling the size of the UML class models."
720,2024,"CS1-LLM: Integrating LLMs into CS1 Instruction nan The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it. Indeed, recent research has shown that LLMs are capable of solving the majority of the assignments and exams we previously used in CS1. In addition, professional software engineers are often using these tools, raising the question of whether we should be training our students in their use as well. This experience report describes a CS1 course at a large research-intensive university that fully embraces the use of LLMs from the beginning of the course. To incorporate the LLMs, the course was intentionally altered to reduce emphasis on syntax and writing code from scratch. Instead, the course now emphasizes skills needed to successfully produce software with an LLM. This includes explaining code, testing code, and decomposing large problems into small functions that are solvable by an LLM. In addition to frequent, formative assessments of these skills, students were given three large, open-ended projects in three separate domains (data science, image processing, and game design) that allowed them to showcase their creativity in topics of their choosing. In an end-of-term survey, students reported that they appreciated learning with the assistance of the LLM and that they interacted with the LLM in a variety of ways when writing code. We provide lessons learned for instructors who may wish to incorporate LLMs into their course."
721,2023,"An Exploratory Study on the Impact of AI tools on the Student Experience in Programming Courses: an Intersectional Analysis Approach nan This work-in-progress paper presents a study that sheds light on the concerns that students may not develop sufficient programming skills and as a result, be less competent with the use of ChatGPT. The potential benefits for students are significant: Access to ChatGPT increases the ability for students to work constructively on their own schedule. The ease of use of ChatGPT may engage students who might otherwise hesitate in asking for support. Before these tools can be meaningfully introduced into a course, work must be done to study the impact of these AI tools on a student's ability to learn. In this study, participants are recruited from introductory Java programming courses at a large public university in the United States. This paper presents preliminary findings from a mixed method study design that consists of a pre-task assessment quiz; and a programming task in one of three conditions: (1) with no external help, (2) with the help of an AI chatbot, or (3) with the help of a generative AI tool like GitHub Copilot; followed by a post-task assessment and an interview on their experience and perceptions of the tools. Our preliminary findings describe our data collection, thematic analysis of the students' prompts and chatGPT responses, and a summary of the experience for 3 students. Our findings demonstrate a range of students' attitudes and behaviors towards chatGPT that provides insight for future research and plans for incorporating such AI tools in a course."
722,2024,"A New Generation of Intelligent Development Environments nan The practice of programming is undergoing a revolution with the introduction of AI assisted development (copilots) and the creation of new programming languages that are designed explicitly for tooling, analysis, and automation. Integrated Development Environments (IDEs) as they are currently conceptualized have not yet responded to these changes. They are still designed around the idea of a human programmer typing textual code into an editor window with the IDE providing assistance via the integration of various tools for syntax highlighting, compilation, debugging, and (maybe) code version control. This paper presents a vision for transforming the IDE from an Integrated Development Environment to an Intelligent Development Environment. The new IDE will be designed around the idea of a human programmer as the manager or curator of a software project who, rather than manually typing in code to implement a solution, will instead use the IDE to direct AI programming agents and/or automated tools to combine existing APIs, packages, and new code to implement the needed features. In this new model, the fundamental roles of the IDE are to 1) facilitate the communication between the human programmer and the AI agents and automated tools and 2) organize the workflow tasks needed to go from requirements gathering to the final tested and validated deployed feature. This paper presents a vision for the new Intelligent Development Environment based on a range of proof-of-concept high-value scenarios we have experimented with and discusses the challenges that remain to realizing these in a cohesive intelligent development experience."
723,2024,"Analysis of non-functional requirements for smart contract recommender system nan Non-functional requirements (NFRs) are essential criteria of software quality that affect the performance, usability, security, and reliability of software. The blockchain, as a novel technology expands the set of common NFRs with additional attributes, such as transparency, immutability, and decentralization. However, blockchain integration usually focuses on functional requirements (FRs), while NFRs are not sufficiently addressed. Therefore, NFRs are usually analysed with dedicated APIs, crawlers, and other tools that require ongoing maintenance and manual intervention. This paper proposes an AI-based analysis of NFRs for blockchain solutions, where AI tools such as Copilot, ChatGPT, and others are leveraged. We use these tools to generate, evaluate, and optimize NFRs for blockchain solutions in a more comprehensive way. The results of our solution are presented on a recommender system use case, including explanation on how to improve the sustainability of the proposed solution."
724,2024,"Performance of chatbots in queries concerning fundamental concepts in photochemistry nan The advent of chatbots raises the possibility of a paradigm shift across society including the most technical of fields with regard to access to information, generation of knowledge, and dissemination of education and training. Photochemistry is a scientific endeavor with roots in chemistry and physics and branches that encompass diverse disciplines ranging from astronomy to zoology. Here, five chatbots have each been challenged with 13 photochemically relevant queries. The chatbots included ChatGPT 3.5, ChatGPT 4.0, Copilot, Gemini Advanced, and Meta AI. The queries encompassed fundamental concepts (e.g., Why is the fluorescence spectrum typically the mirror image of the absorption spectrum?), practical matters (e.g., What is the inner filter effect and how to avoid it?), philosophical matters (Please create the most important photochemistry questions.), and specific molecular features (e.g., Why are azo dyes non-fluorescent?). The chatbots were moderately effective in answering queries concerning fundamental concepts in photochemistry but were glaringly deficient in specialized queries for dyes and fluorophores. In some instances, a correct response was embedded in verbose scientific nonsense whereas in others the entire response, while grammatically correct, was utterly meaningless. The unreliable accuracy makes present chatbots poorly suited for unaided educational purposes and highlights the importance of domain experts."
725,2023,"An Exploratory Experiment Using ChatGPT in the Idea Generation Process for Product-Service System nan Background Artificial Intelligence(AI) technology is expanding its utilization across various industries, and in the design field, research and development based on AI tools are actively underway. Among them, generative AI tools, such as ChatGPT-4 and Bard, possess potential applicability in the ideation phase of the design process. They are anticipated to overcome the limitations of conventional methods, facilitating the rapid generation of high-quality ideas. Hence, this study aims to systematically verify and explore the effects of generative AI on ideation.Methods Two teams, each composed of four designers, were formed to conduct a comparative experiment between the conventional ideation method and the ideation method utilizing generative AI.They were tasked to generate high-quality ideas over a 4-hour span on the topic of Healthcare Wearable Devices for Generation Z. The process was observed without intervention. Following the experiment, the feasibility of AI utilization was confirmed through participant FGI(focus group interview) and IDI(in- depth interview). Expert evaluations were conducted to assess the creativity of the ideas generated, and insights were obtained through discussions.Results The method utilizing generative AI produced 6 more ideas than the traditional method, showing an increase of approximately 1.67 times when compared to the conventional method. The quality assessment also showed that the outcomes of the generative AI method were on par with those from the conventional ideation method. Generative AI effectively broadened the confined thinking of designers and clearly displayed efficiency in terms of time-saving. However, there were shortcomings in contextual consistency and structural completeness, making expert validation and convergence essential.Conclusions In order to achieve optimal outcomes using generative AI, it is imperative to provide clear preliminary information and to employ specific, structured questions and prompts, as well as effective communication skills when interacting with AI. Discernment and insight on the part of the designer, and high-level decision-making are essential. By rigorously evaluating and refining the ideas proposed by AI based on established criteria, we can pave the way for superior solutions and designs. It is anticipated that future collaborations between humans and AI will yield increasingly rich and sophisticated results in the field of design."
726,1994,"Engineering approach for Rotorcraft Pilot's Associate cognitive decision aiding systems development nan This paper describes our concurrent engineering approach for the Rotorcraft Pilots Associate (RPA) program. The process integrates knowledge acquisition, rapid prototyping, and evaluation into a system development process. The approach is an iterative process which gathers knowledge, uses rapid prototyping to develop Cognitive Decision Aiding System software, uses simulation and embedded measures of effectiveness and measures of performance to evaluate the Cognitive Decision Aiding System mission effectiveness. Improvement areas identified in the evaluation feed the next iteration. Army helicopter pilots/copilot-gunners are the primary domain experts for Cognitive Decision Aiding knowledge acquisition and are heavily involved in the crew interface design. This end user involvement in the design process ensures feedback early enough to impact development and produces an end product that meets the customers needs."
727,2024,"CodeLMSec benchmark: systematically evaluating and finding security vulnerabilities in black-box code language models nan Large language models (LLMs) for automatic code generation have recently achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. Training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively evaluated for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models. In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. This involves proposing a novel few-shot prompting approach. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we use our method to create a collection of diverse non-secure prompts for various vulnerability scenarios. This dataset serves as a benchmark to evaluate and compare the security weaknesses of code language models."
728,2021,"Designing AI Experiences: Boundary Representations, Collaborative Processes, and Data Tools nan nan"
729,2024,"Technical Brief on Software Engineering for FMware nan Foundation Models (FM) like GPT-4 have given rise to FMware, FM-powered applications, which represent a new generation of software that is developed with new roles, assets, and paradigms. FMware has been widely adopted in both software engineering (SE) research (e.g., test generation) and industrial products (e.g., GitHub copilot), despite the numerous challenges introduced by the stochastic nature of FMs. Such challenges jeopardize the quality and trustworthiness of FMware. In our technical brief, we will present the latest research and industrial practices in engineering FMware, and discuss the SE challenges and opportunities facing both researchers and practitioners in the FMware era. The brief is unique in that it is presented from an SE point of view, not an AI point-of-view ensuring that attendees are not bogged into complex mathematical and AI details unless they are essential for contextualizing the SE challenges and opportunities."
730,2024,"Fine Tuning Large Language Model for Secure Code Generation nan AI pair programmers, such as GitHub's Copilot, have shown great success in automatic code generation. However, such large language model-based code generation techniques face the risk of introducing security vulnerabilities to codebases. In this work, we explore the direction of fine-tuning large language models for generating more secure code. We use real-world vulnerability fixes as our fine-tuning dataset. We craft a code-generation scenario dataset (C/C++) for evaluating and comparing the pre-trained and fine-tuned models. Our experiments on GPT-J show that the fine-tuned GPT-J achieved 70.4% and 64.5% ratios of non-vulnerable code generation for C and C++, respectively, which has a 10% increase for C and a slight increase for C++ compared with the pre-trained large language model."
731,2023,"Attribution problem of generative AI: a view from US copyright law nan Generative Artificial Intelligence (AI) systems, a term that is used to describe AI systems, can generate content in response to a given prompt. Users can request generative AI to create a certain type of output or to edit a text or a picture submitted by the user. The output can be in various formats including text, images, audio or code. Today, technology companies are rushing to invest in generative AI."
732,1988,"Modeling the human activity in rapid process control from a concrete case: combat aircrafts nan Pilots of future combat aircrafts will have to accomplish difficult missions, alone, in extremely complex environments and they will have to be helped. The concept of the electronic copilot has been developed for this purpose. It is a technological challenge in two main areas: intelligent programming for automatically managing complex tasks under heavy time constraints and the interface with the pilot; and inflight recovery from human errors by context analysis. A solution would be to provide this system with thinking patterns copied on the pilot's thinking, to supply it with a real model of the task. The author uses cognitive ergonomics and techniques of artificial intelligence to build an original architecture of human activity in the development of this rapid and complex process from the example of the combat mission."
733,2024,"CS1 with a Side of AI: Teaching Software Verification for Secure Code in the Era of Generative AI nan As AI-generated code promises to become an increasingly relied upon tool for software developers, there is a temptation to call for significant changes to early computer science curricula. A move from syntax-focused topics in CS1 toward abstraction and high-level application design seems motivated by the new large language models (LLMs) recently made available. In this position paper however, we advocate for an approach more informed by the AI itself teaching early CS learners not only how to use the tools but also how to better understand them. Novice programmers leveraging AI-code-generation without proper understanding of syntax or logic can create black box code with significant security vulnerabilities. We outline methods for integrating basic AI knowledge and traditional software verification steps into CS1 along with LLMs, which will better prepare students for software development in professional settings."
734,2010,"A Bio-inspired Nano-Agent Architecture for Intelligent Agents nan Intelligent artificial creatures cover a large range of applications in various domains. Recent advances in intelligent agent technologies make now possible to develop a growing number of real-world applications. However, these applications require a new generation of open software architectures that combines such technologies with lightweight design and portability. This chapter describes a new nano-agent architecture designed for intelligent artificial creatures. This software environment takes advantages of our past experiences in distributed artificial intelligence with the Knowledge-based Operating System, real-time multi-expert applications such as the Electronic Copilot project for combat aircrafts, and the more recent Evolutionary Virtual Agent (EVA) applications."
735,2024,"Future Perspective of Risk Prediction in Aesthetic Surgery: Is Artificial Intelligence Reliable? nan Background: Artificial intelligence (AI) techniques are showing significant potential in the medical field. The rapid advancement in artificial intelligence methods suggests their soon-to-be essential role in physicians' practices. Objectives: In this study, we sought to assess and compare the readability, clarity, and precision of medical knowledge responses provided by 3 large language models (LLMs) and informed consent forms for 14 common aesthetic surgical procedures, as prepared by the American Society of Plastic Surgeons (ASPS). Methods: The efficacy, readability, and accuracy of 3 leading LLMs, ChatGPT-4 (OpenAI, San Francisco, CA), Gemini (Google, Mountain View, CA), and Copilot (Microsoft, Redmond, WA), was systematically evaluated with 14 different prompts related to the risks of 14 common aesthetic procedures. Alongside these LLM responses, risk sections from the informed consent forms for these procedures, provided by the ASPS, were also reviewed. Results: The risk factor segments of the combined general and specific operation consent forms were rated highest for medical knowledge accuracy (P < .05). Regarding readability and clarity, the procedure-specific informed consent forms, including LLMs, scored highest scores (P < .05). However, these same forms received the lowest score for medical knowledge accuracy (P < .05). Interestingly, surgeons preferred patient-facing materials created by ChatGPT-4, citing superior accuracy and medical information compared to other AI tools. Conclusions: Physicians prefer patient-facing materials created by ChatGPT-4 over other AI tools due to their precise and comprehensive medical knowledge. Importantly, adherence to the strong recommendation of ASPS for signing both the procedure-specific and the general informed consent forms can avoid potential future complications and ethical concerns, thereby ensuring patients receive adequate information."
736,1993,Development methodology for knowledge-based systems used in guidance and control: application to the `Copilote Electronique' project nan Knowledge based systems are starting to be applied in the guidance and control field. The authors describe some specificities of such embedded systems and study advantages and drawbacks of existing methodologies. Emphasis is put on project life cycle and the authors also address knowledge acquisition methodologies. They show how insights from these methods underlie the opening phase of the French project `Copilote Electronique' that began during 1993.
737,2024,"ESM Cloud Toolkit:A Copilot for Energy Storage Material Research nan Searching and designing new materials play crucial roles in the development of energy storage devices.In today's world where machine learning technology has shown strong predictive ability for various tasks,the combination with machine learning technology will accelerate the process of material development.Herein,we develop ESM Cloud Toolkit for energy storage materials based on MatElab platform,which is designed as a convenient and accurate way to automatically record and save the raw data of scientific research.The ESM Cloud Toolkit includes multiple features such as automatic archiving of computational simulation data,post-processing of experimental data,and machine learning applications.It makes the entire research workflow more automated and reduces the entry barrier for the application of machine learning technology in the domain of energy storage materials.It integrates data archive,traceability,processing,and reutilization,and allows individual research data to play a greater role in the era of AI."
739,2024,"Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding Practices with Insecure Suggestions from Poisoned AI Models nan AI-powered coding assistant tools (e.g., ChatGPT, Copilot, and IntelliCode) have revolutionized the software engineering ecosystem. However, prior work has demonstrated that these tools are vulnerable to poisoning attacks. In a poisoning attack, an attacker intentionally injects maliciously crafted insecure code snippets into training datasets to manipulate these tools. The poisoned tools can suggest insecure code to developers, resulting in vulnerabilities in their products that attackers can exploit. However, it is still little understood whether such poisoning attacks against the tools would be practical in real-world settings and how developers address the poisoning attacks during software development. To understand the real-world impact of poisoning attacks on developers who rely on AI-powered coding assistants, we conducted two user studies: an online survey and an in-lab study. The online survey involved 238 participants, including software developers and computer science students. The survey results revealed widespread adoption of these tools among participants, primarily to enhance coding speed, eliminate repetition, and gain boilerplate code. However, the survey also found that developers may misplace trust in these tools because they overlooked the risk of poisoning attacks. The in-lab study was conducted with 30 professional developers. The developers were asked to complete three programming tasks with a representative type of AI-powered coding assistant tool (e.g., ChatGPT or IntelliCode), running on Visual Studio Code. The in-lab study results showed that developers using a poisoned ChatGPT-like tool were more prone to including insecure code than those using an IntelliCode-like tool or no tool. This demonstrates the strong influence of these tools on the security of generated code. Our study results highlight the need for education and improved coding practices to address new security issues introduced by AI-powered coding assistant tools."
740,2024,Forecasting the Promotion Rate for Women in the U.S. Senior Foreign Service nan nan
741,1993,"From field work analysis to a cognitive model and the design of support systems: assistance to fighter pilots nan An in-depth approach to the cognitive modeling and computerization of fighter pilot activity was conducted over a period of years in the framework of development of an electronic copilot program for the future fighter plane, the Rafale. Several flying field-experiments have been conducted. The resulting model aims at demonstrating that a time-limited complex and risky task can be carried out with limited resources. Here, the operator's art lies in his ability to systematically plan and prepare actions, reduce and simplify the world as much as possible, foresee problems and actively avoid unknown situations. The last section examines the ways in which this model served in the design of a support system, which rather than being conceived as a means of remedying operator defects, is aimed at optimizing the intelligence of his behavior. This new support philosophy leads to a more general discussion of the drawbacks of current support modes."
742,2022,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote) nan AI can enhance programming experiences for a diverse set of programmers: from professional developers and data scientists (proficient programmers) who need help in software engineering and data wrangling, all the way to spreadsheet users (low-code programmers) who need help in authoring formulas, and students (novice programmers) who seek hints when stuck with their programming homework. To communicate their need to AI, users can express their intent explicitlyas input-output examples or natural-language specificationor implicitlywhere they encounter a bug (and expect AI to suggest a fix), or simply allow AI to observe their last few lines of code or edits (to have it suggest the next steps). The task of synthesizing an intended program snippet from the user's intent is both a search and a ranking problem. Search is required to discover candidate programs that correspond to the (often ambiguous) intent, and ranking is required to pick the best program from multiple plausible alternatives. This creates a fertile playground for combining symbolic-reasoning techniques, which model the semantics of programming operators, and machine-learning techniques, which can model human preferences in programming. Recent advances in large language models like Codex offer further promise to advance such neuro-symbolic techniques. Finally, a few critical requirements in AI-assisted programming are usability, precision, and trust; and they create opportunities for innovative user experiences and interactivity paradigms. In this talk, I will explain these concepts using some existing successes, including the Flash Fill feature in Excel, Data Connectors in PowerQuery, and IntelliCode/CoPilot in Visual Studio. I will also describe several new opportunities in AI-assisted programming, which can drive the next set of foundational neuro-symbolic advances."
743,2024,"Whodunit: Classifying Code as Human Authored or GPT-4 Generated -- A case study on CodeChef problems [arXiv] nan Artificial intelligence (AI) assistants such as GitHub Copilot and ChatGPT, built on large language models like GPT-4, are revolutionizing how programming tasks are performed, raising questions about whether code is authored by generative AI models. Such questions are of particular interest to educators, who worry that these tools enable a new form of academic dishonesty, in which students submit AI generated code as their own work. Our research explores the viability of using code stylometry and machine learning to distinguish between GPT-4 generated and human-authored code. Our dataset comprises human-authored solutions from CodeChef and AI-authored solutions generated by GPT-4. Our classifier outperforms baselines, with an F1-score and AUC-ROC score of 0.91. A variant of our classifier that excludes gameable features (e.g., empty lines, whitespace) still performs well with an F1-score and AUC-ROC score of 0.89. We also evaluated our classifier with respect to the difficulty of the programming problem and found that there was almost no difference between easier and intermediate problems, and the classifier performed only slightly worse on harder problems. Our study shows that code stylometry is a promising approach for distinguishing between GPT-4 generated code and human-authored code."
744,2024,"It's Not Only What You Say, But Also How You Say It: Machine Learning Approach to Estimate Trust from Conversation nan Objective: The objective of this study was to estimate trust from conversations using both lexical and acoustic data.Background: As NASA moves to long-duration space exploration operations, the increasing need for cooperation between humans and virtual agents requires real-time trust estimation by virtual agents. Measuring trust through conversation is a novel and unintrusive approach.Method: A 2 (reliability) x 2 (cycles) x 3 (events) within-subject study with habitat system maintenance was designed to elicit various levels of trust in a conversational agent. Participants had trust-related conversations with the conversational agent at the end of each decision-making task. To estimate trust, subjective trust ratings were predicted using machine learning models trained on three types of conversational features (i.e., lexical, acoustic, and combined). After training, model explanation was performed using variable importance and partial dependence plots.Results: Results showed that a random forest algorithm, trained using the combined lexical and acoustic features, predicted trust in the conversational agent most accurately ( R-adj(2) = 0.71 ) . The most important predictors were a combination of lexical and acoustic cues: average sentiment considering valence shifters, the mean of formants, and Mel-frequency cepstral coefficients (MFCC). These conversational features were identified as partial mediators predicting people's trust.Conclusion: Precise trust estimation from conversation requires lexical cues and acoustic cues.Application: These results showed the possibility of using conversational data to measure trust, and potentially other dynamic mental states, unobtrusively and dynamically."
745,2024,"Rethinking AI code generation: a one-shot correction approach based on user feedback nan Code generation has become an integral feature of modern IDEs, gathering significant attention. Notable approaches like GitHub Copilot and TabNine have been proposed to tackle this task. However, these tools may shift code writing tasks towards code reviewing, which involves modification from users. Despite the advantages of user feedback, their responses remain transient and lack persistence across interaction sessions. This is attributed to the inherent characteristics of generative AI models, which require explicit re-training for new data integration. Additionally, the non-deterministic and unpredictable nature of AI-powered models limits thorough examination of their unforeseen behaviors. We propose a methodology named One-shot Correction to mitigate these issues in natural language to code translation models with no additional re-training. We utilize decomposition techniques to break down code translation into sub-problems. The final code is constructed using code snippets of each query chunk, extracted from user feedback or selectively generated from a generative model. Our evaluation indicates comparable or improved performance compared to other models. Moreover, the methodology offers straightforward and interpretable approaches, which enable in-depth examination of unexpected results and facilitate insights for potential enhancements. We also illustrate that user feedback can substantially improve code translation models without re-training. Ultimately, we develop a preliminary GUI application to demonstrate the utility of our methodology in simplifying customization and assessment of suggested code for users."
746,2024,"Predictions from Generative Artificial Intelligence Models: Towards a New Benchmark in Forecasting Practice nan This paper aims to determine whether there is a case for promoting a new benchmark for forecasting practice via the innovative application of generative artificial intelligence (Gen-AI) for predicting the future. Today, forecasts can be generated via Gen-AI models without the need for an in-depth understanding of forecasting theory, practice, or coding. Therefore, using three datasets, we present a comparative analysis of forecasts from Gen-AI models against forecasts from seven univariate and automated models from the forecast package in R, covering both parametric and non-parametric forecasting techniques. In some cases, we find statistically significant evidence to conclude that forecasts from Gen-AI models can outperform forecasts from popular benchmarks like seasonal ARIMA, seasonal naive, exponential smoothing, and Theta forecasts (to name a few). Our findings also indicate that the accuracy of forecasts from Gen-AI models can vary not only based on the underlying data structure but also on the quality of prompt engineering (thus highlighting the continued importance of forecasting education), with the forecast accuracy appearing to improve at longer horizons. Therefore, we find some evidence towards promoting forecasts from Gen-AI models as benchmarks in future forecasting practice. However, at present, users are cautioned against reliability issues and Gen-AI being a black box in some cases."
747,2024,"Impact of COVID-19 on arthritis with generative AI nan Objective: The study aims to examine the effects of the COVID-19 pandemic on the prevalence of arthritis in the US using a specific generative AI tool. Methods: The AI tool with Bing.com/copilot, designed to generate Python code, uses data from the Centers for Disease Control and Prevention (CDC) to visualize trends and uncover insights in four key areas: (1) The prevalence of arthritis in adults aged 18 years and older who have diabetes, (2) The prevalence of fair or poor health in adults aged 18 years and older who have arthritis, (3) The prevalence of activity limitations due to arthritis in adults aged 18 years and older with doctor -diagnosed arthritis, (4) The prevalence of arthritis in adults aged 18 years and older who are obese. This research did not require approval from an institutional review board or an ethics committee. Results: The findings reveal a significant decline in the prevalence of arthritis among adults with conditions such as diabetes and obesity during the COVID-19 pandemic. There was also an observed improvement in activity limitations among patients with doctor -diagnosed arthritis. Conclusion: The study highlights the potential impact of the pandemic on chronic disease management, particularly arthritis. It underscores the importance of continued monitoring and care for patients with arthritis, especially during a global health crisis like the COVID-19 pandemic. The use of AI tools in generating insights from health data proves to be valuable in this context."
748,2024,"AutoDev: Automated AI-Driven Development nan The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment."
749,2024,"How far are AI-powered programming assistants from meeting developers' needs? nan Recent In-IDE AI coding assistant tools (ACATs) like GitHub Copilot have significantly impacted developers' coding habits. While some studies have examined their effectiveness, there lacks in-depth investigation into the actual assistance process. To bridge this gap, we simulate real development scenarios encompassing three typical types of software development tasks and recruit 27 computer science students to investigate their behavior with three popular ACATs. Our goal is to comprehensively assess ACATs' effectiveness, explore characteristics of recommended code, identify reasons for modifications, and understand users' challenges and expectations. To facilitate the study, we develop an experimental platform that includes a data collection plugin for VSCode IDE and provides functions for screen recording, code evaluation, and automatic generation of personalized interview and survey questions. Through analysis of the collected data, we find that ACATs generally enhance task completion rates, reduce time, improve code quality, and increase self-perceived productivity. However, the improvement is influenced by both the nature of coding tasks and users' experience level. Notably, for experienced participants, the use of ACATs may even increase completion time. We observe that edited line completion is the most frequently recommended way, while comments completion and string completion have the lowest acceptance rates. The primary reasons for modifying recommended code are disparities between output formats and requirements, flawed logic, and inconsistent code styles. In terms of challenges and expectations, optimization of service access and help documentation is also concerned by participants except for functionality and performance. Our study provides valuable insights into the effectiveness and usability of ACATs, informing further improvements in their design and implementation."
750,2024,"Assessing Consensus of Developers' Views on Code Readability nan The rapid rise of Large Language Models (LLMs) has changed software development, with tools like Copilot, JetBrains AI Assistant, and others boosting developers' productivity. However, developers now spend more time reviewing code than writing it, highlighting the importance of Code Readability for code comprehension. Our previous research found that existing Code Readability models were inaccurate in representing developers' notions and revealed a low consensus among developers, highlighting a need for further investigations in this field. Building on this, we surveyed 10 Java developers with similar coding experience to evaluate their consensus on Code Readability assessments and related aspects. We found significant agreement among developers on Code Readability evaluations and identified specific code aspects strongly correlated with Code Readability. Overall, our study sheds light on Code Readability within LLM contexts, offering insights into how these models can align with developers' perceptions of Code Readability, enhancing software development in the AI era."
751,2024,"Current safeguards, risk mitigation, and transparency measures of large language models against the generation of health disinformation: repeated cross sectional analysis nan OBJECTIVES To evaluate the effectiveness of safeguards to prevent large language models (LLMs) from being misused to generate health disinformation, and to evaluate the transparency of artificial intelligence (AI) developers regarding their risk mitigation processes against observed vulnerabilities. DESIGN Repeated cross sectional analysis. SETTING Publicly accessible LLMs. METHODS In a repeated cross sectional analysis, four LLMs (via chatbots/assistant interfaces) were evaluated: OpenAI's GPT-4 (via ChatGPT and Microsoft's Copilot), Google's PaLM 2 and newly released Gemini Pro (via Bard), Anthropic's Claude 2 (via Poe), and Meta's Llama 2 (via HuggingChat). In September 2023, these LLMs were prompted to generate health disinformation on two topics: sunscreen as a cause of skin cancer and the alkaline diet as a cancer cure. Jailbreaking techniques (ie, attempts to bypass safeguards) were evaluated if required. For LLMs with observed safeguarding vulnerabilities, the processes for reporting outputs of concern were audited. 12 weeks after initial investigations, the disinformation generation capabilities of the LLMs were re-evaluated to assess any subsequent improvements in safeguards."
752,2023,"ChatGPT for PLC/DCS Control Logic Generation nan Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. A key contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control logic generation."
753,2024,"Artificial intelligence-powered chatbots in search engines: a cross-sectional study on the quality and risks of drug information for patients nan Background Search engines often serve as a primary resource for patients to obtain drug information. However, the search engine market is rapidly changing due to the introduction of artificial intelligence (AI)-powered chatbots. The consequences for medication safety when patients interact with chatbots remain largely unexplored.Objective To explore the quality and potential safety concerns of answers provided by an AI-powered chatbot integrated within a search engine.Methodology Bing copilot was queried on 10 frequently asked patient questions regarding the 50 most prescribed drugs in the US outpatient market. Patient questions covered drug indications, mechanisms of action, instructions for use, adverse drug reactions and contraindications. Readability of chatbot answers was assessed using the Flesch Reading Ease Score. Completeness and accuracy were evaluated based on corresponding patient drug information in the pharmaceutical encyclopaedia drugs.com. On a preselected subset of inaccurate chatbot answers, healthcare professionals evaluated likelihood and extent of possible harm if patients follow the chatbot's given recommendations.Results Of 500 generated chatbot answers, overall readability implied that responses were difficult to read according to the Flesch Reading Ease Score. Overall median completeness and accuracy of chatbot answers were 100.0% (IQR 50.0-100.0%) and 100.0% (IQR 88.1-100.0%), respectively. Of the subset of 20 chatbot answers, experts found 66% (95% CI 50% to 85%) to be potentially harmful. 42% (95% CI 25% to 60%) of these 20 chatbot answers were found to potentially cause moderate to mild harm, and 22% (95% CI 10% to 40%) to cause severe harm or even death if patients follow the chatbot's advice.Conclusions AI-powered chatbots are capable of providing overall complete and accurate patient drug information. Yet, experts deemed a considerable number of answers incorrect or potentially harmful. Furthermore, complexity of chatbot answers may limit patient understanding. Hence, healthcare professionals should be cautious in recommending AI-powered search engines until more precise and reliable alternatives are available."
754,2024,"A new generation of intelligent development environments nan The practice of programming is undergoing a revolution with the introduction of AI assisted development (copilots) and the creation of new programming languages that are designed explicitly for tooling, analysis, and automation. Integrated Development Environments (IDEs) as they are currently conceptualized have not yet responded to these changes. They are still designed around the idea of a human programmer typing textual code into an editor window with the IDE providing assistance via the integration of various tools for syntax highlighting, compilation, debugging, and (maybe) code version control. This paper presents a vision for transforming the IDE from an Integrated Development Environment to an Intelligent Development Environment. The new IDE will be designed around the idea of a human programmer as the manager or curator of a software project who, rather than manually typing in code to implement a solution, will instead use the IDE to direct AI programming agents and/or automated tools to combine existing APIs, packages, and new code to implement the needed features. In this new model, the fundamental roles of the IDE are to 1) facilitate the communication between the human programmer and the AI agents and automated tools and 2) organize the workflow tasks needed to go from requirements gathering to the final tested and validated deployed feature. This paper presents a vision for the new Intelligent Development Environment based on a range of proof-of-concept high-value scenarios we have experimented with and discusses the challenges that remain to realizing these in a cohesive intelligent development experience."
755,2024,"Strong and weak alignment of large language models with human values nan Minimizing negative impacts of Artificial Intelligent (AI) systems on human societies without human supervision requires them to be able to align with human values. However, most current work only addresses this issue from a technical point of view, e.g., improving current methods relying on reinforcement learning from human feedback, neglecting what it means and is required for alignment to occur. Here, we propose to distinguish strong and weak value alignment. Strong alignment requires cognitive abilities (either human-like or different from humans) such as understanding and reasoning about agents' intentions and their ability to causally produce desired effects. We argue that this is required for AI systems like large language models (LLMs) to be able to recognize situations presenting a risk that human values may be flouted. To illustrate this distinction, we present a series of prompts showing ChatGPT's, Gemini's and Copilot's failures to recognize some of these situations. We moreover analyze word embeddings to show that the nearest neighbors of some human values in LLMs differ from humans' semantic representations. We then propose a new thought experiment that we call the Chinese room with a word transition dictionary, in extension of John Searle's famous proposal. We finally mention current promising research directions towards a weak alignment, which could produce statistically satisfying answers in a number of common situations, however so far without ensuring any truth value."
756,2024,"What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI nan Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity -- variations in users' cognitive styles -- that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice? We developed a theoretical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools. We surveyed software developers (N=238) at two major global tech organizations and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience."
757,2024,"Can the Administrative Loads of Physicians be Alleviated by AI-Facilitated Clinical Documentation? nan BACKGROUND: Champions of AI-facilitated clinical documentation have suggested that the emergent technology may decrease the administrative loads of physicians, thereby reducing cognitive burden and forestalling burnout. Explorations of physicians' experiences with automated documentation are critical in evaluating these claims.OBJECTIVE: To evaluate physicians' experiences with DAX Copilot (DAXC), a generative AI-facilitated clinical documentation tool.DESIGN: Semi-structured interviews were conducted in August and September of 2023 with physician-users of DAXC.PARTICIPANTS: A purposive sample of 12 interviewees, selected from 116 primary care physicians, employed at a multi-site academic learning health system.APPROACH: After completing all 12 interviews, three study personnel independently analyzed and coded the transcripts. Reconciliation sessions were then held to merge the three analyses into one summary, eliminating redundant codes, and grouping findings into themes.KEY RESULTS: For a majority of interviewees, DAXC reduced the amount of time spent documenting encounters, and alleviated anxieties of having to retain important clinical details until there was time to make notes. DAXC also allowed physicians to be more engaged during appointments, resulting in more personable provider-patient encounters. However, some physicians weighed these benefits against an uneasy feeling that interviewees might be asked to see more patients if DAXC was mandated. Physicians also noted that the tool would occasionally imagine or misgender patients, offer unsolicited and inappropriate diagnoses, and mistake critical details in transcription. The few physicians less enthusiastic about the generative technology portrayed themselves as creatures of habit who had cultivated long-standing workflows and particular notation practices that DAXC could neither improve upon nor reproduce.CONCLUSIONS: According to physician interviewees, automated AI-driven clinical documentation has the potential to significantly reduce the administrative burden associated with particular types of provider-patient encounters. Addressing the growing pains of the incipient technology, identified here, may allow for a broader applicability for clinical practice."
758,2024,"Perforator Selection with Computed Tomography Angiography for Unilateral Breast Reconstruction: A Clinical Multicentre Analysis nan Background and Objectives: Despite CTAs being critical for preoperative planning in autologous breast reconstruction, experienced plastic surgeons may have differing preferences for which side of the abdomen to use for unilateral breast reconstruction. Large language models (LLMs) have the potential to assist medical imaging interpretation. This study compares the perforator selection preferences of experienced plastic surgeons with four popular LLMs based on CTA images for breast reconstruction. Materials and Methods: Six experienced plastic surgeons from Australia, the US, Italy, Denmark, and Argentina reviewed ten CTA images, indicated their preferred side of the abdomen for unilateral breast reconstruction and recommended the type of autologous reconstruction. The LLMs were prompted to do the same. The average decisions were calculated, recorded in suitable tables, and compared. Results: The six consultants predominantly recommend the DIEP procedure (83%). This suggests experienced surgeons feel more comfortable raising DIEP than TRAM flaps, which they recommended only 3% of the time. They also favoured MS TRAM and SIEA less frequently (11% and 2%, respectively). Three LLMs-ChatGPT-4o, ChatGPT-4, and Bing CoPilot-exclusively recommended DIEP (100%), while Claude suggested DIEP 90% and MS TRAM 10%. Despite minor variations in side recommendations, consultants and AI models clearly preferred DIEP. Conclusions: Consultants and LLMs consistently preferred DIEP procedures, indicating strong confidence among experienced surgeons, though LLMs occasionally deviated in recommendations, highlighting limitations in their image interpretation capabilities. This emphasises the need for ongoing refinement of AI-assisted decision support systems to ensure they align more closely with expert clinical judgment and enhance their reliability in clinical practice."
759,2023,"The Robots are Here: Navigating the Generative AI Revolution in Computing Education nan Recent advancements in artificial intelligence (AI) and specifically generative AI (GenAI) are threatening to fundamentally reshape computing and society. Largely driven by large language models (LLMs), many tools are now able to interpret and generate both natural language instructions and source code. These capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of generative AI in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles, nearly 80% of which have been published in the first 8 months of 2023. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards GenAI/LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of several current GenAI models/tools on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving.There is little doubt that LLMs and other forms of GenAI will have a profound impact on computing education over the coming years. However, just as the technology will continue to improve, so will our collective knowledge about how to leverage these new models and tools in educational settings. We expect many important conversations around this topic will emerge as the community explores how to provide more effective, inclusive, and personalised learning experiences. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating GenAI and LLM-based tools in computing classrooms."
760,2024,Using AI as a Development Accelerator nan nan
761,2023,"Evaluation of OpenAI Codex for HPC Parallel Programming Models Kernel Generation [arXiv] nan We evaluate AI-assisted generative capabilities on fundamental numerical kernels in high-performance computing (HPC), including AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG. We test the generated kernel codes for a variety of language-supported programming models, including (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numba, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). We use the GitHub Copilot capabilities powered by OpenAI Codex available in Visual Studio Code as of April 2023 to generate a vast amount of implementations given simple + + prompt variants. To quantify and compare the results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. Results suggest that the OpenAI Codex outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding code keywords, while Julia prompts perform acceptably well for its mature programming models (e.g., Threads and CUDA.jl). We expect for these benchmarks to provide a point of reference for each programming model's community. Overall, understanding the convergence of large language models, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions."
763,2024,"GENEVIC: GENetic data Exploration and Visualization via Intelligent interactive Console nan The vast generation of genetic data poses a significant challenge in efficiently uncovering valuable knowledge. Introducing GENEVIC, an AI-driven chat framework that tackles this challenge by bridging the gap between genetic data generation and biomedical knowledge discovery. Leveraging generative AI, notably ChatGPT, it serves as a biologist's copilot. It automates the analysis, retrieval, and visualization of customized domain-specific genetic information, and integrates functionalities to generate protein interaction networks, enrich gene sets, and search scientific literature from PubMed, Google Scholar, and arXiv, making it a comprehensive tool for biomedical research. In its pilot phase, GENEVIC is assessed using a curated database that ranks genetic variants associated with Alzheimer's disease, schizophrenia, and cognition, based on their effect weights from the Polygenic Score (PGS) Catalog, thus enabling researchers to prioritize genetic variants in complex diseases. GENEVIC's operation is user-friendly, accessible without any specialized training, secured by Azure OpenAI's HIPAA-compliant infrastructure, and evaluated for its efficacy through real-time query testing. As a prototype, GENEVIC is set to advance genetic research, enabling informed biomedical decisions.Availability and implementation GENEVIC is publicly accessible at https://genevicanath2024.streamlit.app. The underlying code is open-source and available via GitHub at https://github.com/bsml320/GENEVIC.git (also at https://github.com/anath2110/GENEVIC.git)."
764,2024,"Coaching Copilot: Blended Form of an LLM-Powered Chatbot and a Human Coach to Effectively Support Self-Reflection for Leadership Growth nan Chatbots' role in fostering self-reflection is now widely recognized, especially in inducing users' behavior change. While the benefits of 24/7 availability, scalability, and consistent responses have been demonstrated in contexts such as healthcare and tutoring to help one form a new habit, their utilization in coaching necessitating deeper introspective dialogue to induce leadership growth remains unexplored. This paper explores the potential of such a chatbot powered by recent Large Language Models (LLMs) in collaboration with professional coaches in the field of executive coaching. Through a design workshop with them and two weeks of user study involving ten coach-client pairs, we explored the feasibility and nuances of integrating chatbots to complement human coaches. Our findings highlight the benefits of chatbots' ubiquity and reasoning capabilities enabled by LLMs while identifying their limitations and design necessities for effective collaboration between human coaches and chatbots. By doing so, thiswork contributes to the foundation for augmenting one's self-reflective process with prevalent conversational agents through the human-in-the-loop approach."
765,2024,"GENEVIC: GENetic data Exploration and Visualization via Intelligent interactive Console nan The vast generation of genetic data poses a significant challenge in efficiently uncovering valuable knowledge. Introducing GENEVIC, an AI-driven chat framework that tackles this challenge by bridging the gap between genetic data generation and biomedical knowledge discovery. Leveraging generative AI, notably ChatGPT, it serves as a biologist's 'copilot'. It automates the analysis, retrieval, and visualization of customized domain-specific genetic information, and integrates functionalities to generate protein interaction networks, enrich gene sets, and search scientific literature from PubMed, Google Scholar, and arXiv, making it a comprehensive tool for biomedical research. In its pilot phase, GENEVIC is assessed using a curated database that ranks genetic variants associated with Alzheimer's disease, schizophrenia, and cognition, based on their effect weights from the Polygenic Score Catalog, thus enabling researchers to prioritize genetic variants in complex diseases. GENEVIC's operation is user-friendly, accessible without any specialized training, secured by Azure OpenAI's HIPAA-compliant infrastructure, and evaluated for its efficacy through real-time query testing. As a prototype, GENEVIC is set to advance genetic research, enabling informed biomedical decisions. Availability and implementation: GENEVIC is publicly accessible at https://genevic-anath2024.streamlit.app. The underlying code is open-source and available via GitHub at https://github.com/anath2110/GENEVIC.git."
766,2025,"PragFormer: Data-Driven Parallel Source Code Classification with Transformers nan Multi-core shared memory architectures have become ubiquitous in computing hardware nowadays. As a result, there is a growing need to fully utilize these architectures by introducing appropriate parallelization schemes, such as OpenMP worksharing-loop constructs, to applications. However, most developers find introducing OpenMP directives to their code hard due to pervasive pitfalls in managing parallel shared memory. To assist developers in this process, many compilers, as well as source-to-source (S2S) translation tools, have been developed over the years, tasked with inserting OpenMP directives into code automatically. In addition to having limited robustness to their input format, these compilers still do not achieve satisfactory coverage and precision in locating parallelizable code and generating appropriate directives. Recently, many data-driven AI-based code completion (CC) tools, such as GitHub CoPilot, have been developed to ease and improve programming productivity. Leveraging the insights from existing AI-based programming-assistance tools, this work presents a novel AI model that can serve as a parallel-programming assistant. Specifically, our model, named PragFormer, is tasked with identifying for loops that can benefit from conversion to parallel worksharing-loop construct (OpenMP directive) and even predict the need for specific data-sharing attributes clauses on the fly. We created a unique database, named Open-OMP, specifically for this goal. Open-OMP contains over 32,000 unique code snippets from different domains, half of which contain OpenMP directives, while the other half do not. We experimented with different model design parameters for these tasks and showed that our best-performing model outperforms a statistically-trained baseline as well as a state-of-the-art S2S compiler. In fact, it even outperforms the popular generative AI model of ChatGPT. In the spirit of advancing research on this topic, we have already released source code for PragFormer as well as Open-OMP dataset to public. Moreover, an interactive demo of our tool, as well as a Hugging Face webpage to experiment with our tool, are already available."
767,2024,"Large language models in periodontology: Assessing their performance in clinically relevant questions. nan STATEMENT OF PROBLEM: Although the use of artificial intelligence (AI) seems promising and may assist dentists in clinical practice, the consequences of inaccurate or even harmful responses are paramount. Research is required to examine whether large language models (LLMs) can be used in accessing periodontal content reliably.PURPOSE: The purpose of this study was to evaluate and compare the evidence-based potential of answers provided by 4 LLMs to common clinical questions in the field of periodontology.MATERIAL AND METHODS: A total of 10 open-ended questions pertinent to periodontology were posed to 4 distinct LLMs: ChatGPT model GPT 4.0, Google Gemini, Google Gemini Advanced, and Microsoft Copilot. The answers to each question were evaluated independently by 2 periodontists against robust scientific evidence based on a predefined rubric assessing the comprehensiveness, scientific accuracy, clarity, and relevance. Each response received a score ranging from 0 (minimum) to 10 (maximum). After a period of 2 weeks from initial evaluation, the answers were re-graded independently to gauge intra-evaluator reliability. Inter-evaluator reliability was assessed using correlation tests, while Cronbach alpha and interclass correlation coefficient were used to measure overall reliability. The Kruskal-Wallis test was employed to compare the scores given by different LLMs.RESULTS: The scores provided by the 2 evaluators for both evaluations were statistically similar (P values ranging from.083 to >;.999), therefore an average score was calculated for each LLM. Both evaluators gave the highest scores to the answers generated by ChatGPT 4.0, while Google Gemini had the lowest scores. ChatGPT 4.0 received the highest average score, while significant differences were detected between ChatGPT 4.0 and Google Gemini (P=.042). ChatGPT 4.0 answers were found to be highly comprehensive, with scientific accuracy, clarity, and relevance.CONCLUSIONS: Professionals need to be aware of the limitations of LLMs when utilizing them. These models must not replace dental professionals as improper use may negatively impact patient care. Chat GPT 4.0, Google Gemini, Google Gemini Advanced, and Microsoft CoPilot performed relatively well with Chat GPT 4.0 demonstrating the highest performance."
768,1997,"Rugged COTS 12.1 diagonal AMLCD multipurpose display for the US Army's Rotorcraft Pilot's Associate cockpit nan The primary objective of the Rotorcraft pilot's Associate (RPA) program is to enhance mission effectiveness of future combat helicopters through development and application of knowledge based associate systems for cognitive decision aiding. Enhanced mission capability is supported by an increase in pilot situational awareness made possible through the development of the associate and the integration of advanced sensors, controls, and displays. The crewstation display suite showcases ruggedized commercial off-the-shelf (COTS) 12.1 diagonal, full color, 64 gray shade, XGA (1024*768) resolution AMLCDs (Active Matrix Liquid Crystal Display). These multipurpose displays (MPD) will be installed three abreast landscape style in the Copilot Gunner (CPG) station of an AH-64D Longbow Apache helicopter for RPA system flight testing. The display program requirements and system architecture are outlined and discussed. The display unit subassembly details are provided with justification related to design level trade studies."
769,2024,"Assured Automatic Programming via Large Language Models nan With the advent of AI-based coding engines, it is possible to convert natural language requirements to executable code in standard programming languages. However, AI-generated code can be unreliable, and the natural language requirements driving this code may be ambiguous. In other words, the intent may not be accurately captured in the code generated from AI-coding engines like Copilot. The goal of our work is to discover the programmer intent, while generating code which conforms to the intent and a proof of this conformance. Our approach to intent discovery is powered by a novel repair engine called program-proof co-evolution, where the object of repair is a tuple (code, logical specification, test) generated by an LLM from the same natural language description. The program and the specification capture the initial operational and declarative description of intent, while the test represents a concrete, albeit partial, understanding of the intent. Our objective is to achieve consistency between the program, the specification, and the test by incrementally refining our understanding of the user intent. Reaching consistency through this repair process provides us with a formal, logical description of the intent, which is then translated back into natural language for the developer's inspection. The resultant intent description is now unambiguous, though expressed in natural language. We demonstrate how the unambiguous intent discovered through our approach increases the percentage of verifiable auto-generated programs on a recently proposed dataset in the Dafny programming language."
770,2024,"Trace is the New AutoDiff -- Unlocking Efficient Optimization of Computational Workflows nan We study a class of optimization problems motivated by automating the design and update of AI systems like coding assistants, robots, and copilots. We propose an end-to-end optimization framework, Trace, which treats the computational workflow of an AI system as a graph akin to neural networks, based on a generalization of back-propagation. Optimization of computational workflows often involves rich feedback (e.g. console output or user's responses), heterogeneous parameters (e.g. prompts, hyper-parameters, codes), and intricate objectives (beyond maximizing a score). Moreover, its computation graph can change dynamically with the inputs and parameters. We frame a new mathematical setup of iterative optimization, Optimization with Trace Oracle (OPTO), to capture and abstract these properties so as to design optimizers that work across many domains. In OPTO, an optimizer receives an execution trace along with feedback on the computed output and updates parameters iteratively. Trace is the tool to implement OPTO in practice. Trace has a Python interface that efficiently converts a computational workflow into an OPTO instance using a PyTorch-like interface. Using Trace, we develop a general-purpose LLM-based optimizer called OptoPrime that can effectively solve OPTO problems. In empirical studies, we find that OptoPrime is capable of first-order numerical optimization, prompt optimization, hyper-parameter tuning, robot controller design, code debugging, etc., and is often competitive with specialized optimizers for each domain. We believe that Trace, OptoPrime and the OPTO framework will enable the next generation of interactive agents that automatically adapt using various kinds of feedback. Website: https://microsoft.github.io/Trace"
771,2022,"Diplomats' Behavior on Social Platforms - Cross Country Comparison Between Portuguese, British and Irish Diplomats - The Case of Twitter nan nan"
772,2023,"GPTutor: A ChatGPT-Powered Programming Tool for Code Explanation nan Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPTAPI to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers indicated that GPTutor is user-friendly and can explain given codes satisfactorily. Finally, we discuss possible future research directions for GPTutor. This includes enhancing its performance and personalization via further prompt programming, as well as evaluating the effectiveness of GPTutor with real users."
773,2024,"Unveiling Assumptions: Exploring the Decisions of AI Chatbots and Human Testers nan The integration of Large Language Models (LLMs) and chatbots introduces new challenges and opportunities for decision-making in software testing. Decision-making relies on a variety of information, including code, requirements specifications, and other software artifacts that are often unclear or exist solely in the developer's mind. To fill in the gaps left by unclear information, we often rely on assumptions, intuition, or previous experiences to make decisions. This paper explores the potential of LLM-based chatbots like Bard, Copilot, and ChatGPT, to support software testers in test decisions such as prioritizing test cases effectively. We investigate whether LLM-based chatbots and human testers share similar assumptions or intuition in prohibitive testing scenarios where exhaustive execution of test cases is often impractical. Preliminary results from a survey of 127 testers indicate a preference for diverse test scenarios, with a significant majority (96%) favoring dissimilar test sets. Interestingly, two out of four chatbots mirrored this preference, aligning with human intuition, while the others opted for similar test scenarios, chosen by only 3.9% of testers. Our initial insights suggest a promising avenue within the context of enhancing the collaborative dynamics between testers and chatbots."
774,2022,Digital Public Diplomacy at the Portuguese Ministry of Foreign Affairs: Advantages and Disadvantages of Diplomacy Under the Context of Global Digitalization nan nan
775,2024,Human-Centered Program Synthesis nan nan
777,2024,"The Widening Gap: The Benefits and Harms of Generative AI for Novice Programmers nan Novice programmers often struggle through programming problem solving due to a lack of metacognitive awareness and strategies. Previous research has shown that novices can encounter multiple metacognitive difficulties while programming, such as forming incorrect conceptual models of the problem or having a false sense of progress after testing their solution. Novices are typically unaware of how these difficulties are hindering their progress. Meanwhile, many novices are now programming with generative AI (GenAI), which can provide complete solutions to most introductory programming problems, code suggestions, hints for next steps when stuck, and explain cryptic error messages. Its impact on novice metacognition has only started to be explored. Here we replicate a previous study that examined novice programming problem solving behavior and extend it by incorporating GenAI tools. Through 21 lab sessions consisting of participant observation, interview, and eye tracking, we explore how novices are coding with GenAI tools. Although 20 of 21 students completed the assigned programming problem, our findings show an unfortunate divide in the use of GenAI tools between students who did and did not struggle. Some students who did not struggle were able to use GenAI to accelerate, creating code they already intended to make, and were able to ignore unhelpful or incorrect inline code suggestions. But for students who struggled, our findings indicate that previously known metacognitive difficulties persist, and that GenAI unfortunately can compound them and even introduce new metacognitive difficulties. Furthermore, struggling students often expressed cognitive dissonance about their problem solving ability, thought they performed better than they did, and finished with an illusion of competence. Based on our observations from both groups, we propose ways to scaffold the novice GenAI experience and make suggestions for future work."
779,2023,"Evaluating the Performance of Code Generation Models for Solving Parsons Problems With Small Prompt Variations nan The recent emergence of code generation tools powered by large language models has attracted wide attention. Models such as OpenAI Codex can take natural language problem descriptions as input and generate highly accurate source code solutions, with potentially significant implications for computing education. Given the many complexities that students face when learning to write code, they may quickly become reliant on such tools without properly understanding the underlying concepts. One popular approach for scaffolding the code writing process is to use Parsons problems, which present solution lines of code in a scrambled order. These remove the complexities of low-level syntax, and allow students to focus on algorithmic and design-level problem solving. It is unclear how well code generation models can be applied to solve Parsons problems, given the mechanics of these models and prior evidence that they underperform when problems include specific restrictions. In this paper, we explore the performance of the Codex model for solving Parsons problems over various prompt variations. Using a corpus of Parsons problems we sourced from the computing education literature, we find that Codex successfully reorders the problem blocks about half of the time, a much lower rate of success when compared to prior work on more free-form programming tasks. Regarding prompts, we find that small variations in prompting have a noticeable effect on model performance, although the effect is not as pronounced as between different problems."
780,2024,"Transforming the Synthesis of Carbon Nanotubes with Machine Learning Models and Automation nan Carbon-based nanomaterials (CBNs) are showing significant potential in various fields, such as electronics, energy, and mechanics. However, their practical applications face synthesis challenges stemming from the complexities of structural control, large-area uniformity, and high yield. Current research methodologies fall short in addressing the multi-variable, coupled interactions inherent to CBNs production. Machine learning methods excel at navigating such complexities. Their integration with automated synthesis platforms has demonstrated remarkable potential in accelerating chemical synthesis research, but remains underexplored in the nanomaterial domain. Here we introduce Carbon Copilot (CARCO), an artificial intelligence (AI)-driven platform that integrates transformer-based language models tailored for carbon materials, robotic chemical vapor deposition (CVD), and data-driven machine learning models, empowering accelerated research of CBNs synthesis. Employing CARCO, we demonstrate innovative catalyst discovery by predicting a superior Titanium-Platinum bimetallic catalyst for high-density horizontally aligned carbon nanotube (HACNT) array synthesis, validated through over 500 experiments. Furthermore, with the assistance of millions of virtual experiments, we achieved an unprecedented 56.25% precision in synthesizing HACNT arrays with predetermined densities in the real world. All were accomplished within just 43 days. This work not only advances the field of HACNT arrays but also exemplifies the integration of AI with human expertise to overcome the limitations of traditional experimental approaches, marking a paradigm shift in nanomaterials research and paving the way for broader applications."
781,2024,"Designing Chatbots to Support Victims and Survivors of Domestic Abuse [arXiv] nan Objective: Domestic abuse cases have risen significantly over the last four years, in part due to the COVID-19 pandemic and the challenges for victims and survivors in accessing support. In this study, we investigate the role that chatbots - Artificial Intelligence (AI) and rule-based - may play in supporting victims/survivors in situations such as these or where direct access to help is limited. Methods: Interviews were conducted with experts working in domestic abuse support services and organizations (e.g., charities, law enforcement) and the content of websites of related support-service providers was collected. Thematic content analysis was then applied to assess and extract insights from the interview data and the content on victim-support websites. We also reviewed pertinent chatbot literature to reflect on studies that may inform design principles and interaction patterns for agents used to support victims/survivors. Results: From our analysis, we outlined a set of design considerations/practices for chatbots that consider potential use cases and target groups, dialog structure, personality traits that might be useful for chatbots to possess, and finally, safety and privacy issues that should be addressed. Of particular note are situations where AI systems (e.g., ChatGPT, CoPilot, Gemini) are not recommended for use, the value of conveying emotional support, the importance of transparency, and the need for a safe and confidential space. Conclusion: It is our hope that these considerations/practices will stimulate debate among chatbots and AI developers and service providers and - for situations where chatbots are deemed appropriate for use - inspire efficient use of chatbots in the support of survivors of domestic abuse."
782,2024,"ESM Cloud Toolkit: A Copilot for Energy Storage Material Research nan Searching and designing new materials play crucial roles in the development of energy storage devices. In today's world where machine learning technology has shown strong predictive ability for various tasks, the combination with machine learning technology will accelerate the process of material development. Herein, we develop ESM Cloud Toolkit for energy storage materials based on MatElab platform, which is designed as a convenient and accurate way to automatically record and save the raw data of scientific research. The ESM Cloud Toolkit includes multiple features such as automatic archiving of computational simulation data, post-processing of experimental data, and machine learning applications. It makes the entire research workflow more automated and reduces the entry barrier for the application of machine learning technology in the domain of energy storage materials. It integrates data archive, traceability, processing, and reutilization, and allows individual research data to play a greater role in the era of AI."
783,2024,"Large language model evaluation for high-performance computing software development nan We apply AI-assisted large language model (LLM) capabilities of GPT-3 targeting high-performance computing (HPC) kernels for (i) code generation, and (ii) auto-parallelization of serial code in C ++, Fortran, Python and Julia. Our scope includes the following fundamental numerical kernels: AXPY, GEMV, GEMM, SpMV, Jacobi Stencil, and CG, and language/programming models: (1) C++ (e.g., OpenMP [including offload], OpenACC, Kokkos, SyCL, CUDA, and HIP), (2) Fortran (e.g., OpenMP [including offload] and OpenACC), (3) Python (e.g., numpy, Numba, cuPy, and pyCUDA), and (4) Julia (e.g., Threads, CUDA.jl, AMDGPU.jl, and KernelAbstractions.jl). Kernel implementations are generated using GitHub Copilot capabilities powered by the GPT-based OpenAI Codex available in Visual Studio Code given simple + + prompt variants. To quantify and compare the generated results, we propose a proficiency metric around the initial 10 suggestions given for each prompt. For auto-parallelization, we use ChatGPT interactively giving simple prompts as in a dialogue with another human including simple prompt engineering follow ups. Results suggest that correct outputs for C++ correlate with the adoption and maturity of programming models. For example, OpenMP and CUDA score really high, whereas HIP is still lacking. We found that prompts from either a targeted language such as Fortran or the more general-purpose Python can benefit from adding language keywords, while Julia prompts perform acceptably well for its Threads and CUDA.jl programming models. We expect to provide an initial quantifiable point of reference for code generation in each programming model using a state-of-the-art LLM. Overall, understanding the convergence of LLMs, AI, and HPC is crucial due to its rapidly evolving nature and how it is redefining human-computer interactions."
784,2024,"The Promise and Challenge of Large Language Models for Knowledge Engineering: Insights from a Hackathon nan Knowledge engineering (KE) is the process of building, maintaining and using knowledge-based systems. This recently takes the form of knowledge graphs (KGs). The advent of new technologies like Large Language Models (LLMs) has the potential to improve automation in KE work due to the richness of their training data and their performance at solving natural language processing tasks. We conducted a multiple-methods study exploring user opinions and needs regarding the use of LLMs in KE. We used ethnographic techniques to observe KE workers using LLMs to solve KE tasks during a hackathon, followed by interviews with some of the participants. This interim study found that despite LLMs' promising capabilities for efficient knowledge acquisition and requirements elicitation, their effective deployment requires an extended set of capabilities and training, particularly in prompting and understanding data. LLMs can be useful for simple quality assessment tasks, but in complex scenarios, the output is hard to control and evaluation may require novel approaches. With this study, we aim to evidence the interaction of KE stakeholders with LLMs, identify areas of potential, and understand the barriers to their effective use. We find copilot approaches may be valuable in developing processes where the human or a team of humans is assisted by generative AI."
785,2024,"Assessing Biases in the Names Generated by Generative Artificial Intelligence Chatbots nan Generative artificial intelligence (GenAI) is becoming more prevalent in higher education, and with that comes opportunities and challenges. One opportunity is using this technology to help create educational material, but one challenge is that the output of these tools might produce biased content. Thus, for this work, three text-based GenAI tools (ChatGPT-4o, Microsoft Copilot, and Google Gemini) were used to develop an activity for an analytical chemistry laboratory course. In each response, the student names provided by the chatbots were quantified with respect to gender and broadly assessed for cultural representation. All three chatbots generated an equal percentage of female (she/her) and male (he/him) student names, but none of the chatbots used they/them pronouns, signaling a lack of inclusivity for nonbinary, gender-neutral, or gender-nonconforming individuals. The names provided by the chatbots were dominated by those popular in English-speaking countries, highlighting a lack of cultural diversity in the output provided. Both these biases could be mitigated by asking that the chatbots provide gender-inclusive names and names that represent diverse cultural backgrounds. As educators begin to utilize GenAI tools to create classroom materials or have students use this technology in their assignments, it is important to think about the potential biases that might emerge, share this limitation with those using these tools, and work to not perpetuate them."
787,2024,"Enhancing Incremental Dataflow Analysis in an IDE nan Incremental dataflow analysis is a conventional technique adopted in syntax-directed editors, popularly used in Integrated Development Environments (IDEs). However, dataflow anomaly detection during program editing in IDEs remains a challenge due to the interaction with newer functionalities, such as AI-powered plugins like Copilot, which might insert and modify code lines directly. These interactions impact both the efficiency and effectiveness of dataflow anomaly detection processes. This paper introduces an enhanced incremental dataflow analysis approach in IDEs, focused on the analysis of dataflow within program structures. Our approach introduces a modified version of SP-tree, named SP-graph. This modification, involving the addition of supporting edges, facilitates data storage and enhances dataflow analysis incrementally. We also present a mechanism that streamlines dataflow anomaly detection within IDEs by identifying modifications that may impact dataflow, updating relevant data, and conducting immediate analysis. In addition, incremental concurrent anomaly detection in SP-graph shall be studied further. We demonstrate the effectiveness of our approach through a case study."
788,2024,"CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models nan Large language models (LLMs) for automatic code generation have recently achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. Training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively evaluated for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.In this work, we propose a method to systematically study the security issues of code language models to assess their susceptibility to generating vulnerable code. To this end, we introduce the first approach to automatically find generated code that contains vulnerabilities in black-box code generation models. This involves proposing a novel few-shot prompting approach. We evaluate the effectiveness of our approach by examining code language models in generating high-risk security weaknesses. Furthermore, we use our method to create a collection of diverse non-secure prompts for various vulnerability scenarios. This dataset serves as a benchmark to evaluate and compare the security weaknesses of code language models."
789,2023,"What Skills Do You Need When Developing Software Using ChatGPT? (Discussion Paper) nan Since the release of LLM-based tools such as GitHub Copilot and ChatGPT the media and popular scientific literature, but also journals such as the Communications of the ACM, have been flooded with opinions how these tools will change programming. The opinions range from machines will program themselves, to AI does not help programmers. Of course, these statements are meant to to stir up a discussion, and should be taken with a grain of salt, but we argue that such unfounded statements are potentially harmful. Instead, we propose to investigate which skills are required to develop software using LLM-based tools. In this paper we report on an experiment in which we explore if Computational Thinking (CT) skills predict the ability to develop software using LLM-based tools. Our results show that the ability to develop software using LLM-based tools can indeed be predicted by the score on a CT assessment. There are many limitations to our experiment, and this paper is also a call to discuss how to approach, preferably experimentally, the question of which skills are required to develop software using LLM-based tools. We propose to rephrase this question to include by what kind of people/programmers, to develop what kind of software using what kind of LLM-based tools."
790,2024,"Guidelines for the Evolving Role of Generative AI in Introductory Programming Based on Emerging Practice nan In the rapidly evolving Generative AI (GenAI) landscape, source code and natural language are being mixed and used in new ways. This presents opportunities for rethinking teaching practice in Introductory Programming (CS1) courses that includes, but goes beyond, assessment. In this paper we examine the reasons why and how instructors who are early adopters of GenAI are using it in their teaching, and why others are not. We also explore the changes and adaptations that are currently being made to practice. This is achieved by synthesizing insights from several recent studies that have collected primary data from introductory programming instructors who are teaching with, considering teaching with, or actively not teaching with GenAI.Due to the fast pace of GenAI development and adoption, the fixed-pace and cyclical nature of education, and the relatively slow pace of research (including ethical approvals) and publication cycles, research with primary data from instructors is only being published relatively recently. In computing education, there is not yet enough published research with primary data from CS1 instructors to warrant a systematic literature review, although in the next year this will likely be possible. Based on an analysis of the nascent research that has been published, we propose emerging and flexible guidelines on how CS1 instructors could adapt their practice based on what others have done so far. These guidelines highlight important factors to consider when integrating GenAI in CS1 courses, which for many is only beginning."
792,2024,"Greening Large Language Models of Code nan Large language models of code have shown remarkable effectiveness across various software engineering tasks. Despite the availability of many cloud services built upon these powerful models, there remain several scenarios where developers cannot take full advantage of them, stemming from factors such as restricted or unreliable internet access, institutional privacy policies that prohibit external transmission of code to third-party vendors, and more. Therefore, developing a compact, efficient, and yet energy-saving model for deployment on developers' devices becomes essential.To this aim, we propose Avatar, a novel approach that crafts a deployable model from a large language model of code by optimizing it in terms of model size, inference latency, energy consumption, and carbon footprint while maintaining a comparable level of effectiveness (e.g., prediction accuracy on downstream tasks). The key idea of Avatar is to formulate the optimization of language models as a multi-objective configuration tuning problem and solve it with the help of a Satisfiability Modulo Theories (SMT) solver and a tailored optimization algorithm. The SMT solver is used to form an appropriate configuration space, while the optimization algorithm identifies the Pareto-optimal set of configurations for training the optimized models using knowledge distillation. We evaluate Avatar with two popular language models of code, i.e., CodeBERT and GraphCodeBERT, on two popular tasks, i.e., vulnerability prediction and clone detection. We use Avatar to produce optimized models with a small size (3 MB), which is 160* smaller than the original large models. On the two tasks, the optimized models significantly reduce the energy consumption (up to 184* less), carbon footprint (up to 157* less), and inference latency (up to 76* faster), with only a negligible loss in effectiveness (1.67%).Lay AbstractLarge language models of code have proven to be highly effective for various software engineering tasks, such as spotting program defects and helping developers write code. While many cloud services built on these models (e.g., GitHub Copilot) are now accessible, several factors, such as unreliable internet access (e.g., over 20% of GitHub Copilot's issues are related to network connectivity [22]) and privacy concerns (e.g., Apple has banned the internal use of external AI tools to protect confidential data [53]), hinder developers from fully utilizing these services. Therefore, deploying language models of code on developers' devices like laptops appears promising. However, local deployment faces challenges: (1) Consumer-grade personal devices typically lack sufficient memory and the high-performance CPUs/GPUs required for efficient model execution; (2) Even if the hardware requirements are met, deploying the models on many devices can result in considerable energy consumption and carbon emissions, negatively impacting environmental sustainability.To address these challenges, we present Avatar, an innovative approach that optimizes large language models of code and enables their deployment on consumer-grade devices. Avatar can optimize two popular models from a large size of 481 MB to a compact size of 3 MB, resulting in significant reductions in inference time, energy consumption, and carbon emissions by hundreds of times. Our technique effectively lowers the entry barrier for leveraging large language models of code, making them available to ordinary developers without the need for high-performance computing equipment. Furthermore, it also contributes to a more sustainable and user-friendly software development environment."
793,2017,"Investigating a Two-Way-Audio Query-Response Command Interface with Navigation Data Extraction for Driver Assistance nan Navigation systems for drivers tend to vocalize what they want, when they want. Drivers who need to clarify instructions are often required to read directions in text form, which is a safety risk during real-time control.Instead, we investigate how a two-way-audio command interface might work. Data is easily extracted from text directions so that questions can be answered without knowledge of the GPS location, as long as the system is aware of the most recently vocalized directional cue (and elapsed time since last utterance). Functions envisioned are as simple as scrolling forward or back through the directions, spelling names or pronouncing differently, repeating the road name or directional change, and saying what is next on command. The queries require modest data extraction with potentially great improvement in usability. Queries may further refer to time and the results of some intermittent internet search, but do not attempt to interpret 2d map data.This project is ongoing, and the contribution includes a novel design architecture that puts a second AI assistant in the copilot's seat. This design provides domain-specific memory and vocalization assistance on top of the navigational assistance already familiar to the driver: voice AI (vAI) for improved voice UI (vUI).It aims more for a smart TiVo than co-pilot Siri. When navigation assistance and two-way audio dialogue are provided by the same vendor, the audio recognition and current location errors can be significantly reduced, but the current design is intended as a third-party intervention, adding assistance where the original product is lacking and possibly creating pressure for their products to improve."
794,2024,"Towards Responsible AI: Safeguarding Privacy, Integrity, and Fairness nan nan"
795,2023,"GPTutor: A ChatGPT-Powered Programming Tool for Code Explanation nan Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers indicated that GPTutor is user-friendly and can explain given codes satisfactorily. Finally, we discuss possible future research directions for GPTutor. This includes enhancing its performance and personalization via further prompt programming, as well as evaluating the effectiveness of GPTutor with real users."
796,2024,"Examining the Role of Large Language Models in Orthopedics: Systematic Review. nan BACKGROUND: Large language models (LLMs) can understand natural language and generate corresponding text, images, and even videos based on prompts, which holds great potential in medical scenarios. Orthopedics is a significant branch of medicine, and orthopedic diseases contribute to a significant socioeconomic burden, which could be alleviated by the application of LLMs. Several pioneers in orthopedics have conducted research on LLMs across various subspecialties to explore their performance in addressing different issues. However, there are currently few reviews and summaries of these studies, and a systematic summary of existing research is absent.OBJECTIVE: The objective of this review was to comprehensively summarize research findings on the application of LLMs in the field of orthopedics and explore the potential opportunities and challenges.METHODS: PubMed, Embase, and Cochrane Library databases were searched from January 1, 2014, to February 22, 2024, with the language limited to English. The terms, which included variants of large language model, generative artificial intelligence, ChatGPT, and orthopaedics, were divided into 2 categories: large language model and orthopedics. After completing the search, the study selection process was conducted according to the inclusion and exclusion criteria. The quality of the included studies was assessed using the revised Cochrane risk-of-bias tool for randomized trials and CONSORT-AI (Consolidated Standards of Reporting Trials-Artificial Intelligence) guidance. Data extraction and synthesis were conducted after the quality assessment.RESULTS: A total of 68 studies were selected. The application of LLMs in orthopedics involved the fields of clinical practice, education, research, and management. Of these 68 studies, 47 (69%) focused on clinical practice, 12 (18%) addressed orthopedic education, 8 (12%) were related to scientific research, and 1 (1%) pertained to the field of management. Of the 68 studies, only 8 (12%) recruited patients, and only 1 (1%) was a high-quality randomized controlled trial. ChatGPT was the most commonly mentioned LLM tool. There was considerable heterogeneity in the definition, measurement, and evaluation of the LLMs' performance across the different studies. For diagnostic tasks alone, the accuracy ranged from 55% to 93%. When performing disease classification tasks, ChatGPT with GPT-4's accuracy ranged from 2% to 100%. With regard to answering questions in orthopedic examinations, the scores ranged from 45% to 73.6% due to differences in models and test selections.CONCLUSIONS: LLMs cannot replace orthopedic professionals in the short term. However, using LLMs as copilots could be a potential approach to effectively enhance work efficiency at present. More high-quality clinical trials are needed in the future, aiming to identify optimal applications of LLMs and advance orthopedics toward higher efficiency and precision."
797,1990,Intelligent manoeuvring and execution control nan In the framework of the European Prometheus Eureka project the authors' work concerns the development of an intelligent copilot which assists the pilot in his driving tasks by giving soft and hard warnings. This paper deals with intelligent manoeuvring and execution control for cars on roads. It concerns the development of an interactive system for the execution control which monitors the car manoeuvres in order to maintain sufficient safety margins. The principle consists in hierarchically structuring the driving activities from strategical level (going from one place to another one) to manoeuvring level (speeding up). The key-point of the approach consists in attaching an execution control at each activity. This enables to react at a level as low as possible when environment changes. The proposed structure is composed of a real-time software in which a real-time expert system is integrated. The expert system allows to simply coordinate the set of monitoring tasks. A simulator is designed to reproduce the vehicle behaviors according to the situation in the traffic.
798,2024,"Measuring GenAI Usage Patterns in a University Campus via Network Traffic Analysis nan Generative AI platforms backed by large-language models (LLMs) are taking the world by storm. Starting with ChatGPT launched a mere 18 months ago that can generate amazingly human-like text responses to prompts, there are now platforms that can generate code (GitHub Copilot), images (Dall-E), and even video clips (Sora). In this fast evolving world of GenAI, there is huge interest in the community in tracking the usage patterns of these platforms, as well as performance in terms of responsiveness and network load. Our paper is the first attempt to track usage of emerging GenAI platforms via real-time analysis of network traffic. This can be useful to enterprises seeking to know which GenAI services their employees use most; to Communications Service Providers wanting to know the network loads imposed; and to financial investors needing a pulse on market trends. We begin by explaining the network anatomy of ChatGPT prompt/response interactions in detail, and extend it to six other GenAI platforms supporting text, code, and image generation. We then develop a measurement method to identify and quantify GenAI interactions via real-time analysis of network traffic. We deploy our monitoring system in a University campus over a 5-month period, and reveal interesting insights such as GenAI usage distribution across days of the week and deviations during assessment periods; variation in prompt-to-response-size ratios across the various GenAI platforms; and differences in response times arising from model versions."
799,2023,"The state of human-centered NLP technology for fact-checking nan Misinformation threatens modern society by promoting distrust in science, changing narratives in public health, heightening social polarization, and disrupting democratic elections and financial markets, among a myriad of other societal harms. To address this, a growing cadre of professional fact-checkers and journalists provide high-quality investigations into purported facts. However, these largely manual efforts have struggled to match the enormous scale of the problem. In response, a growing body of Natural Language Processing (NLP) technologies have been proposed for more scalable fact-checking. Despite tremendous growth in such research, however, practical adoption of NLP technologies for fact-checking still remains in its infancy today.In this work, we review the capabilities and limitations of the current NLP technologies for fact-checking. Our particular focus is to further chart the design space for how these technologies can be harnessed and refined in order to better meet the needs of human fact-checkers. To do so, we review key aspects of NLP-based fact-checking: task formulation, dataset construction, modeling, and human-centered strategies, such as explainable models and human-in-the-loop approaches. Next, we review the efficacy of applying NLP-based fact-checking tools to assist human fact-checkers. We recommend that future research include collaboration with fact-checker stakeholders early on in NLP research, as well as incorporation of human-centered design practices in model development, in order to further guide technology development for human use and practical adoption. Finally, we advocate for more research on benchmark development supporting extrinsic evaluation of human-centered fact-checking technologies."
800,1990,"Expert bidder for a pilot monthly schedule nan Several weeks before each month begins, an airline pilot is issued a bid package describing the coming month's schedule. The pilot must turn in a bid on the schedule. Schedulers then assign a `line of time' based on the category seniority number of the pilot. (A category is defined by the base, aircraft type, and seat, i.e. captain, copilot, or flight engineer.) The most senior pilot in a category gets his/her first choice and need bid only one line. The second most senior pilot will get his first choice if it is different from the #1 pilot, or his second choice if their first choices are the same. And so it goes on down the seniority list. It is obvious that a pilot must bid at least as many lines of time as his category seniority to insure he is not assigned an undesired line. Many factors can enter into bidding the lines of time, and especially for pilots near the bottom of the category seniority, a lot of time is involved. Each pilot weighs the importance of each factor differently, and priorities of each pilot often change from month to month. This paper describes an expert system to help make the task easier and more exact. The system is programmed in OPS5 and runs on a VAX 8800 machine."
801,2023,"Chat Overflow: Artificially Intelligent Models for Computing Education - renAIssance or apocAIypse? nan Recent breakthroughs in deep learning have led to the emergence of generative AI models that exhibit extraordinary performance at producing human-like outputs. Using only simple input prompts, it is possible to generate novel text, images, video, music, and source code, as well as tackle tasks such as answering questions and translating and summarising text.However, the potential for these models to impact computing education practice is only just beginning to be explored. For example, novices learning to code can now use free tools that automatically suggest solutions to programming exercises and assignments; yet these tools were not designed with novices in mind and little to nothing is known about how they will impact learning. Furthermore, much attention has focused on the immediate challenges these models present, such as academic integrity concerns. It seems that even in the AI-era a pending apocalypse sells better than a promising renaissance.Generative AI will likely play an increasing role in people's lives in the reasonably foreseeable future. Model performance seems set to continue accelerating while novel uses and new possibilities multiply. Given this, we should devote just as much effort to identifying and exploiting new opportunities as we do to identifying and mitigating challenges.In this talk, we begin by discussing several concrete and researchbacked opportunities for computing educators. Many of these have already shown great promise in positively impacting current practice. We then discuss more short- to medium-term possibilities in areas such as student recruitment, and curricular changes. Finally - against our better judgement - we speculate over the longerterm, including rethinking the very fundamentals of the practice of teaching introductory and advanced computing courses. In these *Randomly ordered by the spin of a roulette wheel, the results of which were eventually confirmed as valid, reliable and replicable by ChatGPT Plus on the fourth attempt (GPT4 March 23, 2023 version). No other artificial intelligence was used in the authoring of this document. discussions we suggest potential research questions and directions. Although making remotely accurate predictions in such a fastchanging landscape is foolhardy, we believe that now is the time to explore and embrace opportunities to help make positive change in as many computing classrooms as possible."
802,2023,"Thrilled by Your Progress! Large Language Models (GPT-4) No Longer Struggle to Pass Assessments in Higher Education Programming Courses nan This paper studies recent developments in large language models' (LLM) abilities to pass assessments in introductory and intermediate Python programming courses at the postsecondary level. The emergence of ChatGPT resulted in heated debates of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming classes (e.g., cheating). Recent studies show that while the technology performs surprisingly well on diverse sets of assessment instruments employed in typical programming classes the performance is usually not sufficient to pass the courses. The release of GPT-4 largely emphasized notable improvements in the capabilities related to handling assessments originally designed for human test-takers. This study is the necessary analysis in the context of this ongoing transition towards mature generative AI systems. Specifically, we report the performance of GPT-4, comparing it to the previous generations of GPT models, on three Python courses with assessments ranging from simple multiple-choice questions (no code involved) to complex programming projects with code bases distributed into multiple files (599 exercises overall). Additionally, we analyze the assessments that were not handled well by GPT-4 to understand the current limitations of the model, as well as its capabilities to leverage feedback provided by an auto-grader. We found that the GPT models evolved from completely failing the typical programming class' assessments (the original GPT-3) to confidently passing the courses with no human involvement (GPT-4). While we identified certain limitations in GPT-4's handling of MCQs and coding exercises, the rate of improvement across the recent generations of GPT models strongly suggests their potential to handle almost any type of assessment widely used in higher education programming courses. These findings could be leveraged by educators and institutions to adapt the design of programming assessments as well as to fuel the necessary discussions into how programming classes should be updated to reflect the recent technological developments. This study provides evidence that programming instructors need to prepare for a world in which there is an easy-to-use widely accessible technology that can be utilized by learners to collect passing scores, with no effort whatsoever, on what today counts as viable programming knowledge and skills assessments."
803,2024,Inferring Human Biomechanics With Deep Learning for Human-Centered Computer Vision Applications nan nan
804,2023,"On Codex Prompt Engineering for OCL Generation: An Empirical Study nan The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex."
805,2024,"LLM Agents as 6G Orchestrator: A Paradigm for Task-Oriented Physical-Layer Automation nan The rapid advancement in generative pre-training models is propelling a paradigm shift in technological progression from basic applications such as chatbots towards more sophisticated agent-based systems. It is with huge potential and necessity that the 6G system be combined with the copilot of large language model (LLM) agents and digital twins (DT) to manage the highly complicated communication system with new emerging features such as native AI service and sensing. With the 6G-oriented agent, the base station could understand the transmission requirements of various dynamic upper-layer tasks, automatically orchestrate the optimal system workflow. Through continuously get feedback from the 6G DT for reinforcement, the agents can finally raise the performance of practical system accordingly. Differing from existing LLM agents designed for general application, the 6G-oriented agent aims to make highly rigorous and precise planning with a vast amount of extra expert knowledge, which inevitably requires a specific system design from model training to implementation. This paper proposes a novel comprehensive approach for building task-oriented 6G LLM agents. We first propose a two-stage continual pre-training and fine-tuning scheme to build the field basic model and diversities of specialized expert models for meeting the requirements of various application scenarios. Further, a novel inference framework based on semantic retrieval for leveraging the existing communication-related functions is proposed. Experiment results of exemplary tasks, such as physical-layer task decomposition, show the proposed paradigm's feasibility and effectiveness."
806,2024,"An Exploratory Study on Fine-Tuning Large Language Models for Secure Code Generation nan AI-powered coding assistants such as GitHub Copilot and OpenAI ChatGPT have achieved notable success in automating code generation. However, these tools rely on pre-trained Large Language Models (LLMs) that are typically trained on human-written code sourced from open-source project hosting sites like GitHub, which often contains inherent security vulnerabilities. These vulnerabilities may then be mirrored in the code generated by these LLMs, a critical risk revealed and highlighted by recent empirical studies. In this work, we present an exploratory study on whether fine-tuning pre-trained LLMs on datasets of vulnerability-fixing commits can promote secure code generation. We explored two parameter-efficient fine-tuning techniques (LoRa and IA3) on two pre-trained LLMs for code generation. We crawled a fine-tuning dataset (14,622 C and C++ files) for secure code generation by collecting code fixes of confirmed vulnerabilities from open-source repositories. Our evaluation dataset comprises 52 vulnerability scenarios designed to cover the top most dangerous C and C++ Common Weakness Enumerations (CWEs). Each scenario is a prompt that may induce LLMs to generate vulnerable code. Our exploration reveals that fine-tuning LLMs can improve secure code generation by 6.4% in C language and 5.4% in C++ language. We further experimented with fine-tuning LLMs using different versions of the collected secure code dataset (block, function, and line). We found that fine-tuning with function-level and block-level datasets achieves the best secure code generation performance, compared to the alternatives (file-level and line-level)."
807,2023,"Development of an Assessment Scale for Measurement of Usability and User Experience Characteristics of Bing Chat Conversational AI nan After the introduction of the ChatGPT conversational artificial intelligence (CAI) tool in November 2022, there has been a rapidly growing interest in the use of such tools in higher education. While the educational uses of some other information technology (IT) tools (including collaboration and communication tools, learning management systems, chatbots, and videoconferencing tools) have been frequently evaluated regarding technology acceptance and usability attributes of those technologies, similar evaluations of CAI tools and services like ChatGPT, Bing Chat, and Bard have only recently started to appear in the scholarly literature. In our study, we present a newly developed set of assessment scales that are related to the usability and user experiences of CAI tools when used by university students, as well as the results of evaluation of these assessment scales specifically regarding the CAI Bing Chat tool (i.e., Microsoft Copilot). The following scales were developed and evaluated using a convenience sample (N = 126) of higher education students: Perceived Usefulness, General Usability, Learnability, System Reliability, Visual Design and Navigation, Information Quality, Information Display, Cognitive Involvement, Design Appeal, Trust, Personification, Risk Perception, and Intention to Use. For most of the aforementioned scales, internal consistency (Cronbach alpha) was in the range from satisfactory to good, which implies their potential usefulness for further studies of related attributes of CAI tools. A stepwise linear regression revealed that the most influential predictors of Intention to Use Bing Chat (or ChatGPT) in the future were the usability variable Perceived Usefulness and two user experience variables-Trust and Design Appeal. Also, our study revealed that students' perceptions of various specific usability and user experience characteristics of Bing Chat were predominantly positive. The evaluated assessment scales could be beneficial in further research that would include other CAI tools like ChatGPT/GPT-4 and Bard."
809,2024,Development of a Framework and Checklist to Guide the Translation of AI Systems for Clinical Care nan nan
810,2022,"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming nan Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI's GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In thiswork, we explore howCodex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known Rainfall Problem along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities."
